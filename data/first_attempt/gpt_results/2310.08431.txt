Introduction Human behavioral studies [1, 2, 3] and animal neurophysiological studies [4, 5] have suggested that the brain performs statistically optimal Bayesian inference to interpret the external world [6, 7, 8, 9]. One promising theory for implementing Bayesian inference in the brain is to interpret the variability of neural responses as Monte Carlo sampling of the posterior distribution [10]. This perspective naturally accounts for the irregular firing patterns and other response properties observed in sensory cortex neurons [11, 12, 13]. Numerous sampling-based models [10, 14, 15, 16, 17, 12, 13, 18] have been proposed to elucidate neural dynamics and the underlying mechanisms of Bayesian inference. To facilitate the brain’s ability to derive meaningful representations from sensory input, it must continually update its generative model to approximate the true distribution of the external world [19]. Previous approaches often neglect this critical learning process. They either maintain fixed parameters for their generative model [15, 17, 16], or they employ biologically implausible methods like the variational approach with backpropagation (BP) for training [9]. Is there a generative model that integrates sampling-based inference with the capability to learn locally in both time and space? 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Energy-based models (EBMs) [20] provide a framework for inference with sampling method and learning with spatially localized rules. The reason for non-locality in time is the need to estimate the partition function. To estimate the global partition function, the network has to perform a top- down pass to obtain a negative sample. This process is referred to as the negative phase. During this pass, it disrupts the neural network’s stored inference results, which is essentially the same reason for the temporal non-locality as in the case of BP. Recently, predictive coding networks (PCNs) [21, 22, 23, 24] use the Gaussian distribution to avoid the negative phase since the partition function of Gaussian is constant. But further study [25] reveals that the Gaussian assumption is restrictive when dealing with complex probability distributions. Moreover, setting aside biological constraints, estimating the partition function itself poses a significant challenge. In the field of machine learning, various sampling methods, such as amortized generation [26] and implicit generation [27], have been proposed to tackle this issue. However, both of these methods involve the use of the negative phase, which is known for being challenging to converge. Generative adversarial networks (GANs) address this issue by utilizing a discriminative model, but GANs are notoriously difficult to train in practice. Besides, the inference dynamic of Gibbs sampling [28, 20] or Langevin sampling [29] in EBMs essentially perform random walks in local regions rather than the whole posterior space, which is too slow to be compatible with brain functions [16]. Therefore, it is crucial to investigate whether neural circuits in the brain have the capacity of realizing sampling-based inference rapidly. Summary of the work. In Sec.2, we propose that our brain holds an EBM as the intrinsic generative model to interpret the external world. The neural dynamics employ a sampling-based method for Bayesian inference. Simultaneously, the learning dynamic aims to minimize the discrepancy between the observed distribution of intrinsic model and the real world. In Section 3, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which allocates the partition function across each layer. This allocation shifts the estimation of the total sample space required for calculating the partition function from a product of individual layer spaces to a sum of layer spaces. This approach enhances the convergence of our model. Furthermore, we efficiently sample the normalization term of the exponential-family in each layer using a group of neurons with fast dynamics. This localizes the learning process in both time and space. In Sec.4, we find that incorporating noisy adaptation, a generic feature of neuronal responses, into the inference dynamics effectively yields a second-order Langevin dynamic. In Sec.5, we validate the capabilities of the HEE model using 2D synthetic datasets and FashionMNIST [30]. Then, we incorporate receptive field as the biological constrains to the HEE model training on CIFAR10 [31]. We show that the HEE model can achieve a performance comparable with previous EBMs [27]. We also investigate the neural representation of semantic information, including orientation, color and category, which exhibit similarities to biological visual systems. And the neural adaptation can trigger neural phenomena including oscillations and transient which are widely observed in biological systems. In Sec.6, we discuss several related theories and models. Main Contributions. We propose a hierarchical EBM whose learning process is localized both in time and space, which could potentially serve as a mechanism for the brain to utilize changes in synaptic strength for learning. And our brain-inspired EBM also presents a technique for estimating the partition function, which is a challenging problem within the machine learning community. 2 The intrinsic generative model In this section, we propose that our brain holds energy-based models (EBMs) as the intrinsic generative model consisting of two components: inference and learning. Inference is believed to be carried out through neural sampling [10, 20], while learning is accomplished through long-term synaptic plasticity [32]. Let x be the observation received by our brain, and let z be the latent variable represented by neurons. The joint distribution of the EBMs is written as pθ(x, z) = pθ(x|z)pθ(z), where θ are stored in the connection weights of neurons (Fig.1A). The EBMs aims to minimize the difference between the intrinsic marginal distribution pθ(x) and the true distribution of the external world ptrue(x), which is described by the Kullback–Leibler divergence, min θ DKL [ptrue(x) ∥ pθ(x)] . (1) 2 (A) Probabilistic model (B) Inference & learning (C) Joint generation (D) Marginal generation x z x z pθ(x|z) pθ(z) x z x z Figure 1: (A) The directed graphical model of the energy-based model (EBM). (B) The latent variable z receives the likelihood information pθ(x|z) from the observation x and combines it with the prior knowledge pθ(z) to perform the inference dynamic. And the connected weights θ changes following the learning dynamic (dashed line). (C) The latent variable z performs inference dynamic and the observation x performs generation dynamic, which leads to the distribution of x, z ∼ pθ(x, z). And the connection weights θ are fixed (solid line). (D) The latent variable doesn’t receive likelihood information from observation and z ∼ pθ(z), x ∼ pθ(x) which is called marginal generation. The neural system can adopts the gradient based learning method such as gradient decent. And the gradients is calculated as (see SI for detailed proof), ∇θDKL [ptrue(x) ∥ pθ(x)] = −Ex∼ptrue(x)Ez∼pθ(z|x) [∇θ ln pθ(x, z)] . The second expectation of the above equation requires the posterior of a given observation x, which is calculated as pθ(z|x) = pθ(x, z)/pθ(x). Practically, the posterior is intractable for the denominator pθ(x) requires complex integral. Variational inference is usually used to solve this problem (such as VAE [33]). And the EBMs adopts the sampling method to avoid complex calculations. By leveraging the relationship ∇z ln pθ(z|x) = ∇z ln pθ(x, z) ,the samples of posterior can be offered by the neural dynamic (Langevin sampling), (2) τz dz dt = ∇z ln pθ(x, z) + √ 2τzξ, (3) where ξ is Gaussian white noise and τz is the time constant. The stationary distribution of the above dynamic is our target distribution pθ(z|x). The sampling algorithm, along with the joint distribution, determines the connections of neurons and their inference dynamic (Fig.1B). For the learning dynamic, the brain receives the observations from the real world continuously (Ex∼ptrue(x)), and the neural dynamic mentioned above can produce samples of their posterior simul- taneously (Ez∼pθ(z|x)). The parameters can be updated according to the gradient, which is often implemented by Hebbian learning rules(Fig.1B), τθ dθ dt = −∇θDKL [ptrue(x) ∥ pθ(x)] = ∇θ ln pθ(x, z), (4) where τθ is the time constant of synapses. For neuroscience, this is the end of the story. Nevertheless, EBMs can also generate observations which the machine learning society follow with interest. In the generation process, the observation is not fixed but undergoes the Langevin sampling, τx dx dt = ∇x ln pθ(x, z) + √ 2τxξ. (5) The latent variable z can follow the inference dynamic Eq.(3). In this case, the observation x and latent variable z together follow the joint distribution pθ(x, z). Thus, the observation x can produce samples following pθ(x), which is called joint generation (Fig.1C). However, in order to get the marginal distribution pθ(x), the latent variable z just needs to follow the prior distribution pθ(z) rather than reaching the posterior. In this case, the generation dynamic of latent variable is written as, τz dz dt = ∇z ln pθ(z) + √ 2τzξ. 3 (6) And the stationary distribution of Eq.(5) performed by observation x still equals to pθ(x), which is called marginal generation (Fig.1D). In Sec.5, we will see that the marginal generation performs better than joint generation. Here is an informal understanding. The process of sampling (x, z) can be understood as searching for a specific pair. We assume that the size of the x-space is O(m) and the size of the z-space is O(n). In joint generation, the search is conducted simultaneously in both the x-space and z-space, resulting in a required search space of O(n ∗ m). In marginal generation, the process involves initially searching in the z-space according to pθ(z). Once z is found, it is fixed. This step’s search space size is O(n). Then, x is searched based on pθ(x|z) in the x-space. This step’s search space size is O(m), leading to a combined required search space size of O(n + m). 3 Exponential-family energy-based model In this section, we provide a neural implementation of the HEE model and outline the specific dynamics involved in inference, learning, and generation, as discussed in Section 2. Approximating the target distribution ptrue(x) which is diverse and complex requires a good representation ability of the model. Exponential families include many of the most common distributions (such as normal, Poisson, gamma distribution and so on). Moreover, exponential families can be easily parameterized, allowing for generalization and flexibility in modeling various types of distributions. Let x0 ∈ Rn0 be the observation (such as an image) received by our brain. And there are L layers of neurons representing the latent variables x1:L = {x1, x2, ..., xL}, xl ∈ Rnl . The joint distribution is a Markov chain (Fig.2A) starting with p(xL) = exp (cid:2)ηT L ϕ(xL) + g(xL) − A(ηL)(cid:3), pθ(x0:L) = p(xl) L−1 (cid:89) l=0 pθ(xl|xl+1), pθ(xl|xl+1) = exp (cid:2)ηT l ϕ(xl) + g(xl) − A(ηl)(cid:3) . (7) The natural parameter ηl ∈ Rnl is a function of xl+1 with parameters θl ∈ Rnl×nl+1, which is written as ηl = θlf (xl+1), where f (·) is the activation function. And ηL is constant. The sufficient statistic ϕ(xl) ∈ Rnl and the base measure g(xl) ∈ R is the function of xl. A(ηl) ∈ R is the normalize term (log-partition function) to make sure the sum of the probability equals to 1. In order to get the inference dynamic, we substitute the joint distribution Eq.(7) into the Langevin dynamic Eq.(3) obtaining, dxl dt l−1 [ϕ(xl−1) − A′(ηl−1)] + ϕ′(xl)ηl + g′(xl) + = f ′(xl)θT 2τzξl, τz √ (8) where f ′(xl), ϕ′(xl) ∈ Rnl×nl are diagonal matrices. The derivative of log-partition A′(ηl−1) is intractable for it needs complex integral. Here, we use a group of interneurons εl−1 ∈ Rnl−1 to rep- resent the term ϕ(xl−1) − A′(ηl−1). It can be proved that A′(ηl−1) = Exl−1∼pθ(xl−1|xl) [ϕ(xl−1)] (See SI for detailed proof). Thus, in order to calculate A′(ηl−1), the interneurons need to produce samples xl−1 following the distribution pθ(xl−1|xl) in a short time compared with the inference dynamic. Therefore, the dynamic of εl−1 can be written as, εl−1 = ϕ(xl−1) − ϕ(ul−1), τu dul−1 dt = ϕ′(ul−1)ηl−1 + g′(ul−1) + √ 2τuξu. (9) 1, we can ensure that ul−1 converges much faster than xl, and the By setting the time constant τu ≪ τz stationary distribution of ul−1 corresponds to pθ(ul−1|xl). This leads to εl−1 = ϕ(xl−1)−A′(ηl−1). Now, we can rewrite the inference dynamic Eq.(8) into, τz dxl dt = f ′(xl)θT l−1εl−1 + ϕ′(xl)ηl + g′(xl) + √ 2τzξl. (10) The first term on the right side of the dynamic equation indicates that neurons xl receive feedback from interneurons εl−1, which provides likelihood information pθ(xl−1|xl). The second term shows 1There are various types of interneurons that target on pyramidal cells, comprising approximately 10-20% of the overall neuron population in the cerebral cortex [34]. The interneurons in the HEE model bear the closest resemblance to the Large Basket Cell or Nest Basket Cell [35], which collectively constitute around 50% of interneurons. Their electrophysiological characteristics include fast spiking, non-accommodating, and non- adapting behaviors. These interneurons have also been identified in the visual cortex of ferrets [36], displaying short-duration action potentials (approximately 0.5 ms at half height). This suggests that these neurons have shorter time constants compared to pyramidal cells. 4 (A) Probabilistic model (B) Inference & learning xl−1 xl−1 ϕ(xl−1) εl−1 pθ(xl−1|xl) pθ(xl|xl+1) xl xl xl+1 xl+1 η l − 1 ϕ(xl) ηl η l εl (C) l l + 1 g(·) θl Figure 2: (A) The directed graphical model of the hierarchical exponential-family energy-based (HEE) model . (B) The inference and learning dynamic of HEE model. The red arrows represent the likelihood information and the black arrows represent the prior information. Neurons xl receive the likelihood information from εl−1 and receive prior information ηl from εl. The interneurons εl−1 receive the natural parameter ηl−1 from neurons xl and