INTRODUCTION Many recent works (e.g., Brohan et al. (2023); Shen et al. (2023); Yao et al. (2022); Shinn et al. (2023)) have explored the use of large language models (LLMs) to drive the decision-making of in- telligent, autonomous agents. Given a problem to solve, these LLM-based agents break the problem down into a sequence of steps, where each step involves either generating text or executing a tool (e.g., an API call Schick et al. (2023a)) which can supply new context to the agent. Importantly, while the order of steps to take is dictated by a high-level, prespecified behavior implemented by the user, the underlying LLM is still allowed a significant amount of flexibility in what it may produce. At each individual step, the outputs (i.e., the specific text or tool) are entirely determined by the LLM, thus allowing the agent to leverage the strong generative capabilities of LLMs while ensuring there are some guardrails to prevent aberrant behavior. Figure 1 provides an example of the popular ReACT Yao et al. (2022) framework, where an agent executes a loop that goes between generative steps (i.e., Thought, Action, Action Input) and tool execution steps (i.e., Observation). Agent-based frameworks confer a number of benefits. First, these agent-based systems are signifi- cantly more performant than simpler systems utilizing standard prompting-based approaches (e.g., chain-of-thought Wei et al. (2022)). Second, because they allow the LLM to seamlessly integrate with external tools and incorporate the tool outputs into its context, agent-based approaches can address a much broader range of datasets and tasks (e.g., those involving web search Yang et al. (2018)). Lastly, by allowing end users to define an agent’s behavior, the users are given more flexi- bility in tailoring very large language models to their needs than they would have otherwise. 1 Preprint under review (a) Text output by ReACT agent (b) Diagram of ReACT agent architecture Figure 1: ReACT agent Yao et al. (2022) that alternates between generative steps (e.g., Thought, Action, Action Input) and tool execution steps (i.e., Observation) Though LLM-based agents show much promise, there still remain challenges involved with their practical application. For instance, as each agent has its own strengths and weaknesses, it can be necessary to try a variety of different agents when approaching a problem. This can be a steep barrier to entry, as the lack of a standard framework for defining agents means that the end user must reimplement in code the exact behavior they wish for an agent to exhibit. In addition, even if a user finds that a particular agent type is well suited to their target task, the surface-level integration of current agent implementations with LLMs can mean running that agent will require several rounds of costly reprompting Xu et al. (2023). To address the aforementioned challenges, inspired by previous work on incorporating logic-based constraints into decoding Lu et al. (2021; 2022), we propose a declarative framework for formally specifying the high-level behavior of an LLM-based agent. To begin, our framework takes in an agent behavior specified in a simple fragment of linear temporal logic (LTL). The LTL specification is then used to define a constrained decoder that ensures the LLM-based agent executes steps in a way that conforms to the user’s expectation. The specification thus serves as a type of contract for agent behavior, which provides straightforward opportunities for optimizing the generation process. Our contributions in this work are as follows: (a) we introduce a declarative framework for defining LLM-based agents that is both lightweight and user-friendly, (b) we demonstrate the benefits of this framework, which includes the ability to enforce complex agent behavior, perform formal validation of prompt examples, and seamlessly incorporate content-focused logical constraints into generation, and (c) provide an analysis using three standard datasets (Hotpot QA Yang et al. (2018), Fever Thorne et al. (2018), and GSM8K Cobbe et al. (2021)) that demonstrates when the hard constraints imposed on LLM generation improve performance. 2 Preprint under review 2 RELATED WORK 2.1 LLM-BASED AGENTS The quest to develop agents that are capable of exhibiting intelligent behavior in real-world settings, without human intervention, has been a constant objective across the evaluation of AI Wooldridge & Jennings (1995); Weiss (1999). The recent advancement of LLMs unrolls new avenues of research to achieve this goal, where agents are backed with LLMs to perform complex tasks. Various LLM-based agents targeting different tasks have been proposed, such as - WebAgent Gur et al. (2023) demonstrates the potential to create language-based agents capable of executing tasks on actual websites by adhering to natural language commands; Generative Agents Park et al. (2023) simulates believable human behavior; MetaGPT Hong et al. (2023) incorporates efficient human workflows as a meta-programming approach into LLM-based multi-agent collaboration; SayCan Ahn et al. (2022) illustrates the capability to use LLMs in a embodied agents. The success of these task-specific agents ignites the initiative to start open-source projects such as - AutoGPT 1, SuperAGI 2, and BabyAGI 3 that are focused on the objective of constructing self-sufficient agents capable of fulfilling users’ requests. 2.2 TOOL AUGMENTED LLMS In recent years, the development of large language models (LLMs) has made tremendous progress, and continues to drive research in prompt-based learning Wei et al. (2022); Khot et al. (2022) and instruction tuning Touvron et al. (2023); Gao et al. (2023b); Peng et al. (2023). While LLMs have demonstrated impressive performance, they are constrained by inherent limitations, with one of the primary drawbacks being their ability to utilize external tools. To address this limitation, there has been an increasing focus on the exploration of incorporating external tools into LLMs or creating tool-augmented LLMs. Self-supervised or self-instructed learning is the leading methodology of augmenting tools into LLMs and we have witnessed multiple works in this direction such as ToolFormer Schick et al. (2023b); Gorilla Patil et al. (2023); TALM Parisi et al. (2022); etc. Through the integration of exter- nal tools, these augmented LLMs can achieve precise mathematical reasoning Cobbe et al. (2021); Thoppilan et al. (2022), access up-to-the-minute information with the assistance of web search en- gines Nakano et al. (2021), and harness domain-specific knowledge from external resources Yu et al. (2022). Additionally, some works use the Python interpreter to create complex programs, which helps them perform logical reasoning tasks better by accessing powerful computational resources Gao et al. (2023a); Chen et al. (2022). 2.3 CONSTRAINED DECODING There has been a long line of constrained generation methods that modify the standard beam search decoding procedure at inference time, to incorporate constraints in the output. Lu et al. (2021; 2022); Hokamp & Liu (2017); Post & Vilar (2018). Anderson et al. (2017) proposes a constrained beam search algorithm that keeps track of constraints via a finite-state machine, and demonstrates its benefits on several image captioning tasks. Their method forces the inclusion of selected tag words in the output, and fixed, pre-trained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Lu et al. (2021) proposes NEUROLOGIC DECODING, which enforces the satisfaction of lexical constraints (specified as any predicate logic formula having word inclusion and exclusion constraints) via adding a penalty term for constraint violation in the beam search de- coding algorithm. Lu et al. (2022) improves on this by incorporating the A∗-search algorithm using lookahead heuristics at each decoding step. Bastan et al. (2023) builds on top of NEUROLOGIC DE- CODING by incorporating structural constraints that capture dependency parsing information. They demonstrate that the use of search algorithms, as well as structural constraints, improve performance over NEUROLOGIC on a variety of tasks such as lexical constrained generation, summarization, and machine translation. 1https://github.com/Significant-Gravitas/AutoGPT 2https://github.com/TransformerOptimus/SuperAGI 3https://github.com/yoheinakajima/babyagi 3 Preprint under review Figure 2: Examples of truth assignments over time with various LTL operators Despite the significant progress in improving text generation using inference-type constraints, there remains a significant gap in the literature for improving generation from large-language models during inference, particularly for constraining the thought and action sequences as specified, for language models as agents. Thus, in this work we take the first step toward this by constraining agents to follow specifications for example, in ReACT Yao et al. (2022), Reflexion Shinn et al. (2023), chain-of-thought Wei et al. (2022) and chat-bot agents. 3 METHOD In this section, we introduce our framework for designing and implementing autonomous agents that can interact with the environment to solve problems expressed in natural language. Our framework is intended to be lightweight (i.e., add as little additional overhead to LLM operation as is possible) and declarative (i.e., the user specifies the desired high-level behavior in terms of constraints without concern for how they should be implemented or enforced). To begin, we introduce linear temporal logic, which underpins our agent specification framework. Then, we provide a more formal defini- tion of agents and how they are specified in our framework. Last, we describe how the specification framework is used to define a constrained decoder that controls what an agent can generate. 3.1 LINEAR TEMPORAL LOGIC Here we provide a light overview of linear temporal logic (LTL), which is the key component to our agent specification framework. LTL is a modal temporal logic originally introduced for formal verification Pnueli (1977) that extends propositional logic with the temporal operators ⃝ (next) and U (until). The two operators have intuitive definitions, with ⃝ (next) being a unary operator that (informally) means a formula φ must hold in the next time step, and U (until) being a binary operator that specifies a formula φi must be true until φj becomes true. LTL formulas are defined over a set of atomic propositions P with their syntax given by φ ::= true (cid:12) (cid:12) p (cid:12) (cid:12) ¬φ (cid:12) (cid:12) φ1 ∧ φ2 (cid:12) ⃝ φ (cid:12) (cid:12) (cid:12) φ1 U φ2 where p ∈ P An LTL formula is evaluated over an infinite sequence of observations, where each observation is a truth assignment over symbols in P. Letting φ be a LTL formula and σ be the sequence of 4 Preprint under review observations σ = ⟨σ1, σ2, . . .⟩, where each σi can be considered the subset of P that is true at time i, then we write σ |= φ (satisfies) when σ σ σ σ σ σ |= true |= p |= ¬φ |= φ1 ∧ φ2 |= ⃝ φ |= φ1 U φ2 (i.e., σ |= φ does not hold) iff p ∈ σ0 iff σ ̸|= φ iff σ |= φ1 and σ |= φ2 iff σ[1 . . .] |= φ iff ∃j ≥ 0. σ[j . . .] |= φ2 and σ[i . . .] |= φ1, for all 0 ≤ i < j where σ[i . . .] = ⟨σi, . . .⟩ is the remaining sequence of observations following time step i. From the operators listed above, we can define additional propositional logic operators ∨ (disjunction), and → (implication) as well as temporal operators ♢ (eventually) and □ (always) φ1 ∨ φ2 φ1 → φ2 ♢ φ □ φ := ¬(¬φ1 ∧ ¬φ2) := ¬φ1 ∨ φ2 := true U φ := ¬ ♢ ¬φ In addition, we also define a convenient shorthand notation for a chained sequence of next operators ⃝ ⟨φ1, φ2, φ3, . . .⟩ := φ1 ∧ (φ1 → ⃝(φ2 ∧ (φ2 → ⃝(φ3 ∧ (φ3 → ⃝(. . .)))))) with this having the straightforward informal interpretation of “φ1 then φ2 then φ3”. In Figure 2, we provide a graphical depiction of the truth assignments over time for the above temporal operators. In this work, we found that only allowing formulas to be those containing atomic propositions p, as well as operators →, ⃝ , □, and U (i.e., we do not allow formulas to include ¬, ∧, etc.) was sufficient to represent the range of existing agent architectures. We leave extending the set of operators (e.g., to include ♢, ∧, etc.) to future work. For more details regarding LTL and its numerous applications, we direct the interested reader to Baier & Katoen (2008). 3.2 SPECIFYING AGENT BEHAVIOR We model agents as generic transition systems, where a transition system is considered a tuple ⟨S, δ, s0, send⟩ consisting of a non-empty set of states S, a state transition function δ, an initial state s0, and a final state send. At each time step, the agent will receive a string from either an LLM or the environment 4, with the source of the string being determined by the particular state the agent is in. To define an agent and its underlying transition system, the user provides a specification consisting of 1) a list of states and their properties and 2) a desired behavior in the form of an LTL formula. Figure 3 shows an example of a specification being provided in the format of a PDDL-style s- expression. In the specification, the :states list contains all possible states for the agent. Each state within the list must specify a prompt string (e.g., “Thought:” for the Thought state), which will serve as both an initial prompt for when the agent is in that state and as a signal to detect when a state transition occurs. By default it is assumed that the string received by an agent will be provided by the LLM. A user may override this by indicating the environment as the intended provider instead by using a special :env-input flag (e.g., shown in the Observation state). The behavior of an agent is provided in the :behavior list. Referring back to the specification, we see an LTL