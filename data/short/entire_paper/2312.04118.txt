3
2
0
2
c
e
D
7

]

V
C
.
s
c
[

1
v
8
1
1
4
0
.
2
1
3
2
:
v
i
X
r
a

Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play

Timothy Schauml¨offel1,2,∗, Arthur Aubret3,4,∗, Gemma Roig1,2,†, Jochen Triesch1,3,†
1Goethe University, Frankfurt am Main, Germany
2The Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany
3Frankfurt Institute for Advanced Studies, Frankfurt am Main, Germany
4Xidian-FIAS international Joint Research Center, Frankfurt am Main, Germany
{schaumloeffel, roig}@em.uni-frankfurt.de
{aubret, triesch}@fias.uni-frankfurt.de

Abstract

1. Introduction

Infants’ ability to recognize and categorize objects de-
velops gradually. The second year of life is marked by
both the emergence of more semantic visual representa-
tions and a better understanding of word meaning. This
suggests that language input may play an important role
in shaping visual representations. However, even in suit-
able contexts for word learning like dyadic play sessions,
caregivers utterances are sparse and ambiguous, often re-
ferring to objects that are different from the one to which the
child attends. Here, we systematically investigate to what
extent caregivers’ utterances can nevertheless enhance vi-
sual representations. For this we propose a computational
model of visual representation learning during dyadic play.
We introduce a synthetic dataset of ego-centric images per-
ceived by a toddler-agent that moves and rotates toy ob-
jects in different parts of its home environment while “hear-
ing” caregivers’ utterances, modeled as captions. We pro-
pose to model toddlers’ learning as simultaneously align-
ing representations for 1) close-in-time images and 2) co-
occurring images and utterances. We show that utterances
with statistics matching those of real caregivers give rise
to representations supporting improved category recogni-
tion. Our analysis reveals that a small decrease/increase in
object-relevant naming frequencies can drastically impact
the learned representations. This affects the attention on
object names within an utterance, which is required for ef-
ficient visuo-linguistic alignment. Overall, our results sup-
port the hypothesis that caregivers’ naming utterances can
improve toddlers’ visual representations. Code and dataset
are available at: https://github.com/neuroai-
arena/ToddlerVisionLearning.git

*Equal contribution
†Shared last authorship

Second-year toddlers are proficient learners of object
representations that support the recognition of objects inde-
pendently of viewpoint (object instance recognition) and the
assignment of novel exemplars to learned categories (ob-
ject categorization recognition). What learning mechanisms
support this development? On one side, there is evidence
that biological organisms learn similar representations for
close-in-time visual inputs [19, 38]. This is called the slow-
ness principle [37] and it supports the building of view-
invariant object representations during, e.g., object manip-
ulations [18, 29]. On the other side, the acquisition of ob-
ject names correlates with a stronger focus on global shape
features relative to texture [13, 17, 28], which spurs per-
formance in name-agnostic categorization tasks [9, 21], see
also [14]. For learning object names, the synchronization
of toddlers’ visual attention on the object and its naming by
the caregivers is crucial [27].

Dyadic play could be particularly important for pro-
moting the emergence of more semantic object representa-
tions, because both learning mechanisms are likely to co-
occur during such play:
the toddler extensively focuses
on/manipulates objects and the caregiver frequently talks
to the toddler about the present objects [26, 31]. How-
ever, naming utterances during such play are sparse [6] and
ambiguous: they typically contain several words unrelated
to the object. Furthermore, even though toddlers have bi-
ased attention towards held objects [39], they often play
in contexts that admit several other objects in the back-
ground [3, 40], adding another potential source of confu-
sion.

In this paper, we investigate two what extent dyadic
play sessions can support the emergence of semantic ob-
ject representations in toddlers. We tackle this question
by proposing a computational model of toddlers’ learning
during dyadic play. First, we introduce the Dyadic Play

1

 
 
 
 
 
 
Figure 1. A) Top view of the Virtual Home Environment, where blue and red dots, respectively, indicate possible agent and toy positions.
The agent is always turned towards a toy position. Toy positions marked 1–3 correspond to sessions 1–3 in C. B) Zoom-in of the scene
in A with turquoise lines indicating the agent’s field of view. C) Images extracted in a temporally ordered fashion (left to right) for three
different “play” sessions of the Dyadic Play Dataset. Text boxes show examples of captions related to the manipulated object (white) or
to another object in the background (red). D) Summary of the learning architecture, see Section 3.2 for details. Abbreviations: MLP:
multi-layer perceptron, MMCL: multimodal contrastive learning, CLTT: contrastive learning through time.

Dataset, a novel dataset of images simulating at-home ego-
centric dyadic play sessions. In this dataset, we simulate
a playing toddler that moves and turns 3D toy models ex-
tracted from the Toy4k dataset [30]. The agent “plays” in
front of a background composed of household furniture (cf.
Fig. 1A) and a plausible number of toys scattered on the
floor (cf. Fig. 1B). As shown in Fig. 1C, The agent occa-
sionally “hears” caregiver’s utterances, modeled as captions
extracted from the CHILDES database, a comprehensive
data repository of children’s language acquisition includ-
ing child-directed speech [20]. Then, we consider a bio-
inspired model of toddlers’ learning that 1) maps close-in-
time visual inputs to similar representations and 2) similarly
aligns the representations of visual inputs and co-occurring
naming utterances (Fig. 1D). We further simulate toddlers’
visual attention by biasing the model to extract visual fea-
tures from the currently held object. The overall model al-
lows us to systematically study the potential impact of the
sparsity and ambiguity of naming utterances on the learned
visual representations.

Our experiments show that utterance statistics reported
in developmental psychology experiments support the con-
struction of semantic visual representations. Our analysis
shows that realistic object-relevant naming frequencies fall
within a range of values for which a multiplicative factor of
two drastically impact the learned representation. This im-

pacts whether the model attends to the object name within
an utterance, which is necessary to efficiently guide visual
representations. Thus, our paper provides computational
support to the hypothesis that caregivers’ sparse and am-
biguous utterances help to build visual representations dur-
ing dyadic play sessions.

2. Related work

Models of toddlers’ object learning Recently, the avail-
ability of datasets of images extracted from toddlers’ head-
mounted cameras [3, 32] has enabled the study of represen-
tations learnt through the senses of toddlers. This way, two
recent models managed to learn word-vision mappings, but
they either used a pretrained vision model [33,36] or trained
on curated data in a supervised fashion [33]. Another model
extracted semantic visual representations by making similar
close-in-time representations [22], but it did not leverage
any sort of utterances. Working with such real-world ego-
centric data has the limitation that a precise control of the
statistics of the training data, like the sparsity or ambigu-
ity of naming utterances, is difficult. This makes it hard to
study how such statistics might affect toddlers’ object learn-
ing. A recent model also trained visual representations in
cross-situational contexts, i.e. when naming utterances can
refer to several visible objects [35]. However, they only
consider synthetic images based on 9 digits rather than sim-

2

ulated ego-centric play sessions.

Slowness principle for learning visual representations
Several computational models showed that making similar
close-in-time representations can lead to visual representa-
tion suitable for object instance recognition, scene recog-
nition and object categorization [1, 10, 11, 25]. Integrating
constraints from toddlers’ perceputal experience into these
models, like short arms or foveation, can lead to improved
ability to recognize objects in front of novel backgrounds
[2]. None of these works modelled interactions with a care-
giver whose utterances provide weak language supervision.

representation

Visuo-language
learning Previous
works explored multi-modal contrastive learning of
audio-visual or text-visual representations and showed
that language can supervise visual representation learn-
ing [7, 15, 24]. However, none of these works studied
whether naming statistics in dyadic play sessions elicit
semantic visual representations. A recent work showed
that adding sparse labeling on top of contrastive learning
through time enhances visual categorization [2]. Unlike
them, we here consider textured objects in a cluttered home
environment and developmentally-relevant statistics of
caregivers’ utterances.

3. Methods

In this section, we expose our computational model of
visual representation learning during dyadic play. We first
describe the Dyadic Play Dataset, which simulates dyadic
play with objects (Section 3.1). Then, we explain in Section
3.2 how we model a toddler’s learning process.

3.1. Dyadic Play Dataset (DPD)

The DPD contains 857,760 images of 224 × 224 pixels
split into 42,888 “play sessions,” each containing 20 im-
ages. There are 12 sessions dedicated to each of 3574 ob-
jects [1], which are extracted from the Toys4k dataset [30].
To create them, we simulate a toddler that interacts with
objects using the simulation platform ThreeDWorld (TDW)
[12].

Image recording At the beginning of each play session,
we place the agent in a random location, with a random
orientation within the Virtual Home Environment [1] (cf.
Fig. 1A). We approximate an ego-centric view of a seated
toddler by positioning a camera at 0.4 m above the ground.
At the beginning, the camera always watches the object be-
ing held, which we call “main object,” making the camera
orientation dependent on the initial position of the main
object. Because toddlers’ short arms constrain the way

they hold objects, we randomly sample the relative start-
ing position of the main object within a distance range of
[0.25; 0.35] m from the agent, [−30; 30]◦ on the sides and
[−30; −10]◦ in elevation. Furthermore, toddlers tend to
keep objects in an upright position [23], hence, we ran-
domly orient the object around the yaw axis (unbounded)
but bound the object orientation in [−5; 5]◦ around the pitch
and roll axes.

In addition to the main object, we add 5 to 20 randomly
sampled (without replacement) background objects to the
scene, following the number of toys commonly present in
the field of view of toddlers during play [3]. To place them,
we follow the same procedure as for the main object, but
change the range of distances from the agent to [0.42; 0.875]
m. In contrast to the object being held, we let them fall to
the floor one at a time using the physics engine of TDW.
Fig. 1B shows a top-view of a subsequent play area.

To simulate natural interactions, in each of the 20 frames
of the play sessions the agent slowly moves and turns the
object along/around the three axes. We display a high vari-
ability of manipulation sequences by generating all move-
ments with an Ornstein-Uhlenbeck stochastic process for
which we randomly generate a new recall coefficient in
[0.1, 1] at the beginning of each session, for each kind
of manipulation and axis. Thus, the agent dynamically
changes the manipulation speed within a session, as well
as the amplitude of changes from one session to the next,
independently for each axis of rotation/motion. We bound
the motion speed by 3◦ per frame for side/elevation motion
and 0.05 m for in-depth motion. However, the agent can not
exceed the starting absolute bounds described above. Sim-
ilarly, we bound the rotation speed by 20◦ around the yaw
axis and 4◦ around the pitch and roll axis, also to simulate
the upright bias of toddlers [23].

Finally, while the agent observes the moving object,
it also executes additional relative small eye movements
around the object in focus (yaw and pitch axis), whose end-
points are sampled from a Normal distribution N ([0, 0], 2×
I). We also allow the agent to focus its gaze on an object
different from the main one with probability 0.7; however,
we discard these images during training as previous meth-
ods already analyzed the impact of changes of attention on
contrastive through time losses [1, 25]. Fig. 1D shows ex-
amples of the resulting sequences of toddler-centric views.
For evaluation purposes, we build a test dataset of 5 im-
ages per main object (a total of 17,870 images), each in dif-
ferent scenes generated as above. To avoid correlated im-
ages, we do not apply temporal manipulations used during
training. Since rotations around pitch/roll axis may go be-
yond their initial boundaries after some manipulation time
during training, it may create a subsequent data distribu-
tion shift between train and test images. Assuming that the
pitch/roll rotations rarely go beyond [−20; 20]◦ from their

3

starting orientation during training, we increase the initial
range of test object orientations to [−20; 20]◦ around the
pitch and roll axis

to

aim

utterances We

statistics of utterances

Naming
simulate
developmentally-relevant
from
a caregiver to a toddler during a play session. To achieve
this, we source relevant transcripts from the CHILDES
database [20]. Our study focuses specifically on transcripts
from native English-speaking children residing in North
America and the UK, between the ages of six months
and two years. We only keep utterances of caregivers.
As we are interested in object-related statements, we
curate captions derived from statements identifying objects
from the Toys4k dataset [30]. To form general templates,
we only keep captions that occur across multiple object
categories. After filtering out incomplete utterances and
those containing less than three words, as well as a manual
review, we retained 820 templates. Each of these templates
can be used with any category name without relying on the
context of the object.

3.2. Self-supervised learning of visual representa-

tions

In this work, we postulate that toddlers learn visual rep-
resentations that 1) slowly change over time and 2) align
with co-occurring linguistic representations. To model their
learning process, we consider two previously introduced
self-supervised loss functions. The first loss aligns embed-
dings of close-in-time images; this entails that seeing an
object from different viewpoints elicits viewpoint invariant
object representations [25]. The second loss function aligns
embeddings of co-occurring images and naming utterances.
Since the naming utterance may inform about the category
of the object in focus, this provides weak supervision re-
garding the object category. To implement our learning
mechanisms, we use SimCLR [5], a state-of-the-art con-
trastive learning algorithm. We sum up the learning archi-
tecture in Fig. 1E.

Contrastive learning through time (CLTT) At each
learning iteration, we sample a mini-batch X that contains
N randomly sampled images xi. For each image xi, we
also randomly sample a single successor/predecessor xj be-
longing to the same “play session” as xi. Thus, xi and xj
always display the same object, but observed through differ-
ent orientations and positions. In addition, we expect that
the active and multi-modal manipulation of the object bi-
ases toddlers’ attention onto the main object [27, 39]; thus,
we apply on images, with probability 0.5, a center crop of
size ranging in [8; 100]% of the image size. Thereafter, we
compute embeddings of images z1 = g1(f (x)) using a fea-
ture extractor f and a projection head g1, both implemented

as neural networks [5]. Finally, for a pair (zi, zj), we mini-
mize

lT(z1

i , z1

j ) = − log

ecos(z1

i ,z1

j )/τ

(cid:80)

z∈Z 1,z̸=z1
i

ecos(z1

i ,z)/τ

,

(1)

where cos stands for the cosine similarity, Z 1 contains all
embeddings z1 and τ is the temperature hyper-parameter
[5]. The top part of (1) aims to move together image embed-
dings that belong to the same manipulation session while
the bottom part of (1) ensures that all image embeddings
remain dissimilar.

Multimodal contrastive learning For each sampled im-
age xi, we also sample, if provided by the caregiver, its co-
occurring naming utterance li. We extract the pre-trained
features of the naming utterances with a state-of-art text
embedding model h [8]. Then name=’adamw’,, we com-
pute different embeddings of images z2
i = g2(f (xi)) and
captions z3
i = g3(h(li)) using projection heads g2, g3 and
a text feature extractor h in order to minimize, for each pair
(z2

i , z3

i ):

lM(z2

i , z3

i ) − log

ecos(z2

i ,z3
i )/τ
ecos(z2

i ,z2

k)/τ

(cid:80)

z∈Z 2
z̸=z2
i

(2)

where Z 2 contains the provided embeddings z2 and z3. The
top part of (2) ensures that co-occurring naming utterances
and images have similar embeddings while the bottom part
prevents all embeddings to collapse into a single vector.

The total loss function for a batch of size N is the sym-

metric and batch-wise sum of (1) and (2):

1
2N

N
(cid:88)

i=1

lM(z3

i , z2

i ) + lM(z2

i , z3

i ) + lT(z1

i , z1

j ) + lT(z1

j , z1

i ).

3.3. Developmentally-relevant utterance statistics

To model developmentally-relevant utterance statistics,
we extract statistics reported in at-home studies of toddlers’
dyadic play sessions [26, 31]. First, [26] reports that 56%
of naming events match the visual input of toddlers. Thus,
we define the (conditional) probability of naming the ma-
nipulated object (versus another object in the background)
as pcorrect = 0.5. Second, a caregiver approximately pro-
vides, on average, one naming utterance related to the ob-
ject being manipulated per time-extended object manipula-
tion [31] (exactly 2.54
2 = 1.27 in their study). Since our
play sessions last for 20 frames, we approximate the prob-
ability of naming the manipulated object during a frame as
pname = 1
20 = 0.05. We assume that consecutive frames
are spaced by one second, making the duration of our play
sessions correspond to the average of 20 seconds reported
in [31]. Finally, it allows us to define the probability of
naming any object in a frame psparse = pname
pcorrect

= 0.1.

4

3.4. Training and evaluation

Training We use a ResNet18 [16] as vision encoder f
and, for all projection heads, a fully connected neural net-
work with one hidden layer of size 256 followed by batch
normalization and ReLU activation. For encoding the text,
we use a pre-trained BERT [8] with 4 layers, 8 self-attention
heads and a hidden size of 512, introduced as BERT-small
by [34]. We train our models for 50 epochs with the
AdamW optimizer, a learning rate of 0.001 and weight
decay of 0.01. We tested a temperature hyper-parameter
τ ∈ {0.07, 0.1, 0.5} and found 0.07 to be the best. The ra-
tionale behind using a pre-trained text model lies in our re-
search focus on visual representation learning. Preliminary
experiments (reported in experiments Section 4.1), demon-
strate comparable recognition performance, although with a
longer convergence time when training a randomly initial-
ized text model concurrently with visual encoding.

Evaluation To evaluate if the learned representation sup-
ports category recognition, we apply a repeated random
sub-sampling cross-validation to split the non-overlapping
object instances into 2382 train (2/3 of the total) and 1191
test objects (1/3 of the total). Then, we extract images of
train and test objects to form the train and test datasets,
respectively (see Section 3.1). To evaluate our representa-
tion with respect to object instance recognition, we extract
novel images of the train objects to build a test dataset. For
both object instance and category recognition, we freeze the
weights of the vision encoder and train a linear classifier in
an online and supervised fashion on top of the latent visual
representation [4].

4. Experiments & Results

We first study whether developmentally-relevant utter-
ance statistics support learning semantic visual representa-
tions. Then, we analyze the impact of the sparsity, ambigu-
ity and the attention paid to the object’s name on the learn-
ing process.

4.1. Developmentally relevant utterance statistics

improve object representations

To assess the impact of plausible utterance statistics on
the visual representation, Fig. 2 compares such statistics
with two upper-bound baselines: Oracle refers to super-
vised learning and Ideal refers to a caregiver who always
utters sentences containing the correct object. Our base-
line (None) does not use any utterances relying solely on
the time-contrastive loss for visual representations. We ob-
serve that our model improves category recognition over
not using naming utterances, even though it does not reach
the performance of oracles. Additionally, we examine how
plausible utterance statistics perform when used to train the

Figure 2. A) Category recognition accuracy and B) object instance
recognition accuracy for different settings. Oracle represents su-
pervised learning, while an Ideal caregiver consistently names
the correct object. Plausible stands for developmentally-
relevant utterance statistics, Plausible* is identical but trains
the text encoder from scratch.

Figure 3.
t-SNE visualization of the feature representations ex-
tracted by the vision-encoder in different training settings. For
better visualization, we show a random subgroup of all classes.

text representations jointly with the visual representations
from scratch over 100 epochs. The model yields similar out-
comes, albeit necessitating significantly more training time,
as it has not yet reached convergence. We employ t-SNE
visualization to validate our learned output feature repre-
sentations. Fig. 3 shows that the models trained with ut-
terances exhibit better separation than without textual guid-
ance (baseline). We conclude that developmentally-relevant
imperfect naming utterances can help to build semantic vi-
sual representation.

4.2. Small changes in utterance frequency and am-
biguity drastically impact object representa-
tions

To understand how frequency and ambiguity of utter-
ances affect object representations, we systematically vary
both factors in Fig. 4 and Fig. 5, respectively. We observe
that developmentally-relevant values (red points) are close
to the steepest point of the function, where small shifts have
a high impact on the quality of the learned representation.
This suggests that toddler’s object learning may be quite
sensitive to caregivers’ presence and the quality of their ut-
terances. We also notice worse instance recognition with
very few utterances in comparison to none. We suspect that
these rare and ambiguous utterances mostly inject noise into
the representation.

5

OracleIdealPlausiblePlausible*NoneUtterance statistics0.00.10.20.30.40.5AccuracyCategoryOracleIdealPlausiblePlausible*NoneUtterance statistics0.00.20.40.60.8AccuracyABInstanceFigure 4. Analysis of the impact of the sparsity parameters on A)
category recognition and B) object instance recognition. The red
points indicate the developmentally-relevant value.

Figure 5. Analysis of the impact of naming ambiguity on A) cate-
gory recognition and B) instance recognition. A high pcorrect im-
plies low ambiguity. The red points indicate the developmentally-
relevant value.

Figure 6. Analysis of the effect of the sparsity parameter and the
correct naming probability pcorrect on the textual representations.
The heatmaps show the cosine similarity between the text embed-
dings of utterances from two random classes. The first element of
each class is the raw category name.

the visual representation. When considered alongside the
accuracy values of classification experiments, this observa-
tion strongly suggests that increased textual attention to the
object name is crucial for learning object representations.

5. Conclusion

We proposed a computational model of the development
of toddlers object representations during dyadic play and in-
vestigated if and how caregivers’ utterances affect learned
visual representations. We found that realistic utterance
statistics elicit more semantic visual representations and the
quality of these representations is sensitive to small shifts in
the statistics of utterances. Our analysis also revealed that
realistic statistics of utterances lead the model to focus on
the object name within an utterance, suggesting that visual
and language representations reciprocally affect each other.
The main limitation of our work is the gap between the
actual experiences of toddlers and our simplified dataset.
For instance, our “play sessions” are short and discon-
nected, as there is no notion of temporal proximity between
different sessions and our captions represent a small sub-
set of the space of possible utterances. In addition, hands,
caregivers’ visual cues and the touch modality have been
completely ignored. In the future, we plan to validate our
findings on a dataset of real-world videos extracted from
head-mounted cameras during dyadic play.

ACKNOWLEDGMENT

We gratefully acknowledge support from GENCI–IDRIS
(Grant 2022-AD011014008) for providing computing and
data-processing resources needed for this work. Addi-
tional support was received by the Deutsche Forschungsge-
meinschaft (DFG project 5368 “Abstract REpresentations
in Neural Architectures (ARENA)”), as well as the project
“The Adaptive Mind” funded by the Excellence Program
of the Hessian Ministry of Higher Education, Science, Re-
search and Art (HMWK). Jochen Triesch was supported by
the Johanna Quandt foundation.

4.3. Attending to object names is crucial for learning

good visual representations

References

To assess the importance of attention in acquiring seman-
tic visual representations, we present in Fig. 6 a comparison
of text embeddings across various utterances. Even with the
infrequent application of the cross-modal loss, the model
demonstrates the capability to group utterances sharing the
same category name, clearly separating them from other ut-
terance classes (as illustrated by the blocks in Fig. 6). In-
creasing the degree of textual guidance by small increments
yields improved class separation. In addition, more good
naming utterances amplify linguistic attention on the name
of the object being held, thereby allowing to better guide

[1] Arthur Aubret, Markus R. Ernst, C´eline Teuli`ere, and Jochen
Triesch. Time to augment self-supervised visual representa-
tion learning. In The Eleventh International Conference on
Learning Representations, 2023. 3

[2] Arthur Aubret, C´eline Teuli`ere, and Jochen Triesch. Toddler-
inspired learning induces hierarchical object representations.
In IEEE ICDL-Sensorimotor Interaction, language, and Em-
bodiement of Symbols (SMILES) workshop, 2022. 3

[3] Sven Bambach, David J Crandall, Linda B Smith, and Chen
Yu. Active viewing in toddlers facilitates visual object learn-
ing: An egocentric vision approach. In CogSci, 2016. 1, 2,
3

6

0.00.20.40.60.81.0Sparsity0.2250.2500.2750.3000.3250.350AccuracyCategory0.00.20.40.60.81.0Sparsity0.40.50.60.7AccuracyInstance0.00.20.40.60.81.0pcorrect0.200.250.300.350.40AccuracyCategory0.00.20.40.60.81.0pcorrect0.500.550.600.650.700.75AccuracyInstance01234567891011121314150 - saw1 - looks like a saw to me2 - give Mommy your saw3 - what's that saw4 - that looks like a saw5 - a saw I think6 - that's our saw7 - oh you found a saw8 - fish9 - looks like a fish to me10 - give Mommy your fish11 - what's that fish12 - that looks like a fish13 - a fish I think14 - that's our fish15 - oh you found a fishSparsity = 0.1 & pcorrect=0.50123456789101112131415Sparsity = 0.01& pcorrect=0.50.00.20.40.60.81.0Cosine Similarity[4] Florian Bordes, Randall Balestriero, and Pascal Vincent. To-
wards democratizing joint-embedding self-supervised learn-
ing. arXiv preprint arXiv:2303.01986, 2023. 5

[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning, pages 1597–1607. PMLR, 2020. 4

[6] Elizabeth M Clerkin and Linda B Smith. Real-world statis-
tics at two timescales and a mechanism for infant learning
of object names. Proceedings of the National Academy of
Sciences, 119(18):e2123239119, 2022. 1

[7] Karan Desai and Justin Johnson. Virtex: Learning visual
representations from textual annotations. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 11162–11173, 2021. 3

[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
2019 ACL, pages 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. 4, 5
[9] Gil Diesendruck and Paul Bloom. How specific is the shape

bias? Child development, 74(1):168–178, 2003. 1

[10] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Gir-
shick, and Kaiming He. A large-scale study on unsupervised
In Proceedings of
spatiotemporal representation learning.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3299–3309, 2021. 3

[11] Mathias Franzius, Niko Wilbert, and Laurenz Wiskott. In-
variant object recognition and pose estimation with slow fea-
ture analysis. Neural computation, 23(9):2289–2323, 2011.
3

[12] Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca,
Martin Schrimpf, James Traer, Julian De Freitas, Jonas Ku-
bilius, Abhishek Bhandwaldar, Nick Haber, et al. Threed-
world: A platform for interactive multi-modal physical sim-
ulation. arXiv preprint arXiv:2007.04954, 2020. 3

[13] Lisa Gershkoff-Stowe and Linda B Smith. Shape and the
first hundred nouns. Child development, 75(4):1098–1114,
2004. 1

[14] Alison Gopnik and Andrew Meltzoff. The development of
categorization in the second year and its relation to other
cognitive and linguistic developments. Child development,
pages 1523–1531, 1987. 1

[15] David Harwath, Adria Recasens, D´ıdac Sur´ıs, Galen
Chuang, Antonio Torralba, and James Glass.
Jointly dis-
covering visual objects and spoken words from raw sensory
input. In Proceedings of the European conference on com-
puter vision (ECCV), pages 649–665, 2018. 3

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 5

[17] Susan S Jones. Late talkers show no shape bias in a novel
name extension task. Developmental Science, 6(5):477–483,
2003. 1

using an operant learning procedure.
Development, 29(1):11–23, 2006. 1

Infant Behavior and

[19] Nuo Li and James J DiCarlo. Unsupervised natural visual
experience rapidly reshapes size-invariant object representa-
tion in inferior temporal cortex. Neuron, 67(6):1062–1075,
2010. 1

[20] Brian MacWhinney and Catherine Snow. The child language
data exchange system. Journal of child language, 12(2):271–
295, 1985. 2, 4

[21] Deborah G Kemler Nelson, Anne Frankenfield, Catherine
Morris, and Elizabeth Blair. Young children’s use of func-
tional information to categorize artifacts: Three factors that
matter. Cognition, 77(2):133–168, 2000. 1

[22] Emin Orhan, Vaibhav Gupta, and Brenden M Lake. Self-
In H.
supervised learning through the eyes of a child.
Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems,
volume 33, pages 9960–9971. Curran Associates, Inc., 2020.
2

[23] Alfredo F Pereira, Karin H James, Susan S Jones, and
Linda B Smith. Early biases and developmental changes in
self-generated object views. Journal of vision, 10(11):22–22,
2010. 3

[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PMLR, 2021. 3

[25] Felix Schneider, Xia Xu, Markus R Ernst, Zhengyang Yu,
and Jochen Triesch. Contrastive learning through time. In
SVRHM 2021 Workshop@ NeurIPS, 2021. 3, 4

[26] Sara E Schroer, Ryan E Peters, Alyssa Yarbrough, and Chen
Yu. Visual attention and language exposure during every-
day activities: an at-home study of early word learning using
wearable eye trackers. In Proceedings of the Annual Meeting
of the Cognitive Science Society, volume 44, 2022. 1, 4
[27] Sara E Schroer and Chen Yu. Looking is not enough:
Multimodal attention supports the real-time learning of new
words. Developmental Science, 26(2):e13290, 2023. 1, 4
[28] Linda B Smith, Susan S Jones, Barbara Landau, Lisa
Gershkoff-Stowe, and Larissa Samuelson. Object name
learning provides on-the-job training for attention. Psycho-
logical science, 13(1):13–19, 2002. 1

[29] Kasey C Soska and Scott P Johnson. Development of three-
dimensional object completion in infancy. Child develop-
ment, 79(5):1230–1236, 2008. 1

[30] Stefan Stojanov, Anh Thai, and James M. Rehg. Using shape
to categorize: Low-shot learning with an explicit shape bias.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1798–1808,
June 2021. 2, 3, 4

[31] Catalina Suarez-Rivera, Jacob L Schatz, Orit Herzberg, and
Catherine S Tamis-LeMonda. Joint engagement in the home
environment is frequent, multimodal, timely, and structured.
Infancy, 27(2):232–254, 2022. 1, 4

[18] Kimberly S Kraebel and Peter C Gerhardstein. Three-month-
old infants’ object recognition across changes in viewpoint

[32] Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica Wo-
jcik, and Michael C Frank. Saycam: A large, longitudinal

7

audiovisual dataset recorded from the infant’s perspective.
Open mind, 5:20–29, 2021. 2

[33] Satoshi Tsutsui, Arjun Chandrasekaran, Md Alimoor Reza,
David Crandall, and Chen Yu. A computational model of
early word learning from the infant’s point of view. arXiv
preprint arXiv:2006.02802, 2020. 2

[34] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Well-read students learn better: On the impor-
tance of pre-training compact models, 2019. 5

[35] Wai Keen Vong and Brenden M Lake. Cross-situational word
learning with multimodal neural networks. Cognitive sci-
ence, 46(4):e13122, 2022. 2

[36] Wai Keen Vong, Emin Orhan, and Brenden Lake. Cross-
situational word learning from naturalistic headcam data.
In 34th CUNY Conference on Human Sentence Processing,
2021. 2

[37] Laurenz Wiskott and Terrence J Sejnowski. Slow feature
analysis: Unsupervised learning of invariances. Neural com-
putation, 14(4):715–770, 2002. 1

[38] Justin N Wood and Samantha MW Wood. The develop-
ment of invariant object recognition requires visual expe-
rience with temporally smooth objects. Cognitive Science,
42(4):1391–1406, 2018. 1

[39] Jane Yang, Linda Smith, David Crandall, and Chen Yu. Us-
ing manual actions to create visual saliency: an outside-in
In Pro-
solution to sustained attention and joint attention.
ceedings of the Annual Meeting of the Cognitive Science So-
ciety, 2023. 1, 4

[40] Chen Yu, Yayun Zhang, Lauren K Slone, and Linda B Smith.
The infant’s view redefines the problem of referential uncer-
tainty in early word learning. Proceedings of the National
Academy of Sciences, 118(52):e2107019118, 2021. 1

8

