The CONCATENATE workshop at the ACM/IEEE International Conference on Human-Robot Interaction 2023

How much informative is your XAI? A decision-making assessment task to
objectively measure the goodness of explanations

Marco Matarese1,2

Francesco Rea2 Alessandra Sciutti2
1 University of Genoa
2 Italian Institute of Technology
marco.matarese, francesco.rea, alessandra.sciutti@iit.it

3
2
0
2
c
e
D
7

]
I

A
.
s
c
[

1
v
9
7
3
4
0
.
2
1
3
2
:
v
i
X
r
a

Abstract

There is an increasing consensus about the effectiveness of
user-centred approaches in the explainable artificial intelli-
gence (XAI) field. Indeed, the number and complexity of per-
sonalised and user-centred approaches to XAI have rapidly
grown in recent years. Often, these works have a two-fold ob-
jective: (1) proposing novel XAI techniques able to consider
the users and (2) assessing the goodness of such techniques
with respect to others. From these new works, it emerged
that user-centred approaches to XAI positively affect the in-
teraction between users and systems. However, so far, the
goodness of XAI systems has been measured through indi-
rect measures, such as performance. In this paper, we propose
an assessment task to objectively and quantitatively measure
the goodness of XAI systems in terms of their information
power, which we intended as the amount of information the
system provides to the users during the interaction. Moreover,
we plan to use our task to objectively compare two XAI tech-
niques in a human-robot decision-making task to understand
deeper whether user-centred approaches are more informative
than classical ones.

Introduction
The number and complexity of user-centred approaches to
XAI have been increasing in recent years (Williams 2021).
The application contexts of such approaches are various.
They go from computer applications aiming at providing
personalised teaching (Embarak 2022)(Cohausz 2022) to
human-robot interaction (HRI) contexts in which the robot
maintains users’ models to provide explanations tailored to
them (Matarese, Rea, and Sciutti 2021)(Stange et al. 2022).
These works show a clear trend: user-centred XAI posi-
tively affects the interaction between users and systems. It
brings to higher users’ willingness to reuse the system (e.g.,
with recommendation systems (Conati et al. 2021)), robots’
persuasiveness during human-robot decision-making tasks
and human-AI teams performance (Schemmer et al. 2022b).
One of the reasons why the XAI research field is mov-
ing toward user-centred approaches is to improve the good-
ness of the explanations. Alongside producing personalised
XAI, recent works also aim to evaluate the goodness of the
explanation produced. What is meant for “XAI goodness”
is still under debate; nonetheless, there is a broad consen-
sus about the dependency of this concept on the applica-

tion context. Unfortunately, most of these evaluations rely
on subjective or indirect measurements (e.g., questionnaires
or performance).

However, the main aim of an XAI system is to provide the
user with information about the functioning of the underly-
ing AI model. So far, the amount of information an XAI sys-
tem provides has been assessed through indirect measures,
such as users’ ability to simulate the AI behaviour. Indeed,
recent surveys (Mohseni, Zarei, and Ragan 2018) and sys-
tematic reviews (Nauta et al. 2022) highlight the need for
more objective and quantitative measures to assess the good-
ness of XAI techniques.

In our opinion, it is worth taking a step back and mea-
suring the goodness of XAI systems from the information
they can provide to users. Although measuring how much
information a system can generate could be challenging, we
can focus on the amount of new knowledge that such flow
of information creates in the users’ mental models. Hence,
if we let only non-expert users interact with XAI systems,
we can state that the knowledge they acquired arose from
the interaction with the system. Finally, if such knowledge
is quantifiable, we can assess how much the system has been
informative.

In this paper, we aim to propose an assessment task to
objectively and quantitatively measure the goodness of XAI
systems during human-AI decision-making tasks intended
as how much they are informative to non-expert users. More-
over, we plan to use such a task to study the effectiveness
of user-centred XAI in HRI scenarios. In particular, we are
interested in understanding whether a user-centred XAI ap-
proach is more informative than a classical one.

Related work
Several works assessed XAI systems’ properties through
user studies. The more interesting ones, in our opinion, are
addressed to non-expert users (Janssen et al. 2022). These
latter used tasks that only some people know about. For ex-
ample, authors in (Lage et al. 2019) used two application
domains: an artificial alien’s food preferences based on the
meals’ recipes, and clinical diagnosis. Those in (Wang and
Yin 2021) also used two peculiar decision-making tasks: a
recidivism prediction and a forest cover prediction task. Fur-
ther, authors in (van der Waa et al. 2021) used a diabetes
self-management use-case where naive users and the system

1

 
 
 
 
 
 
had to find the optimal insulin doses for meals. Finally, both
(Goyal et al. 2019) and (Wang and Vasconcelos 2020) used
image classification tasks in which participants were asked
to recognise two bird species.

Several works regarding the assessment or comparison of
XAI methods tend to define their own measure of good-
ness (van der Waa et al. 2021)(Lage et al. 2019). However,
a method has been proposed to objectively measure the de-
gree of explainability of the information provided by an XAI
system (Sovrano and Vitali 2022). Moreover, the authors in
(Holzinger, Carrington, and M¨uller 2020) proposed the Sys-
tem Causability Scale to measure the quality of the expla-
nations based on their notion of causability (Holzinger et al.
2019). Finally, the authors in (Wang and Yin 2022) proposed
a different point of view regarding assessing XAI systems’
goodness by comparing several types of XAI in different
application contexts with respect to three desiderata: to im-
prove people’s understanding of the AI model, help people
recognise the model uncertainty, and support people’s cali-
brated trust in the model.

Recent works regarding user-centredness focus on users’
trust towards the system and highlight context-awareness
and personalisation as main approaches to user-centredness
(Williams 2021). Personalisation in XAI has also been im-
plemented by exploiting the users’ personality traits and cor-
relating them with users’ preferences or behaviours (B¨ockle,
Yeboah-Antwi, and Kouris 2021)(Martijn, Conati, and Ver-
bert 2022). Instead, authors in (Bertrand et al. 2022) re-
viewed a relevant corpus of literature to understand which
human biases researchers reflect in their XAI methods (also
without noticing).

Most of the works in the XAI field with user studies re-
gard decision-making (Wang and Yin 2021) or classification
tasks (Goyal et al. 2019). The rationale behind this choice
is the promise of better performance when coupling human
users with expert AI systems (Wang and Yin 2022). Such a
promise is almost always kept, although a few studies show
that team performance decreases when using some form of
XAI (Schemmer et al. 2022b). Indeed, some authors high-
lighted that AI advice is not always beneficial mainly be-
cause humans have shown to be unable to ignore incorrect
AI advice (Schemmer et al. 2022a)(Ferreira and Monteiro
2021)(Janssen et al. 2022).

Motivation and hypotheses
So far, researchers have measured the goodness of an XAI
system used in human-AI decision-making tasks by relying
on indirect measures, such as team performance, systems’
persuasiveness, or users’ ability to predict the AI’s decisions.
This means that no objective and quantitative measures have
been used to state how good XAI systems are per se. This
lack contributes to the impossibility of rigorously comparing
two XAI strategies regardless of their particular application
context. Since all the XAI systems have been examined with
respect to their application scenario, it is also hard to gener-
alise the results of such research.

We need to introduce an objective and quantitative assess-
ment task to measure one of the most-ignored-so-far fac-
tors of XAI: its information power. With the term informa-

Figure 1: The co-constructing approach to the explaining
process, as in (Rohlfing et al. 2020). The actors’ explana-
tion behaviours adapt by monitoring and scaffolding each
other. In our setting, the ER monitors the EE through feed-
back about their actions and questions.

tion power, we mean the amount of information that an XAI
system provides about: (1) underlying AI models’ (general)
functioning, (2) reasons behind a particular model’s choice,
or (3) what the system would do in other circumstances.

Under the assumption that the goodness of an XAI sys-
tem reflects the accuracy of the users’ mental models about
the underlying AI system (Hoffman et al. 2018), we want
to understand deeper why user-centred XAI is more effec-
tive than those approaches that do not take the users in con-
sideration. Our experimental hypothesis regards the users’
level of involvement in the explanation generation process.
In particular, the more users’ are involved in the explanation
generation process:

H1 the more the XAI system has more information power
(resulting in more accurate users’ mental models of the
AI).

H2 the more they become independent of the robot’s sugges-

tions and explanations.

H3 the higher the users’ willingness to reuse the XAI system.
H4 the more positive the users’ feelings toward the robot.

Methods
We are interested in introducing an objective and quantita-
tive assessment task to measure the goodness of XAI sys-
tems without relying only on indirect measures. Hence, we
plan to measure the XAI systems’ information power di-
rectly and validate the task through a user study.

The task
The assessment consists of a decision-making task where
users can interact with a control panel to perform actions
in a simulated environment (see Section ). During the task,
users can interact with an expert AI (in our case, with the

2

Figure 2: The schema of a pressurised water reactor that we implemented in our simulated environment. This schema considers
only the control rods, but we also allow the users to manage the other three types of rods: the fuel, sustain and regulatory ones.

humanoid robot iCub) by asking what it would do, and its
XAI system by asking why it would do that. Apart from the
instructions about interacting with the control panel and the
robot, users start the task knowing nothing about it.

In a fixed amount of time, the users have to perform ac-
tions and interact with the robot to discover (1) which is the
task at hand (e.g., what is the goal of the task), as well as
(2) the rules that the AI model uses to make its actions and,
since the robot is an expert agent, (3) the rules that govern
the simulated environment.

Interaction modalities The roles between the human-
robot team and their interaction modalities are simple. The
robot can not perform actions, but its role is limited to assist-
ing users during decision-making. However, the robot can
not take the initiative in giving suggestions either, but it al-
ways replies to users’ questions. Thus, only the users can
interact with the control panel and act in the simulated envi-
ronment.

We expect the following HRI modalities. If the user al-
ready knows what to do, they act on the control panel. Oth-
erwise, they ask the robot what it would do; then, the robot
answers this what question saying what action it would do in
the current scenario. If the user understands the robot’s rea-
sons, they act on the control panel (not necessarily the sug-
gested one). Otherwise, the user asks the robot why it would
do that. At this point, the robot answers this why question by
explaining as justification for its suggestion. All that remains
for the user is to act on the control panel.

Characteristics of the task We need non-expert users to
consider the information passed through the interaction as
new information. Taking it to extremes, we consider only
participants without knowledge about the task and its under-
lying rules. For this reason, we implemented a simulation
of a nuclear power plant management task (Figure 2). We
chose this kind of task because it met all the requirements

we needed: it is challenging and captivating for non-expert
users, simple rules govern it, an AI model can learn those
rules and, usually, people know nothing about the function-
ing of nuclear power plants.

The main objectives of the task (which we hide from
users) are to generate as much energy as possible and main-
tain the system in an equilibrium state. The features of the
environment are subject to rules and constraints, which we
can summarise as follow:

· Each action corresponds to an effect on the environment:

thus, a change of its features’ value.

· Several preconditions must be satisfied to start and con-

tinue nuclear fission and produce energy.

· Some conditions irremediably damage the plant.
· The task is divided into steps of fixed time duration (e.g.,
10 seconds), in which the users can interact with either
the robot or the control panel.

The simulated environment
Features of the environment Our simulated power plant
is composed of 4 continuous features:
· The pressure in the reactor’s core.
· The temperature of the water in the reactor
· The amount of water in the steam generator
· The reactor’s power.

Furthermore, the power plant has four other discrete features
that regard the reactor’s rods: security rods, fuel rods, sustain
rods, and regulatory rods. The firsts two have two levels:
up and down. Instead, the latter two have three levels: up,
medium, and down.

The reactor power linearly decreases over time for the ef-
fect of the de-potentiation of the fuel rods. Hence, the reac-
tor’s power depends on the values of the environment’s fea-
tures and whether nuclear fission is taking place. Moreover,

3

the energy produced at each step is computed by dividing the
reactor’s power by 360, which is the power that a 1000MW
reactor without power dispersion produces in 10 seconds.

Actions to perform on the environment The actions that
the user can perform (12 in total) go from changing the po-
sition of the rods to adding water to the steam generator or
skipping to the next step. All those actions change the value
of 3 parameters - T , P , and L - which correspond to the
water’s temperature in the core, the core’s pressure, and the
water level in the steam generator, respectively. The setting
of the rods determines the entity of the feature updates; such
updates are performed at the end of each step, right after
the users’ action. For example, if the safety rods are lowered
in the reactor’s core, the nuclear fission stops; thus, T and P
decrease until they reach their initial values, and the water in
the steam generator remains still. On the other hand, if nu-
clear fission occurs and the user lowers the regulatory rods,
the fission accelerates. This acceleration consumes more wa-
ter in the steam generator, raising the core’s temperature and
pressure more quickly, but also raising the reactor’s power
and the electricity produced. If the users do not act within
the time provided for each step, the application automati-
cally chooses a skip action, which applies the features’ up-
dates based on the setting of the rods at hand.

The robot’s AI
Regarding the robot’s AI model, we trained a deterministic
decision tree (DT) using the Conservative Q-Improvement
learning algorithm (Roth et al. 2019), which allowed us to
train the DT using a reinforcement learning strategy. In-
stead of extracting the DT from a more complex ML model
(Vasilev, Mincheva, and Nikolov 2020)(Xiong, Zhang, and
Zhu 2017), we used this learning strategy to simplify the
translation from the AI to the XAI without losing perfor-
mance. The robot uses this expert DT to choose its action: it
can perform each of the 12 actions based on the eight envi-
ronment’s features.

Starting from its root node, the DT is queried on each of
its internal nodes - which represent binary splits - to decide
which of the two sub-trees continue the descent. Each inter-
nal node regards a feature xi and a value for that feature vi:
the left sub-tree contains instances with values of xi ≤ vi,
while the right sub-tree contains instances with values of
xi > vi (Buhrman and de Wolf 2002).

The DT’s leaf nodes represent actions; in the implementa-
tion of Roth et al. (Roth et al. 2019), they are defined with an
array containing the actions’ expected Q-values: the greater
Q-value is associated with the most valuable action. This
way, the DT can be queried by users with both what and why
questions. To answer a what question, we only need to nav-
igate the DT using the current values of the environment’s
features and present the resulting action to the user.

The robot’s XAI
As we already saw in Section , answering why questions re-
gards the robot XAI. Since the AI model to explain is already
transparent (Adadi and Berrada 2018), it also constitutes the
XAI model. We can use DTs to provide explanations simply

Figure 3: An example of DT where the leaf nodes 5 and 7
are the robot’s suggestion and the predicted user’s action, re-
spectively. Let us assume that node 1 has already been used:
then, the classical XAI selects node 2 for the explanations
since it is the most unused relevant node. The user-aware
XAI selects instead node 3 because it represents a perfect
counterfactual explanation for fact 5 and foil 7.

by using one (or more) of the feature values we encounter
during the descent.

As we have seen in Section , during the DT descent, we
encounter a set of split nodes defined by a feature xi and a
value vi; the direction of the descent tells us if the current
scenario has a value of xi ≤ vi or xi > vi. Each of those
inequalities can be used to provide an explanation that can
help user to relate actions with specific values of the envi-
ronment’s features. In our case, an explanation for the ac-
tion ”add water to the steam generator” could be ”because
the water level in the steam generator is ≤ 25, which is dan-
gerously low.

Which of the features we encounter during descent to use
is a problem called “explanation selection”. In our case, the
explanation selection depends on the XAI strategy we want
to test. For example, classical approaches use the most rel-
evant features (in terms of the Gini index, information gain,
or other well-established measures (Stoffel and Raileanu
2001)). We plan to objectively and quantitatively compare
two XAI strategies at first glance: a classical approach vs an
user-aware one.

The classical XAI explains using only the AI outcomes
and environment’s states. In particular, it justifies the robot’s
suggestions by using the most relevant features, which are
the first ones in the DT’s structure (see (Roth et al. 2019)).
The system tries to give always different explanations by
taking track of the DT’s node already used and preferring
to use the others in decreasing the level of relevance.

On the other hand, the user-aware XAI approach, through
monitoring and scaffolding (Figure 1), maintains a model of
the users to take track of their previous actions and predict
their future behaviour (Rohlfing et al. 2020). Hence, the pre-
dicted users’ actions can be used to produce counterfactual
explanations: the fact (the outcome to explain) will be the
robot’s suggestion; in contrast, the foil (the expected out-

4

come) will be the predicted users’ action. Figure 3 shows an
example of these two approaches.

Measuring the explanations’ goodness
The assessment of the model’s information power involves
the interaction between non-expert users and the system it-
self. During this assessment, the users aim to learn as many
system rules as possible. Thus, we need to collect measures
for each rule and combine them to obtain the model’s infor-
mation power. The general assessment steps are the follow-
ing:
1. To quantify how many rules regard each feature and de-
fine a method for measuring the number of learned rules
relative to each feature.

2. To quantify how much informative weight each fea-
ture has. For the sake of simplicity, we can assume
that they have the same informative weight (equal to 1
k ,
where k is the number of features). The features’ infor-
mative weights must respect the following rule: ∀j ∈
{1, ..., k}, (cid:80)
j γj = 1. The features’ informative weight
is intended to describe how difficult it is to understand
the rules regarding such features.

3. To measure the model’s informative power for each user,
obtaining a measure (and a set of secondary descriptive
measures) for each of them.

4. To average those measures obtaining the final results; the
secondary descriptive measures could also have valuable
meaning.
Hence, if k ∈ N is the number of the model’s features,
j ∈ N
γj ∈ [0, 1] is the informative weight of the feature j, nr
is the number of rules regarding the feature j, nlr(i)
∈ N is
the number of rules regarding the feature j learned by the
user i, and am is the accuracy of the AI model m, then the
informative power of the model m for the user i is computed
as follows:

j

IPi(m) = am

(cid:33)

k
(cid:88)

j=1

γj

(cid:32) nlr(i)
j
nr
j

∈ [0, 1].

(1)

Thus, if np is the number of users who took part in the as-
sessment, the information power of the model m is

IP (m) =

1
np

np
(cid:88)

i=1

IPi(m) ∈ [0, 1].

(2)

Apart from the number of rules regarding each feature,
the most delicate aspect of the assessment regards the def-
inition of the features’ information weights. We suggest at
least two ways to set them: to make them all equal or to
define the weights using experimental data. The former ap-
proach depends on the task at hand. For our task (Section ),
we set the features’ information weight by normalising the
number of interactions with the system that users needed to
understand those features. To state about which feature the
interaction regarded, we plan to proceed as follows: if the
interaction stopped at the request for suggestions, then the
interaction regards the feature on which the action suggested

effects; otherwise, if the interaction continues with a request
for an explanation, then the interaction regards the feature
contained into the explanation.

Experimental measures During our task, we plan to col-
lect several quantitative measures to compute the model’s
information power as defined above:

(M1) Measures of performance, such as the users’ final score.
(M2) Measures of rules understanding, such as the number of
task rules learned, the number of requests and interac-
tions users needed to learn such rules, and the number of
correct answers to the post-experiment test.

(M3) Measures of generalisation, such as the number of correct
answers to what-if questions about the robot’s decisions
in particular states of the environment.

Moreover, we collect some subjective measures:

(M4) Measures of satisfaction, such as users’ satisfaction level

about the explanations and the interaction.

(M5) Measures of robot perception, such as users’ feelings to-

wards the robot and perception of it.

Discussion
The assessment task we propose satisfies properties unique
to the XAI research field. Firstly, it focuses on the infor-
mation power of the XAI system intended as the amount
of information it can give to the user. Then, it defines the
goodness of the XAI system as a function of its information
power. Secondly, it allows for an objective and quantitative
analysis of such goodness.

The most critical requirement of our assessment task is
user interaction. It allows for a two-way, possibly iterative,
interaction with the users. In particular, it allows the users
to query the system by asking what it would do in a specific
situation and why. Consequently, the XAI system should be
able to answer both what and why questions to exploit the
full potential of the assessment task.

Another essential factor of our task is that we can easily
generalise it. To generalise our task, we need the following:
· A decision-making task with characteristics similar to the

ones listed in Section .

· An expert AI and several non-expert users.
· The approaches to compare could be XAI algorithms or

HRI dynamics.

· At least one quantitative measure about users’ under-
standing of the task: if two or more, a method to compact
them into a single measure is also needed.

· At least one quantitative measure about users’ ability to
generalise to unseen scenarios: if two or more, a method
to compact them into a single measure is also needed.
An assessment task with those characteristics is flexible
enough to test different AI models and XAI techniques as
long as they allow interaction between the user and the sys-
tem. Moreover, it can be used to objectively compare two
or more HRI approaches to test whether HRI dynamics ease
the users in understanding the robot’s suggestions and expla-
nations. Indeed, in our future work, we plan to use this task

5

to test whether interacting with a social robot makes users
more receptive to learning than interacting with a non-social
one.

References
Adadi, A.; and Berrada, M. 2018. Peeking inside the black-
box: a survey on explainable artificial intelligence (XAI).
IEEE access, 6: 52138–52160.
Bertrand, A.; Belloum, R.; Eagan, J. R.; and Maxwell, W.
2022. How cognitive biases affect XAI-assisted decision-
making: A systematic review. In Proceedings of the 2022
AAAI/ACM conference on AI, ethics, and society, 78–91.
B¨ockle, M.; Yeboah-Antwi, K.; and Kouris, I. 2021. Can
You Trust the Black Box? The Effect of Personality Traits on
Trust in AI-Enabled User Interfaces. In International Con-
ference on Human-Computer Interaction, 3–20. Springer.
Buhrman, H.; and de Wolf, R. 2002. Complexity measures
and decision tree complexity: a survey. Theoretical Com-
puter Science, 288(1): 21–43. Complexity and Logic.
Cohausz, L. 2022. Towards real interpretability of student
success prediction combining methods of XAI and social
science. Proceedings of the 15th International Conference
on Educational Data Mining, 361–367.
Conati, C.; Barral, O.; Putnam, V.; and Rieger, L. 2021. To-
ward personalized XAI: A case study in intelligent tutoring
systems. Artificial Intelligence, 298: 103503.
Embarak, O. H. 2022. Internet of Behaviour (IoB)-based AI
models for personalized smart education systems. Procedia
Computer Science, 203: 103–110.
Ferreira, J. J.; and Monteiro, M. 2021. The human-AI
relationship in decision-making: AI explanation to sup-
port people on justifying their decisions. arXiv preprint
arXiv:2102.05460.
Goyal, Y.; Wu, Z.; Ernst, J.; Batra, D.; Parikh, D.; and Lee, S.
2019. Counterfactual visual explanations. In International
Conference on Machine Learning, 2376–2384. PMLR.
Hoffman, R. R.; Mueller, S. T.; Klein, G.; and Litman, J.
2018. Metrics for explainable AI: Challenges and prospects.
arXiv preprint arXiv:1812.04608.
Holzinger, A.; Carrington, A.; and M¨uller, H. 2020. Measur-
ing the quality of explanations: the system causability scale
(SCS). KI-K¨unstliche Intelligenz, 34(2): 193–198.
Holzinger, A.; Langs, G.; Denk, H.; Zatloukal, K.; and
M¨uller, H. 2019. Causability and explainability of artificial
intelligence in medicine. Wiley Interdisciplinary Reviews:
Data Mining and Knowledge Discovery, 9(4): e1312.
Janssen, M.; Hartog, M.; Matheus, R.; Yi Ding, A.; and
Kuk, G. 2022. Will algorithms blind people? The effect
of explainable AI and decision-makers’ experience on AI-
supported decision-making in government. Social Science
Computer Review, 40(2): 478–493.
Lage, I.; Chen, E.; He, J.; Narayanan, M.; Kim, B.; Ger-
shman, S.; and Doshi-Velez, F. 2019. An evaluation of
the human-interpretability of explanation. arXiv preprint
arXiv:1902.00006.

Martijn, M.; Conati, C.; and Verbert, K. 2022. “Knowing
me, knowing you”: personalized explanations for a music
recommender system. User Modeling and User-Adapted In-
teraction, 32(1): 215–252.
Matarese, M.; Rea, F.; and Sciutti, A. 2021. A user-centred
framework for explainable artificial intelligence in human-
robot interaction. arXiv preprint arXiv:2109.12912.
Mohseni, S.; Zarei, N.; and Ragan, E. D. 2018. A survey of
evaluation methods and measures for interpretable machine
learning. arXiv preprint arXiv:1811.11839, 1.
Nauta, M.; Trienes, J.; Pathak, S.; Nguyen, E.; Peters, M.;
Schmitt, Y.; Schl¨otterer, J.; van Keulen, M.; and Seifert, C.
2022. From anecdotal evidence to quantitative evaluation
methods: A systematic review on evaluating explainable ai.
arXiv preprint arXiv:2201.08164.
Rohlfing, K. J.; Cimiano, P.; Scharlau, I.; Matzner, T.; Buhl,
H. M.; Buschmeier, H.; Esposito, E.; Grimminger, A.; Ham-
mer, B.; H¨ab-Umbach, R.; et al. 2020. Explanation as a so-
cial practice: Toward a conceptual framework for the social
design of AI systems. IEEE Transactions on Cognitive and
Developmental Systems, 13(3): 717–728.
Roth, A. M.; Topin, N.; Jamshidi, P.; and Veloso, M.
2019. Conservative q-improvement: Reinforcement learn-
ing for an interpretable decision-tree policy. arXiv preprint
arXiv:1907.01180.
Schemmer, M.; Hemmer, P.; K¨uhl, N.; Benz, C.; and
Should I Follow AI-based Advice?
Satzger, G. 2022a.
Measuring Appropriate Reliance in Human-AI Decision-
Making. arXiv preprint arXiv:2204.06916.
Schemmer, M.; Hemmer, P.; Nitsche, M.; K¨uhl, N.; and
V¨ossing, M. 2022b. A Meta-Analysis on the Utility of
Explainable Artificial Intelligence in Human-AI Decision-
Making. arXiv preprint arXiv:2205.05126.
Sovrano, F.; and Vitali, F. 2022. How to Quantify the Degree
of Explainability: Experiments and Practical Implications.
In 2022 IEEE International Conference on Fuzzy Systems
(FUZZ-IEEE), 1–9. IEEE.
Stange, S.; Hassan, T.; Schr¨oder, F.; Konkol, J.; and Kopp,
S. 2022. Self-Explaining Social Robots: An Explainable
Behavior Generation Architecture for Human-Robot Inter-
action. Frontiers in Artificial Intelligence, 87.
Stoffel, K.; and Raileanu, L. E. 2001. Selecting optimal
In Research and Devel-
split-functions for large datasets.
opment in Intelligent Systems XVII, 62–72. Springer.
van der Waa, J.; Nieuwburg, E.; Cremers, A.; and Neerincx,
M. 2021. Evaluating XAI: A comparison of rule-based and
example-based explanations. Artificial Intelligence, 291:
103404.
Vasilev, N.; Mincheva, Z.; and Nikolov, V. 2020. Decision
Tree Extraction using Trained Neural Network. In SMART-
GREENS, 194–200.
Wang, P.; and Vasconcelos, N. 2020. Scout: Self-aware dis-
In Proceedings of
criminant counterfactual explanations.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 8981–8990.

6

Wang, X.; and Yin, M. 2021. Are explanations helpful?
a comparative study of the effects of explanations in ai-
assisted decision-making. In 26th International Conference
on Intelligent User Interfaces, 318–328.
Wang, X.; and Yin, M. 2022. Effects of Explanations in
AI-Assisted Decision Making: Principles and Comparisons.
ACM Transactions on Interactive Intelligent Systems (TiiS).
Williams, O. 2021. Towards Human-Centred Explainable
AI: A Systematic Literature Review.
Xiong, Z.; Zhang, W.; and Zhu, W. 2017. Learning decision
In NIPS Workshop on
trees with reinforcement learning.
Meta-Learning.

7

