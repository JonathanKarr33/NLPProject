 reliability diagrams and confidence histograms. The results show that a high proportion of wrong predictions are of high confidence, indicating that the model is often confidently wrong. Additionally, the calibration degradation worsens as the model size increases or when fine-tuning occurs using specialized data. Temperature scaling, a commonly used calibration method, is not effective in addressing miscalibration in the in-context learning (ICL) setting. Furthermore, repeating the context without incorporating labels fails to yield improved performance in ICL, and diversity in prompt construction significantly impacts performance, particularly concerning larger language models.

Overall, the paper provides an in-depth evaluation and analysis of the calibration of language models, particularly in the context of in-context learning, and highlights the miscalibration issue and the limitations of existing calibration methods in addressing it. The findings underscore the need for further research to address the miscalibration issue of language models in the ICL setting and to improve their reliability and accuracy for safety-critical applications.