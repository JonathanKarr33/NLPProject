INTRODUCTION A good measure of our understanding of a complex system or process is our ability to rebuild it. In the context of human cognitive development this translates to constructing models of how the developing brain comes to control the developing body in increasingly sophisticated ways. Human cognitive development depends crucially on embodied interactions with the physical and social environment, that is sensed through our different sensory modalities. Faithful models of cognitive development therefore need to reproduce these interactions and how they give rise to sensory representations, motor skills, conceptual structures, and a broad range of cognitive abilities. D. Mattern is with the Department of Computer Science and Mathematics, Goethe-University Frankfurt, Frankfurt am Main, Germany. P. Schumacher is with the Max Planck Institute for Intelligent Systems, T¨ubingen, Germany and the Hertie Institute for Clinical Brain Research, T¨ubingen, Germany. F.M. L´opez, M. Raabe, M.R. Ernst, A. Aubret and J. Triesch are with the Frankfurt Institute for Advanced Studies, Frankfurt am Main, Germany. This research was supported by “The Adaptive Mind” and “The Third Wave of Artificial Intelligence” funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art. J. Triesch was supported by the Johanna Quandt foundation. P. Schumacher was supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS). This version of MIMo extends a previous version published at ICDL [1]. (a) Full body view (b) Facial expressions Fig. 1. MIMo, the multimodal infant model. (a) MIMo sitting in a room with toys. (b) Six facial expressions of MIMo. Importantly, these interactions are largely initiated and con- trolled by the developing child. For example, an infant controls its visual, proprioceptive, and haptic inputs through its eye and body movements. This ability to control and “experiment” with different movements to observe their effects on sensory inputs may be crucial for cognitive development, giving the developing mind a means to probe the causal structure of the world, rather than merely passively observing correlations among sensed variables. Importantly, such an active, self-controlled form of learning is in stark contrast to a large body of work in Artificial Intelligence (AI), including large language or vision models, which learn certain aspects of the statistical structure of large training datasets without having any control over their inputs. Despite the impressive successes of such models, they may ultimately be severely limited in their ability to understand the causal mechanisms underlying their training data, which will limit their ability to generalize in novel situations. This should come as no surprise. Every scientist learns that mere correlation is not sufficient evidence for inferring causation. Therefore, scientists routinely exploit the ability to manipulate a system under study using clever experimental designs where they interfere with some variables and observe the effects on others to distill the causal mechanisms at work. Therefore, recreating the physical interaction with the envi- ronment must be a central aspect of rebuilding cognitive devel- opment and it may be equally essential for achieving human- like intelligence and consciousness in AIs. The idea that an AI could learn like a developing child can be traced back all the way to Turing [2]. However, serious “Developmental Robotics” or “Developmental AI” efforts have become more common and practical only in the last 20 years, for reviews see [3]–[8]. There are two options for modeling the interaction of a developing mind with its physical and social environment. The first option is using humanoid robots. This is the approach taken by the Developmental Robotics community. The main advantage is the inherent realism: the system operates in the real world governed by the actual laws of physics. However, working with humanoid robots is expensive, time-consuming, and suffers from the brittleness of today’s humanoid hardware. All these factors negatively impact the reproducibility of the research. Furthermore, the sensing abilities of today’s robots are usually not comparable to those of actual humans. This is particularly problematic for the sense of touch. The human body is covered by a flexible skin containing various types of mechanoreceptors, thermoreceptors, and nociceptors (pain receptors). These allow us to sense touch, pressure, vibration, temperature, and pain. Today, reproducing such a human-like skin in humanoid robots is still out of reach. The second option for modeling the interaction of the developing mind with its physical and social environment is to do it completely in silico. Many physics simulators or game engines are available today that can approximate the physics of such interactions [9]–[11], for review see [12]. Among the disadvantages of such an approach are 1) inaccuracies of such simulations due to inevitable approximations and 2) the high computational costs, especially when non-rigid body parts and objects are considered. Nevertheless, the in silico approach avoids all the problems of working with humanoid robot hardware mentioned above. Furthermore, it ensures perfect reproducibility. Lastly, if simulations can be run (much) faster than real-time, this greatly facilitates the simulation of developmental processes unfolding over long periods of time (weeks, months, or even years). To support such in silico research, we here present the open source software platform MIMo, the Multi-Modal Infant Model (Fig. 1). MIMo is intended to support two kinds of research: 1) developing computational models of human cognitive development and 2) building developmental AIs that develop more human-like intelligence and consciousness by similarly exploiting their ability to probe the causal structure of the world through their actions. We have decided to model the body of MIMo after an average 18-month-old child. In total, MIMo has 82 degrees of freedom of the body and 6 degrees of freedom of the eyes. MIMo also features different facial expressions that can be used for studies of social development. To simulate MIMo’s interaction with the physical environment we use the MuJoCo physics engine [13], because of its strength at simulating contact physics with friction. Generally, we have aimed for a balance between realism and computational efficiency in the design of MIMo. To accelerate the simulation of physics and touch sensation, MIMo’s body is composed of simple rigid shape primitives such as a sphere for the head and capsules 2 for most other body parts. Presently, MIMo features four sensory modalities: binocular vision, proprioception, full-body touch sensation, and a vestibular system. We also plan to add audition in the future. The remainder of this article describes the detailed design of MIMo and illustrates how to use it. In particular, we present four scenarios where MIMo learns to 1) reach for an object, 2) stand up, 3) touch different locations on his body, and 4) catch a falling ball with his five-fingered hand. These examples are used to illustrate and benchmark potential uses of MIMo. They are not intended as faithful models of how infants acquire these behaviors. This paper is an extended version of preliminary work pre- viously published at the ICDL 2022 conference [1]. Compared to the preliminary MIMo version we have added 1) five- fingered hands, 2) a new actuation model that more accurately models the force-generating behaviour and compliance of real muscles, 3) a detailed play room (see Fig. 2), in which MIMo can interact, and 4) a new demo environment of learning to catch a falling ball. A fifth contribution is that we have improved computational efficiency and benchmarks have been updated accordingly. II. RELATED WORK We focus on two major classes of software platforms for simulating cognitive development during embodied interac- tions with the environment. The first kind is designed to simulate a particular physical robot used in Developmental Robotics research and intended to complement the work with that physical robot. Examples are the iCub simulator [17] and the simulator for the NICO robot [18] that has been imple- mented using the V-Rep robotics simulation environment [10]. Fig. 2. Top view of the play room environment. The room has a size of 4×4 m2 and is filled with furniture and toys. Furniture models are taken from the 3D-FUTURE dataset [14] and toys from the Toys4K 3D Object dataset [15]. Pictures taken from the Man-made category of the McGill Calibrated Color Image Database [16] are placed on the walls. Additionally, the room has a door and a window, which provides a view into a garden. Such platforms typically aim to faithfully reproduce the design and behavior of the physical robot, permitting to substitute work with the real robot through simulations. However, there always remains a notorious gap between simulation and real world. Furthermore, such simulation platforms also inherit any shortcomings of the robot design relative to the human body and human sensing capabilities. For example, if the robot possesses only poor touch sensation, its simulated counterpart will suffer from the same limitation. The second kind of platform emulates human body and sensing abilities directly and thus is not restricted by limi- tations of current robotics technology in general or that of specific robots in particular. An early example is the seminal work by Kuniyoshi and Sangawa [19]. More frequently, simu- lation models of specific aspects of sensorimotor development have been proposed. These typically encompass only a small subset of degrees of freedom and sensory modalities. An early example is work on the development of grasping by Oztop and colleagues [20]. A more recent example is the OpenEyeSim simulator, which has been designed to support modeling the development of active binocular vision [21] and is built using the OpenSim software for simulating neuromusculoskeletal systems [22]. While widely used in Reinforcement Learning (RL) for locomotion tasks [23], standard humanoids intro- duced within the MuJoCo [24] or Bullet [11] platforms only incorporate very limited haptic abilities and do not model a child-like physical appearance. This may be too limiting, because the structure and physiology of the body constrains the kinds of interactions that are possible. III. PHYSICAL DESIGN MIMo’s design is based on the MuJoCo humanoid, con- sisting of geometric primitives, primarily capsules. His overall body dimensions and proportions were adapted from anthropo- metric measurements of 16–19 month old infants [25], treating the unit of distance in MuJoCo as one meter. Body dimensions for which no direct measurement is found in [25] where induced from other measurements. For example, the length of the lower arm is derived from the elbow-to-fingertip and hand length measurements. We then made many small adjustments to MIMo’s proportions to give him a more natural look, while staying close to the experimental measurements. For example, the upper arms appeared very thin compared to the torso and had their circumferences increased by 0.4 cm, well within the 1.3 cm standard deviation reported by [25]. All joints are modeled as a series of one-axis hinges and split into flexion/extension, abduction/adduction or inter- nal/external rotation as appropriate for the joint. For the shoul- ders we merged the commonly used abduction and flexion axes, instead using horizontal flexion, abduction and internal rotation. This allows MIMo the same total range of motion by combining horizontal flexion and abduction. Keeping both axes would have allowed for an unrealistic, very large total range of motion if both flexion and abduction were at their limit, and MuJoCo’s joint limit functions were too simple to prevent this in a satisfactory manner. We provide two different versions of the hands for MIMo, a simple one with only 3 (a) Mitten hand (b) Full hand Fig. 3. Different versions of MIMo’s hands. (a) Simple “mitten” hand with 4 degree of freedom. (b) Five-fingered hand with 26 degrees of freedom. a single finger and a fully articulated five-fingered version, modified from [26], to enable experiments where dexterous manipulation is required (see Fig. 3). In total the mitten hand version has 44 degrees of freedom, while the full hand version has 88. These consist of 30 in the body, 6 in the eyes and 8 and 26 degrees of freedom in the hands for the two versions, respectively. To produce range of motion and strength measurements, we collected data from a large number of sources [27]–[38] and then inferred missing values with the available data. For range of motion we took values from the youngest age reported in the various studies and assumed no change. For muscle strengths we took data from [27] as a baseline. These authors consider children aged 3–9 and we used values from the lower end of this range for all joints reported. Joint strengths missing from [27] were induced from other studies by assuming that the relative strengths of joints stay constant, using the strengths of the knee or elbow from [27] as reference values. Where required we converted forces to torques using the appropriate lever arms from MIMo based on the methodologies used in the respective source. Table I shows the range of motion and strengths for all main joints. We treat extension, adduction and internal rotation as positive and flexion, abduction and external rotation as negative. Citations show the source of the data, while entries without marked sources indicate best guesses based on the other values. For the full hand model we kept the range of motion from [26], with finger strengths for individual fingers derived from [39]. For the mitten hand the strength was derived from [27], with a range of motion that allows him to fully close the hand. MIMo can also change his facial expression. This is im- plemented by changing the texture of the head (see Fig. 1). In addition to the neutral expression we provide six extra textures, corresponding to the six basic emotions proposed by [40] (enjoyment, sadness, surprise, disgust, anger, and fear). These can be used to convey an internal emotional state, for