Introduction The number and complexity of user-centred approaches to XAI have been increasing in recent years (Williams 2021). The application contexts of such approaches are various. They go from computer applications aiming at providing personalised teaching (Embarak 2022)(Cohausz 2022) to human-robot interaction (HRI) contexts in which the robot maintains users’ models to provide explanations tailored to them (Matarese, Rea, and Sciutti 2021)(Stange et al. 2022). These works show a clear trend: user-centred XAI posi- tively affects the interaction between users and systems. It brings to higher users’ willingness to reuse the system (e.g., with recommendation systems (Conati et al. 2021)), robots’ persuasiveness during human-robot decision-making tasks and human-AI teams performance (Schemmer et al. 2022b). One of the reasons why the XAI research field is mov- ing toward user-centred approaches is to improve the good- ness of the explanations. Alongside producing personalised XAI, recent works also aim to evaluate the goodness of the explanation produced. What is meant for “XAI goodness” is still under debate; nonetheless, there is a broad consen- sus about the dependency of this concept on the applica- tion context. Unfortunately, most of these evaluations rely on subjective or indirect measurements (e.g., questionnaires or performance). However, the main aim of an XAI system is to provide the user with information about the functioning of the underly- ing AI model. So far, the amount of information an XAI sys- tem provides has been assessed through indirect measures, such as users’ ability to simulate the AI behaviour. Indeed, recent surveys (Mohseni, Zarei, and Ragan 2018) and sys- tematic reviews (Nauta et al. 2022) highlight the need for more objective and quantitative measures to assess the good- ness of XAI techniques. In our opinion, it is worth taking a step back and mea- suring the goodness of XAI systems from the information they can provide to users. Although measuring how much information a system can generate could be challenging, we can focus on the amount of new knowledge that such flow of information creates in the users’ mental models. Hence, if we let only non-expert users interact with XAI systems, we can state that the knowledge they acquired arose from the interaction with the system. Finally, if such knowledge is quantifiable, we can assess how much the system has been informative. In this paper, we aim to propose an assessment task to objectively and quantitatively measure the goodness of XAI systems during human-AI decision-making tasks intended as how much they are informative to non-expert users. More- over, we plan to use such a task to study the effectiveness of user-centred XAI in HRI scenarios. In particular, we are interested in understanding whether a user-centred XAI ap- proach is more informative than a classical one. Related work Several works assessed XAI systems’ properties through user studies. The more interesting ones, in our opinion, are addressed to non-expert users (Janssen et al. 2022). These latter used tasks that only some people know about. For ex- ample, authors in (Lage et al. 2019) used two application domains: an artificial alien’s food preferences based on the meals’ recipes, and clinical diagnosis. Those in (Wang and Yin 2021) also used two peculiar decision-making tasks: a recidivism prediction and a forest cover prediction task. Fur- ther, authors in (van der Waa et al. 2021) used a diabetes self-management use-case where naive users and the system 1 had to find the optimal insulin doses for meals. Finally, both (Goyal et al. 2019) and (Wang and Vasconcelos 2020) used image classification tasks in which participants were asked to recognise two bird species. Several works regarding the assessment or comparison of XAI methods tend to define their own measure of good- ness (van der Waa et al. 2021)(Lage et al. 2019). However, a method has been proposed to objectively measure the de- gree of explainability of the information provided by an XAI system (Sovrano and Vitali 2022). Moreover, the authors in (Holzinger, Carrington, and M¨uller 2020) proposed the Sys- tem Causability Scale to measure the quality of the expla- nations based on their notion of causability (Holzinger et al. 2019). Finally, the authors in (Wang and Yin 2022) proposed a different point of view regarding assessing XAI systems’ goodness by comparing several types of XAI in different application contexts with respect to three desiderata: to im- prove people’s understanding of the AI model, help people recognise the model uncertainty, and support people’s cali- brated trust in the model. Recent works regarding user-centredness focus on users’ trust towards the system and highlight context-awareness and personalisation as main approaches to user-centredness (Williams 2021). Personalisation in XAI has also been im- plemented by exploiting the users’ personality traits and cor- relating them with users’ preferences or behaviours (B¨ockle, Yeboah-Antwi, and Kouris 2021)(Martijn, Conati, and Ver- bert 2022). Instead, authors in (Bertrand et al. 2022) re- viewed a relevant corpus of literature to understand which human biases researchers reflect in their XAI methods (also without noticing). Most of the works in the XAI field with user studies re- gard decision-making (Wang and Yin 2021) or classification tasks (Goyal et al. 2019). The rationale behind this choice is the promise of better performance when coupling human users with expert AI systems (Wang and Yin 2022). Such a promise is almost always kept, although a few studies show that team performance decreases when using some form of XAI (Schemmer et al. 2022b). Indeed, some authors high- lighted that AI advice is not always beneficial mainly be- cause humans have shown to be unable to ignore incorrect AI advice (Schemmer et al. 2022a)(Ferreira and Monteiro 2021)(Janssen et al. 2022). Motivation and hypotheses So far, researchers have measured the goodness of an XAI system used in human-AI decision-making tasks by relying on indirect measures, such as team performance, systems’ persuasiveness, or users’ ability to predict the AI’s decisions. This means that no objective and quantitative measures have been used to state how good XAI systems are per se. This lack contributes to the impossibility of rigorously comparing two XAI strategies regardless of their particular application context. Since all the XAI systems have been examined with respect to their application scenario, it is also hard to gener- alise the results of such research. We need to introduce an objective and quantitative assess- ment task to measure one of the most-ignored-so-far fac- tors of XAI: its information power. With the term informa- Figure 1: The co-constructing approach to the explaining process, as in (Rohlfing et al. 2020). The actors’ explana- tion behaviours adapt by monitoring and scaffolding each other. In our setting, the ER monitors the EE through feed- back about their actions and questions. tion power, we mean the amount of information that an XAI system provides about: (1) underlying AI models’ (general) functioning, (2) reasons behind a particular model’s choice, or (3) what the system would do in other circumstances. Under the assumption that the goodness of an XAI sys- tem reflects the accuracy of the users’ mental models about the underlying AI system (Hoffman et al. 2018), we want to understand deeper why user-centred XAI is more effec- tive than those approaches that do not take the users in con- sideration. Our experimental hypothesis regards the users’ level of involvement in the explanation generation process. In particular, the more users’ are involved in the explanation generation process: H1 the more the XAI system has more information power (resulting in more accurate users’ mental models of the AI). H2 the more they become independent of the robot’s sugges- tions and explanations. H3 the higher the users’ willingness to reuse the XAI system. H4 the more positive the users’ feelings toward the robot. Methods We are interested in introducing an objective and quantita- tive assessment task to measure the goodness of XAI sys- tems without relying only on indirect measures. Hence, we plan to measure the XAI systems’ information power di- rectly and validate the task through a user study. The task The assessment consists of a decision-making task where users can interact with a control panel to perform actions in a simulated environment (see Section ). During the task, users can interact with an expert AI (in our case, with the 2 Figure 2: The schema of a pressurised water reactor that we implemented in our simulated environment. This schema considers only the control rods, but we also allow the users to manage the other three types of rods: the fuel, sustain and regulatory ones. humanoid robot iCub) by asking what it would do, and its XAI system by asking why it would do that. Apart from the instructions about interacting with the control panel and the robot, users start the task knowing nothing about it. In a fixed amount of time, the users have to perform ac- tions and interact with the robot to discover (1) which is the task at hand (e.g., what is the goal of the task), as well as (2) the rules that the AI model uses to make its actions and, since the robot is an expert agent, (3) the rules that govern the simulated environment. Interaction modalities The roles between the human- robot team and their interaction modalities are simple. The robot can not perform actions, but its role is limited to assist- ing users during decision-making. However, the robot can not take the initiative in giving suggestions either, but it al- ways replies to users’ questions. Thus, only the users can interact with the control panel and act in the simulated envi- ronment. We expect the following HRI modalities. If the user al- ready knows what to do, they act on the control panel. Oth- erwise, they ask the robot what it would do; then, the robot answers this what question saying what action it would do in the current scenario. If the user understands the robot’s rea- sons, they act on the control panel (not necessarily the sug- gested one). Otherwise, the user asks the robot why it would do that. At this point, the robot answers this why question by explaining as justification for its suggestion. All that remains for the user is to act on the control panel. Characteristics of the task We need non-expert users to consider the information passed through the interaction as new information. Taking it to extremes, we consider only participants without knowledge about the task and its under- lying rules. For this reason, we implemented a simulation of a nuclear power plant management task (Figure 2). We chose this kind of task because it met all the requirements we needed: it is challenging and captivating for non-expert users, simple rules govern it, an AI model can learn those rules and, usually, people know nothing about the function- ing of nuclear power plants. The main objectives of the task (which we hide from users) are to generate as much energy as possible and main- tain the system in an equilibrium state. The features of the environment are subject to rules and constraints, which we can summarise as follow: · Each action corresponds to an effect on the environment: thus, a change of its features’ value. · Several preconditions must be satisfied to start and con- tinue nuclear fission and produce energy. · Some conditions irremediably damage the plant. · The task is divided into steps of fixed time duration (e.g., 10 seconds), in which the users can interact with either the robot or the control panel. The simulated environment Features of the environment Our simulated power plant is composed of 4 continuous features: · The pressure in the reactor’s core. · The temperature of the water in the reactor · The amount of water in the steam generator · The reactor’s power. Furthermore, the power plant has four other discrete features that regard the reactor’s rods: security rods, fuel rods, sustain rods, and regulatory rods. The firsts two have two levels: up and down. Instead, the latter two have three levels: up, medium, and down. The reactor power linearly decreases over time for the ef- fect of the de-potentiation of the fuel rods. Hence, the reac- tor’s power depends on the values of the environment’s fea- tures and whether nuclear fission is taking place. Moreover, 3 the energy produced at each step is computed by dividing the reactor’s power by 360, which is the power that a 1000MW reactor without power dispersion produces in 10 seconds. Actions to perform on the environment The actions that the user can perform (12 in total) go from changing the po- sition of the rods to adding water to the steam generator or skipping to the next step. All those actions change the value of 3 parameters - T , P , and L - which correspond to the water’s temperature in the core, the core’s pressure, and the water level in the steam generator, respectively. The setting of the rods determines the entity of the feature updates; such updates are performed at the end of each step, right after the users’ action. For example, if the safety rods are lowered in the reactor’s core, the nuclear fission stops; thus, T and P decrease until they reach their initial values, and the water in the steam generator remains still. On the other hand, if nu- clear fission occurs and the user lowers the regulatory rods, the fission accelerates. This acceleration consumes more wa- ter in the steam generator, raising the core’s temperature and pressure more quickly, but also raising the reactor’s power and the electricity produced. If