INTRODUCTION Connected mobility is changing all our lives every day, be it in the field of social robotics or self driving cars. Specifically in the case of self driving cars, the importance of predicting Vulnerable Road Users’ (VRUs) positions becomes paramount. It also becomes a hard problem to solve since most VRU motion, especially that of pedestrians and bicyclists on the street are not deterministic but highly stochastic. Cars, to navigate safely around such VRUs, need to have a high degree of Situational Awareness (SA). In literature, one of the methods exploited to achieve this high level of SA is to use the Social Force model [1]. Other works have dealt with the importance of the semantics and the inherent goals existing in the environment in establishing pedestrian behaviour [2], [3]. In this work, we aim to model pedestrian behaviour in urban areas so as to create a risk map of the environment that can then be used by other parts of the AV system to navigate through such an area. We do this by modelling the social forces around the car in the environment, identifying the different possible goals for each pedestrian in the scene and utilising the map information around each detected pedestrian at each time step in their trajectory. In summary, our contributions presented in this work are as follows: (1) we propose a novel model architecture that captures social and environmental interactions to accurately model spatio-temporal pedestrian trajectories over a short to medium term horizon (2) this architecture incorporates 1 Ecole des Name.LastName@eleves.enpc.fr Ponts, Marne-la-Vall´ee, France, email: First- 2 Akkodis Research, 78280 Guyancourt, France, email: First- Name.LastName@akkodis.com a novel goal identification module that generates multiple likely end points for each observed pedestrian partial trajec- tory. (3) A generative model that beats the state of the art on two drone datasets proving that the model can generalise to similar environmental scenarios. Section II discusses existing work in the domain of multi- modal, medium-horizon, trajectory prediction for VRUs. Section III discusses our theoretical approach and presents the CVAE based model for generating multi-modal pedes- trian trajectories. Section IV provides a detailed explanation on the implementation and results of our approach on various open datasets as well as ablation studies. II. RELATED WORK A. Sequence modelling Capturing temporal sequences and predicting future states have long been studied, using classical methods or neural network methods, especially in the field of pedestrian tra- jectory prediction. In the former category, some methods use Hidden Markov models and self organising graphs to predict trajectories [3], while in the latter category Recurrent Neural Networks have been largely popular in capturing pedestrian behaviour [4]–[9]. In other cases, CNNs like in Time-Extrapolator Convolution Neural Network (TXP-CNN) [10] and U-Net [11] have also been used to extract temporal features from pedestrian trajectories. Recently however, the sequence Transformer which was originally proposed in the NLP domain [12], has proven to be very successful in modelling pedestrian behaviour [13]–[16]. This class of architectures have come in handy when used in scenarios with partial observability and partial trajectories, which are difficult for RNNs to deal with. B. Social forces modelling The concept of social forces in humans was first pro- pounded in [1] which has then been applied to trajectory forecasting [2]. Social forces are those forces that act on an ego agent in a built environment as a function of its surroundings and other agents present in the scene, causing an attractive or repulsive interactions with them. In brief, these can be commonly classified as neighbour interactions, map interactions and goal based interactions. Modelling of these forces using neural networks have been studied in mechanisms such as Social pooling [4], [7], attention mech- anisms [9], [17], [18] and graph-based neural networks for homogeneous [6], [15], [19] and heterogeneous interactions [20], [21]. Map-derived interactions have also been studied by em- bedding the map information as an aid to prediction [5], [14], [22]while [17] utilizes physical attention to take in account the map information. This information in turn can be used to identify and predict goal based interactions [11], [21] or to generate less expensive goal heatmaps [23]–[25] C. Multi-modal output Considering that human motion is inherently stochas- tic, it becomes necessary to handle this uncertainty while performing prediction. This is handled in different ways in different works. A simple random noise is added or concatenated to the input of the trajectory generator in some works [6], [15]. [7], [17] use GANs adding adversarial loss to generate prediction with higher quality. CVAE models have also been popular to implement multi-modal trajectory prediction [5], [8], [14], [18], [25] due to its ability to learn the latent variable distribution from prior conditions. To explicitly express different modalities, certain anchor- conditioned methods have been proposed [26], such as predicted endpoint conditioned (PEC) [23] and prototype trajectory conditioned (PTC) [27] frameworks to encourage more explainable predictions. In our work, we take ideas from each of these systems such as using Transformers to capture temporal trajectories, Graph Networks to capture varied and heterogeneous interactions and CVAEs to model the multi-modality of human behaviour. The exact method in which these components are assembled together in our architecture will be described in the next section. III. THEORETICAL APPROACH A. Problem formulation Our problem of multimodally predicting pedestrian trajec- tories can be formally prescribed as follows. Starting with a bird’s eye view (BEV) scene of an environment captured in RGB, semantically segmented into different classes, denoted by M contains a history of positions of each pedestrian in this scene. For each scene, there exist a finite number of goals that can be reached by a pedestrian denoted by G. Each pedestrian also interacts with every other agent in the scene, denoted by E. Given this information, the model aims to predict the distribution of pedestrian’s trajectory for H timesteps in the future. We aim to predict the distribution of the future trajectory based on all the history information we have, and then generate multi-modal trajectories for each agent. We learn the parameters θ of the probability Pθ(Y|X, M, G, E) with X denoting trajectory history and Y denoting the predicted trajectory. For an observed agent i, the model outputs the pre- dicted future positions in the next H time steps from the current time t, defined as Yi = (Yt+1 ). Yt i ) is the position coordinate of the agent with x, y ∈ R. The input of the model is the history information in the past n time steps, which can be denoted as Xi = , ..., Yt+H i = (xt , Yt+2 i i, yt i i (Xt−n+1 , Xt−n+2 , ..., Xt i). In order to consider the effect i i the history states of ego agent and all of social forces, neighbors as well as the semantic map information are input to the model. We assume that the number of agents in the area of the shared space around the ego agent (including the ego agent) is a time varying variable N t. The input at i = (St each time step can be denoted as Xt a=1, where St a, ˙yt a = (xt a) represents the state of the agent a, and It i denotes the semantic map information at time t around the ego agent. i)N t a, ˙xt a, yt a, It B. Proposed model Our proposed model can be split into three main compo- nents, in two phases as depicted in Fig. 1. In the Encoding Phase, we encode the ego agent’s interactions with the environment and other agents in the scene to create “Social Embeddings”. These Embeddings are used along with other data to gererate multi-modal trajectories in the Generative Phase of the model. The three main components are the Graph Attention Encoder (GAE), the Temporal Transfmer Encoder-Decoder (TT) and the generative CVAE model for the ultimate output of multi-modal trajectories. Each of these components and their sub-components are described in detail below. Fig. 1: GSGFormer model overview. Consider various VRUs in a structured urban environment, in BEV as shown. Our model uses the understanding of the interplay of social forces between different VRUs and the environment in which the motion is captured. A map encoder captures the influence of the environment on the VRU under consideration while the other encoders - for encoding the ego motion of the VRU, identifying the different goals of this VRU and those of its neighbours - are fed into a Graph Attention Network Encoder to capture a “Social Embedding” within the environment. Social Embeddings for each time step are then passed on to the Generative Phase of the model, where different contexts from the Transformer Decoder and the CVAE Encoder are concatenated along the last dimension of the tensor and passed to the Residual GMM generator. Finally, positions for a time interval h from current time t are predicted by integrating these velocities over the time interval dt. Best viewed zoomed in. 1) Agent State Encoders: This submodule acts as input to the GAE. Each agent in the scene is classified as one of four classes - pedestrian, bicycle, car and bus. The state of each class of agent is embedded by a corresponding linear layer. For each ego agent trajectory, neighbour tracks are identified by converting to an ego-centric coordinate, their position relative to the ego agent [30], also embedded using a linear layer. All outputs of the corresponding linear layers are denoted by dmodel. 2) Map Encoder: Encoding the Map first requires pro- cessing the RGB image a semantically segmented image, creating an abstraction over the real observation. To encode this, we use a U-Net [31] architecture for semantically seg- menting the RGB image. The semantic map labels are drawn from [11]. Where such labels are not available for training, they were manually annotated in 6 classes. In addition to the SDD[32] and inD[33] datasets in the work[11], we also used map images and our own segmentation annotations from the OpenDD dataset[34] as training set. The model was trained with Dice Loss [?] and the resulting segmentation masks are as shown in Fig. 2. Fig. 2: Examples from the inD, SDD and OpenDD dataset. Above is the original image and below is the segmentation mask. This mask output containing semantic information of the scene is then encoded using a 2D CNN to generate the map embedding of size dmodel 3) Goal Encoder: This subcomponent is one of our novel contributions in this paper. We propose a 1D CNN to estimate the goal position. Initially, The state of the ego agent for each history time step is embedded using the agent state encoder. Then the embedding is stacked over the time di- mension, resulting in a tensor of shape (n×dmodel). The 1D convolutional neural network extracts the temporal feature between the n channels(time steps). The map embedding is concatenated to the output of the goal encoder and passed to a linear layer. The output of the network is a vector of size dmodel. The vector then serves as the input condition for the CVAE-Residual-GMM module detailed further in III-B.6, facilitating the generation of a range of potential goals. 4) Social Graph Attention Neural Network: To model the social force acting on the ego agent, we introduce a hetero- geneous graph representation. This representation comprises four distinct nodes: the ego agent, neighboring agents, the semantic map, and the goal populated by their respective embeddings. Physically, we only consider the effect of these embeddings unidirectionally on the ego agent at time step t. Then a heterogeneous graph attention network[35] performs both node-level and semantic-level attention aggregating the social information to the ego agent node. The output is the social force embedding in size of dmodel is embedded from the aggregated ego agent node. This embedding is then element-wise combined with the state embedding of the ego agent, producing the social embedding at time step t. 5) Temporal Transformer Model: Once the social infor- mation are encoded from the social graph of the ego agent at each time step, it is necessary to build the dependencies between graphs across different time steps. We employ a Transformer framework [12] to model the temporal relation- ship. The history sequence of social embedding are fed into the Transformer encoder as the source sequence with a fixed length n. Since the primary operation in Transformer is the attention mechanism which is not sensitive to the relative or the absolute position of the elements in the sequence, we add the ”positional encodings” with the same dimension to each social embedding in the sequence. We compute positional encodings through sine and cosine functions, as described in [12], which allows the model to easily learn to attend by relative positions. Meanwhile, it enables the model recognize any missing observations, as each comes with its distinct positional encoding. The Transformer encoder processes the history sequence in parallel and outputs the memory providing the query for each observation time step. The Transformer decoder takes the embedded future ego agent’s state sequence as the target sequence. The query of the memory is used to calculate the cross attention between the history sequence and future sequence. From the Transformer decoder, we obtain the predicted embedding sequence. Every embedding within this sequence serves as a condition for the waypoint CVAE-Residual-GMM Module (see in III-B.6), generating multi-modal states for each prediction horizon. 6) CVAE-Residual-GMM Module: We propose a gener- ative CVAE-Residual-GMM Module to realize the one-to- many mapping for both goal and waypoints. The module