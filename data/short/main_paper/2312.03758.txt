INTRODUCTION
Acknowledg-
ing the inherent unpredictability of precise stock prices [2],
our research leverages these blue-chip stocks to forecast future
trends in stock price movement and volatility [3], [4].
Research within the realm of the stock market generally
falls into two branches: technical analysis and fundamental
analysis. Moreover, the existing
models are primarily focused on predicting whether trends will
change [10], but overlook the significance of the magnitude of
such changes. In reality, the extent of these changes constitutes
an important part of stock behavior.
In this paper,
the aforementioned limita-
3GICS is a method for assigning companies to a specific economic sector
and industry group that best defines its business operations.
1The ratio is a measure of the total value of all publicly traded stocks in a
4Investment-worthy refers to high-quality companies rated Baa or higher
market divided by that economy’s gross domestic product (GDP).
according to evaluations by Moody’s and Standard & Poor’s (S&P).
2A blue chip stock is stock issued by a large, well-established, financially-
5Although Twitter has recently been re-branded to “X�, this article contin-
sound company with an excellent reputation.
ues to use the original name,
In this study, we aim to overcome the limitations of previous
methodologies. We adopt an efficient tweet filter to eliminate
redundant content and reduce noise. Additionally, we embrace
a multi-dimensional approach to the stock market, integrating
correlations of stock market information in a multi-task man-
ner. By skillfully combining the strengths of both technical and
model, we achieve state-of-the-art performance in predicting
stock movements.
III. PROBLEM SETUP
We offer a formal definition for the problem of predicting
stock movement and volatility. Let S denote a set of target
stocks for which predictions are sought. We define a set xst
of feature vectors, with s ∈ S and t ∈ T , which encapsulate
historical prices. Let T represents the set of available training
days. In addition, we identify a set ε consisting of tweets,
each referencing at least one stock in S. These historical
prices incorporate the opening, highest, lowest, closing prices
and adjusted closing price7 for each stock. Using P to
signify the adjusted closing price of stock s on day t as
P = {Ps,1, . . . , Pst}, we then express the actual labels for
movement and volatility as follows:
We usually can generate multiple training examples by
shifting the time lag within extensive historical stock data in
practical situations. However, in order to simplify the expla-
nation of our proposed method, we focus on a specific time
lag for both movement and volatility prediction. We’ve also
integrated the use of adjusted stock prices into our predictive
models. [23]. This allows our models to learn from the genuine
changes in a stock’s value that occur due to market conditions,
rather than being swayed by artificial fluctuations resulting
from corporate actions.
IV. ECON STRUCTURE
We present an overview of data ingestion, text preprocess-
ing, and model architecture, as illustrated in Figure 1. Initially,
we process the historical stock prices and macroeconomic
factors to distill pertinent features. Following this, we sift
through Twitter data to identify tweets that most aptly reflect
public sentiment. During text preprocessing, we mask stock
tickers associated with companies found in the curated tweets.
These masked tweets are then transformed into embeddings.
Leveraging a self-aware mechanism, we generate embeddings
for both sectors and tweets. In the concluding step, we amal-
gamate information from stocks, sectors, and macroeconomic
indicators in a multi-trends approach to predict stock price
movement and volatility.
 In our research, we consistently selected the
top six tweets, those garnering the highest daily impressions,
as our primary input for text preprocessing.
Sentiment Analysis. In our research, we undertook a sen-
timent analysis on a dataset comprising 7.7 million tweets.
To achieve this, we leveraged a language model based on
Roberta8, and the model has been fine-tuned on Twitter data.
This optimized model can produce one of three sentiment
outcomes for each tweet: positive, neutral, or negative. For
every tweet e associated with a blue-chip stock s, we compared
the predicted sentiment pe with the same day’s binary adjusted
price movement ys
m gauges the
price fluctuation between days t − 1 and t. Same as problem
setup, if the stock price ascends in this interval, ys
m is labeled
“Up�, and conversely, if it declines, it’s labeled “Down�. It’s
pertinent to note that our dataset only incorporated tweets from
the trading hours of day t. To aggregate daily sentiment, if a
day witnesses a predominance of negative sentiments in tweets
concerning a specific stock, we designate that day as “nega-
tive�. In contrast, if the tweets are primarily characterized by
positive sentiments, the day is classified as “positive�.
m of stock s. The measure ys
Tweets extracting. In addition to textual information, we
gathered metrics on likes, retweets, and impressions for each
V 0.0394 0.0643 0.0747 0.0816 0.0833 0.0877 0.0872 0.0853 ... 0.0917
C. Text preprocessing
In natural language processing (NLP), text preprocessing
has long been recognized as a key step. By streamlining
text into a format more amenable to training, it not only
simplifies the content but also boosts the efficiency of machine
learning algorithms. Shifting focus to the concept of self-
aware mechanism, our main objective is pinpointing sector
identification. Within this context, the embeddings of both
sectors and tweets are crucial. When the stock symbol s is
masked, we aim to identify the sector c a tweet e refers to
based solely on its content.
Tokenization. Tokenization is the process by which text is
divided into smaller units, such as sentences, words, charac-
ters, or subwords. Word tokenization, specifically, refers to the
segmentation of text into individual words. In our research, we
utilized spaCy9—an open-source natural language processing
tool—for word tokenization of every tweet e. Moreover, a
9https://spacy.io/
stock ticker like “AAPL� for Apple might inadvertently be
tokenized into “AA� and “PL�. To circumvent such issues,
we established specific rules to ensure that stock tickers are
consistently recognized as single, intact tokens. After tokeniza-
tion, words are converted to lowercase because stock tickers
won’t overlap with regular vocabulary. Such normalization not
only ensures vocabulary consistency across the dataset but also
augments computational efficiency.
Company Masking. For the subsequent self-awareness
portion of the model architecture, we replace all tokens in
εs that correspond to the name of stock s with a special token
[mask]. Based on the sector classification rules of GISC, we
maintained a dictionary mapping the masked companies to
their respective sectors. For example, after data filtering, we
obtain a tweet with a high impression: “With Vision pro,
$AAPL is about to soar!� Next, we mask the stock ticker,
converting it to: “With Vision pro, [mask] is about to soar!�
Sentence encoding. Words from the sentence are first
tokenized and then mapped to their corresponding word em-
beddings. These embeddings are high-dimensional vectors that
capture the intrinsic semantics of each word. We record the
original length of each tweet, storing this information in the
variable l. This data proves crucial when padding comes
into play, a technique used to append or prepend sequences
with filler values, ensuring uniformity. Given the importance
of maintaining consistent sequence lengths, especially when
working in batches, padding ensures our dataset’s sequences
adhere to a standardized length, adding consistency and struc-
ture to our data processing pipeline.
In our approach, we adopted a
bi-directional LSTM (BiLSTM) for the predictor network to
produce the tweet and sector embeddings.
The first attention is to find tweets relevant to each target sector
c. We find relevant tweets by using the embedding hc of sector
c as the query of attention as follows:
let εt represent
Considering a day t,
the collection of
tweets. For each tweet e within this set, its corresponding
embedding is denoted by he. Additionally, αe signifies the
attention weight attributed to the tweet. The vector wct ∈ R2k,
which can be envisioned as a weighted mean of all the tweet
embeddings for day t. This averaging is influenced by the
pertinence of each tweet to the specific target sector c. Then,
we employ a linear layer to project wct onto all possible
companies S:
gst = W1(wct) + b1,
where W1 ∈ Rn×2k, b1 ∈ Rn. As a result of the attention
function of Equation (10), gst ∈ Rn represent that summarizes
all tweets at day t considering the relevance to sector c. Our
second attention uses the resulting tweet vector to find the
most relevant stocks at the moment. This is done by using gst
as the query vector of attention to aggregate the stock price
features at day t based on the stock embeddings:
The micro trend vector ist ∈ Rn encapsulates all tweets for
day t, integrating the information from companies associated
with the sector. This vector is then combined with the stock
price vector xt ∈ Rn×p which generates from Yahoo’s stock
data and the macro trend vector at, serving as the input to the
Attention GRU for forecasting stock movement and volatility.
Predictions layer. The model is primarily designed to ex-
tract nuanced information from stock market, macroeconomic
factors, and tweet data. Once the macro trend at and micro
trend ist are generated, they are concatenated with the stock
price feature xst ∈ Rp using a linear layer, producing a multi-
level feature.
V. EXPERIMENT
We conduct experiments to answer the following questions
about the performance of framework:
Q1. Baseline methods. How do other baseline models perform
on our task?
Q2. Tweets quality. Does using our model to extract data
yield better results than performing sentiment analysis using
the entire 7.7 million tweets as input?
Q3. Ablation study. How do the macro and micro trend affect
its performance?
A. Baseline Methods
We evaluate our model’s effectiveness by comparing it with
technical and fundamental analysis models. Technical analysis
models focus on capturing patterns of historical prices and
TABLE III : Classification performance of ECON and baseline
methods, measured with the accuracy (ACC) , the Matthews
correlation coefficient (MCC) and Receiver Operating Char-
acteristic (ROC). The best is in bold, Our ECON shows the
best performance in all evaluation metrics.
fundamental analysis models combine historical prices with
other government and financial market information for stock
price movement and volatility prediction.
• Extreme Gradient Boosting (XGBoost) [32] is a high-
performance gradient boosting framework renowned for its
efficiency in handling large datasets, regularization features,
and capability in both classification and regression tasks.
• Autoregressive Integrated Moving (ARIMA) [33] is a fore-
casting model for time series data, consisting of autoregressive
(AR), differencing (I), and moving average (MA) components.
It assumes data linearity and stationarity.
• Long Short-Term Memory (LSTM) [34] is a type of
recurrent neural network (RNN) architecture specifically de-
signed to tackle the vanishing and exploding gradient problems
encountered in traditional RNNs. LSTM units use gate mech-
anisms to regulate the flow of information, making them well-
suited for learning from long-term dependencies in sequences.
• Attention LSTM (ALSTM) [12] is a LSTM network with
an attention mechanism, enhancing sequence modeling by
emphasizing relevant elements.
• DTML [35] is based on Transformer, it learns temporal
correlations and combines the contexts of all target stocks.
• SLOT [9] is based on ALSTM, it captures correlations
between tweets and stocks through self-supervised learning.
To test whether our Twitter filter can still accurately capture
market sentiment even with a significantly reduced Twitter
• AGRUD-A concatenated the sentiment score of all 7.7
million tweets with stock prices and macroeconomic data,
keeping the structure of main predictor AGRUD unchanged.
• AGRUD-F concatenated the sentiment score of data refined
with stock prices and macroeconomic data, keeping the struc-
ture of main predictor AGRUD unchanged.
Evaluation metrics. Following previous work for stock
movement prediction, we adopt the standard measure of Accu-
racy (ACC) and Matthews Correlation Coefficient (MCC) as
evaluation metrics. MCC avoids bias due to data skew. Given
the confusion matrix containing the number of samples clas-
sified as true positive (tp), false positive (f p), true negative
(tn) and false negative (f n), Acc. and MCC are calculated as
TABLE IV : This table illustrates the effectiveness of the tweet
filter. By retaining only the tweets with the highest impres-
sions, the results for predicting stock price movement and
volatility are consistent with those obtained using sentiment
analysis on all tweets.
even though the number of tweets significantly decreased,
AGRUD-A and AGRUD-F results remained almost the same.
This demonstrates that the filtered tweets effectively represent
people’s genuine intentions. Consequently, this allows ECON
to achieve more accurate forecasting results at a reduced cost
and with greater efficiency.
For stock volatility prediction, given that data points in-
volving stock price changes greater than 5% only constitute
a minor portion of our dataset, using accuracy to evaluate
ECON can be misleading due to model overfitting. When
evaluating the imbalanced data, we use Area Under the Curve
- Receiver Operating Characteristic (AUC-ROC) where AUC
is the area under the ROC curve, which falls in the range
[0,1]. AUC is insensitive to the absolute numbers of positive
and negative samples. It primarily focuses on how the model
distinguishes between positive and negative samples. AUC is
the area under the ROC curve, which falls in the range [0,1].
AUC is calculated as follows:
T P R(F P R) dF P R,
where T P R(F P R) represents the True Positive Rate as a
function of the False Positive Rate. The T P R gives the
proportion of actual positives that are correctly identified.
F P R is the False Positive Rate, indicating the proportion of
negative samples that are mistakenly identified as positive.
The performances of our ECON and the established base-
lines are detailed in Table III. ECON is observed to be the
superior baseline model in terms of accuracy and MCC for
movement prediction. While DTML shows an outstanding
performance in the AUC and MCC for volatility prediction.
ECON surpasses both of these models by significant margins.
In terms of accuracy, ECON achieves a score of 53.36 and
62.84, outperforming SLOT and DTML by 1.7% and 2.7%
respectively, and outshines in AUC by a margin of 4.0%
and 4.0%. We argue this improvement of price movement
prediction is attributed to use of macro and micro trends
to investigate the influence of multiple sectors on stocks.
On the other hand, leveraging macro insights and placing a
stronger emphasis on recent information have also significantly
contributed to improved volatility forecasting.
As shown in Table V, our
ablation study revealed that incorporating multi-dimensional
perception of stock market significantly enhances the pre-
dictive capabilities of the model for stock movement and
volatility. A noteworthy point is that the result of ECON(A)
demonstrates a significant role of the macro trend in enhancing
the accuracy of volatility prediction.
