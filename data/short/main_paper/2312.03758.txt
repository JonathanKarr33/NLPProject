INTRODUCTION
The stock market, a vital mechanism for facilitating stock
trading and crucial capital raising for companies, exerts a
significant influence on other business sectors [1]. Over the
past few years, the U.S. stock market capitalization-to-GDP
ratio1 nearly exceeded 200%, and despite a slight dip in 2023,
it has stabilized at around 150%.

This prominence underscores the stock market’s role as a
key indicator of the U.S. economy. Blue-chip stocks2, serving

as a microcosm of the stock market, become the focus of
our research. Our study on the financial market employs a
selection of 42 blue-chip stocks from 10 Global Industry
Classification Standard (GICS) 3 Sectors, each deemed as
investment-worthy4 by both Moody’s and S&P. Acknowledg-
ing the inherent unpredictability of precise stock prices [2],
our research leverages these blue-chip stocks to forecast future
trends in stock price movement and volatility [3], [4].

Research within the realm of the stock market generally
falls into two branches: technical analysis and fundamental
analysis. Technical analysis employs historical stock prices
as features to forecast future movements [5]. However,
it
is heavily reliant on past data, and often fails to account
for abrupt market shifts triggered by unforeseen events. This
methodology also assumes uniform rational market behavior,
potentially leading to an echo chamber effect where trading
signals self-reinforce, detaching from the underlying economic
reality. Fundamental analysis, on the other hand, incorporates
not only price features but also external information, such as
social media data [6] and search engine data [7]. Mao et al. [8]
shows the accuracy of predicting the S&P 500 closing price
increases when Twitter5 data is incorporated into their model.
Commonly, these data sources often serve as valuable indica-
tors, mirroring not just the financial market but also other key
economic indicators. However, current fundamental analysis
research predominantly focuses on the historical information
of individual stock [9], overlooking the interplay between the
macro-economy and the stock market. Moreover, the existing
models are primarily focused on predicting whether trends will
change [10], but overlook the significance of the magnitude of
such changes. In reality, the extent of these changes constitutes
an important part of stock behavior.

In this paper,

to address

the aforementioned limita-

3GICS is a method for assigning companies to a specific economic sector

and industry group that best defines its business operations.

1The ratio is a measure of the total value of all publicly traded stocks in a

4Investment-worthy refers to high-quality companies rated Baa or higher

market divided by that economy’s gross domestic product (GDP).

according to evaluations by Moody’s and Standard & Poor’s (S&P).

2A blue chip stock is stock issued by a large, well-established, financially-

5Although Twitter has recently been re-branded to “X”, this article contin-

sound company with an excellent reputation.

ues to use the original name, “Twitter”.

979-8-3503-2445-7/23/$31.00 ©2023 IEEE

 
 
 
 
 
 
tions, we propose ECON (A Framework Leveraging Tweets,
Macroeconomic Indicators, and Historical Prices to Predict
Stock Movement and Volatility).

Our contributions can be summarized as follows:
• Designing an efficient

framework for tweet data
extraction in financial domain. The framework employs a
sophisticated tweet filtering mechanism. By effectively re-
moving redundant content and minimizing noise, it focuses
on capturing tweets that genuinely mirror market sentiments.
This optimized approach not only reduces the computational
strain, which in turn allows for a more cost-effective device
setup, but also enhances the accuracy of our subsequent market
predictions.

• Developing a self-aware mechanism for multi-level
dynamic financial analysis. The proposed ECON learns the
dynamic and temporal distance-based relationships inherent
within stocks, sectors, and macroeconomic factors through
a self-aware process. This allows ECON to not only focus
on individual stocks but also to harness effective market
information by hedging against macro and micro trends.

• Formulating a multi-dimensional perception of stock
market
in a multi-task fashion. By integrating multi-
dimensional correlations of stock market, we can not only
predict stock price movements but also efficiently extract
information from stock market volatility. By capitalizing the
same day stock price movements, the model greatly amplifies
its precision in predicting stock market volatility. This allows
us to provide advance warning of any unusual fluctuations in
the stock market in the future.

• Validating the effectiveness and efficiency of the pro-
posed model via comprehensive experiments. We conduct
experiments on one real-world dataset6. Both conventional
methods and deep learning based methods for stock mar-
ket movements and volatility are selected for comparisons.
Evaluations of various metrics are presented, illustrating the
effectiveness of our proposed model.

II. RELATED WORK

Methods for predicting stock movement predominantly clas-
sify into two main categories: technical analysis and funda-
mental analysis. Technical analysis strictly hinges on historical
prices to forecast future movements. In contrast, fundamental
analysis casts a wider net,
just historical
prices but also data from textual sources, economic indicators,
financial metrics, and a diverse range of both qualitative and
quantitative factors.

integrating not

A. Technical analysis

Technical analysis harnesses the historical prices of stocks
as predictive features for future movements. The majority of
contemporary methods employ deep learning architectures to
unravel stock trends. Prominently, recurrent neural networks,
with LSTM as a standout example, have been instrumental
in identifying the temporal dynamics of stock prices [11].

6Our dataset is available at https://github.com/hao1zhao/Bigdata23.

The integration of temporal attention mechanisms has fur-
ther enhanced the effectiveness of RNN-based models by
amalgamating information from multiple time points [12].
Moreover, other sophisticated neural frameworks, such as
convolutional neural networks and the Distillation model [13],
have demonstrated their aptitude in capturing the intricate non-
linear behaviors inherent to stock prices.

A significant drawback of these methods is their inability
to forecast price movements when the necessary information
is absent from historical prices. In this paper, our foremost
objective is to surpass these technical strategies by adeptly
integrating tweet and macroeconomic data into our model.

B. Fundamental analysis

It is widely believed that information sources such as tweets,
or government reports give meaningful evidence for stock
price prediction. Most fundamental analysis studies concerning
the financial market utilize text information for stock price
prediction, often employing data sources such as social media,
news articles, web search queries, or government documents.
Social Media: Various studies have ventured into predicting
stock market leveraging social media data. Recent research
employs sentiment analysis to fuse historical price data with
insights gathered from platforms like Yahoo’s message board
[14] , blogs [15], Twitter [4], and Reddit [16], subsequently
uncovering correlations with stock market trends.

News and Search Engine: Research has delved into the
intersection of public news and user browsing activity, inves-
tigating their influence on traders’ reactions to news events.
Studies like those by Xiong et al. [17] view Google trends
and market data as drivers behind daily S&P 500 fluctuations.
Other research, such as that by Bordino et al. [18], correlates
search query traffic with stock movements. Advanced methods,
such as the hierarchical attention mechanisms proposed by Hu
et al. [19], mine news sequences directly from texts for stock
trend prediction.

Macroeconomic Indicators: Studies have identified a mul-
titude of economic factors that contribute to the determination
of stock returns. Particularly, Ferson et al. [20] demonstrates
that interest rates play a pivotal role in shaping stock returns.
Furthermore, Jank et al. [21] suggests additional indicators like
the relative T-Bill rate and the consumption-wealth ratio to be
significant predictors. Besides these factors, economic indi-
cators such as unemployment rates, inflation, and commodity
prices have also been proven to notably influence stock returns
[22].

However, previous studies utilizing social network data
present two significant limitations. First, their predictions of
stock movements concentrate solely on the company, over-
looking the broader context of its industry, even though the
two are deeply interconnected. Second, these studies do not
incorporate a reliable information filtering mechanism, making
them susceptible to the plethora of misinformation prevalent
on social platforms and public media. This over-reliance can
compromise the accuracy of stock movement predictions.

Figure 1: The architecture of ECON is designed to predict the movement yst
v of stock s on day t. In the data
input phase, information related to stock prices and macroeconomic factors is extracted. An optimal subset of tweet data is
chosen via the tweet filter. Once company specifics are masked, ECON derives sector and tweet embeddings through self-aware
mechanism, subsequently generating macro and micro trend features. The attention GRU amalgamates these two trends with
the stock price features to make predictions.

m, and volatility yst

In this study, we aim to overcome the limitations of previous
methodologies. We adopt an efficient tweet filter to eliminate
redundant content and reduce noise. Additionally, we embrace
a multi-dimensional approach to the stock market, integrating
correlations of stock market information in a multi-task man-
ner. By skillfully combining the strengths of both technical and
model, we achieve state-of-the-art performance in predicting
stock movements.

III. PROBLEM SETUP

We offer a formal definition for the problem of predicting
stock movement and volatility. Let S denote a set of target
stocks for which predictions are sought. We define a set xst
of feature vectors, with s ∈ S and t ∈ T , which encapsulate
historical prices. Let T represents the set of available training
days. In addition, we identify a set ε consisting of tweets,
each referencing at least one stock in S. These historical
prices incorporate the opening, highest, lowest, closing prices
and adjusted closing price7 for each stock. Using P to
signify the adjusted closing price of stock s on day t as
P = {Ps,1, . . . , Pst}, we then express the actual labels for
movement and volatility as follows:

ˆyst
m = 1(Pst − Ps,t−1),

ˆyst
v = 1(Pst − Ps,t−1)/Ps,t−1.

(1)

(2)

We usually can generate multiple training examples by
shifting the time lag within extensive historical stock data in
practical situations. However, in order to simplify the expla-
nation of our proposed method, we focus on a specific time
lag for both movement and volatility prediction. We’ve also
integrated the use of adjusted stock prices into our predictive
models. [23]. This allows our models to learn from the genuine
changes in a stock’s value that occur due to market conditions,
rather than being swayed by artificial fluctuations resulting
from corporate actions.

IV. ECON STRUCTURE

We present an overview of data ingestion, text preprocess-
ing, and model architecture, as illustrated in Figure 1. Initially,
we process the historical stock prices and macroeconomic
factors to distill pertinent features. Following this, we sift
through Twitter data to identify tweets that most aptly reflect
public sentiment. During text preprocessing, we mask stock
tickers associated with companies found in the curated tweets.
These masked tweets are then transformed into embeddings.
Leveraging a self-aware mechanism, we generate embeddings
for both sectors and tweets. In the concluding step, we amal-
gamate information from stocks, sectors, and macroeconomic
indicators in a multi-trends approach to predict stock price
movement and volatility.

The crux of our problem lies in forecasting the binary move-
ment and volatility of each stock’s price on day t, using the
associated features and tweets within the lagged timeframe
[t − d, t − 1], where d represents a predefined window size.

7The adjusted closing price amends a stock’s closing price to reflect that

stock’s value after accounting for any corporate actions.

A. Data Ingest

Yahoo Finance. Yahoo Finance is a comprehensive finan-
cial news and data platform providing stock market informa-
tion, financial reports, and investment resources. We obtained
historical data from Yahoo Finance, tracking the performance
of 42 blue-chip stocks from June 1, 2020, to June 1, 2023.

To streamline our prediction targets, we set a threshold range
from -0.5% to 0.5% to exclude minor fluctuations, as in recent
works for stock movement prediction [4], [9]. According to
Baker et al. [24], daily stock price changes above 2.5% are
often considered significant. However, our model is designed
to predict unusual volatility. Therefore, in line with Ding et
al.’s [25] explanation on special stock fluctuation constraints,
we’ve set a higher benchmark, identifying a 5% fluctuation
as an abnormal change. Consequently, we label samples with
fluctuation percentages less than 5% as 0 and those equal to
or more than 5% as 1.

Google Trends. Google Trends is a tool offered by Google,
providing users with a normalized count of total searches for
specific terms within a given period. In this system, the volume
of searches is benchmarked against a normalized scale, where
100 represents the peak search interest and 0 the lowest. For
our study, we sourced our search data from web searches.
Drawing inspiration from Wikipedia’s ”Outline of economics”
page [26], we curated and refined our keyword list based on
expert knowledge. Given the weekly granularity of our data,
we partitioned our data retrieval into distinct time windows.
We then normalized the search volumes within each window
against each other, ensuring consistent normalization over the
entire three-year span. This approach allowed us to seamlessly
extract macroeconomic-specific trends by querying the chosen
keywords.

Federal Reserve Economic Data (FRED). The FRED
database, overseen by the Federal Reserve Bank of St. Louis,
contains over 816,000 economic time series from diverse
sources, encompassing sectors like banking, employment,
GDP, and more. Many of these series are gathered from gov-
ernment entities like the U.S. Census and the Bureau of Labor
Statistics. For our study, we used the same macroeconomic
keywords as in Google Trends, aiming to capture insights from
both societal and official viewpoints.

Twitter. Twitter is a social media platform where users
share and interact with brief messages called “tweets”. During
the same date range with Yahoo Finance data, we included
7.7 million tweets, gathered via Twitter’s official API at a
sampling rate of 10%. Our criteria for tweet selection was
stringent: they needed to contain at least one cashtag ($) and
had to be posted within standard U.S. trading hours. Besides,
we recognized the significant influence that Twitter volume
has on stock trading, a fact underscored by Cazzoli et al. [27].
Thus, we ensured that our stock feature matrix included the
daily count of processed Twitter posts.

B. Tweets filter

In stock analysis leveraging Twitter data,

two predomi-
nant text processing approaches emerge due to token length
constraints:
the first approach uses pre-trained models to
gauge tweet sentiments, converting these into sentiment scores.
Consider a tweet such as “With Vision pro, $AAPL is about
to soar!” This would likely be identified as positive in sen-
timent. These scores subsequently serve as features in model
analysis. However, this method’s primary shortcoming lies in

its exclusive emphasis on sentiment, overlooking the depth of
other textual nuances. Additionally, the vast sea of collected
tweets often contains a great deal of redundancy, with only a
fraction truly being insightful. This leads to an analysis that
may not genuinely capture the overarching public sentiment.
The second strategy employs a custom encoder to derive
features from tweets, setting a daily limit on the number
of tweets analyzed for each stock, often not surpassing a
few hundred tokens. At a glance, this may seem practical.
However, given the vast number of tweets we encounter daily,
often averaging in the hundreds with surges nearing 10,000,
this approach becomes increasingly untenable. Relying on a
limited, randomly chosen subset of tweets to extract influential
features not only tests our analytical prowess but also brings
into question the breadth and validity of the conclusions
drawn.

To tackle the challenges previously outlined, we developed a
sophisticated tweet filtering system. Each tweet’s sentiment is
ascertained via sentiment analysis. For stock s on day t, tweets
are prioritized according to their impressions. The number of
tweets retained daily is determined by assessing the correlation
between Sentiment and Movement, leveraging Cramer’s V
coefficient V . In our research, we consistently selected the
top six tweets, those garnering the highest daily impressions,
as our primary input for text preprocessing.

Sentiment Analysis. In our research, we undertook a sen-
timent analysis on a dataset comprising 7.7 million tweets.
To achieve this, we leveraged a language model based on
Roberta8, and the model has been fine-tuned on Twitter data.
This optimized model can produce one of three sentiment
outcomes for each tweet: positive, neutral, or negative. For
every tweet e associated with a blue-chip stock s, we compared
the predicted sentiment pe with the same day’s binary adjusted
price movement ys
m gauges the
price fluctuation between days t − 1 and t. Same as problem
setup, if the stock price ascends in this interval, ys
m is labeled
“Up”, and conversely, if it declines, it’s labeled “Down”. It’s
pertinent to note that our dataset only incorporated tweets from
the trading hours of day t. To aggregate daily sentiment, if a
day witnesses a predominance of negative sentiments in tweets
concerning a specific stock, we designate that day as “nega-
tive”. In contrast, if the tweets are primarily characterized by
positive sentiments, the day is classified as “positive”.

m of stock s. The measure ys

TABLE I : Observed frequency between stock sentiment and
stock price movement.

M ovement1 M ovement2

Sentiment1
Sentiment2
...
Sentimenti

O11
O21
...
Oi1

O12
O22
...
Oi2

... M ovementj
...
...
...
...

O1j
O2j
...
Oij

Tweets extracting. In addition to textual information, we
gathered metrics on likes, retweets, and impressions for each
tweet. Impressions on Twitter signify the cumulative count

8https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest.

(a) Top one impression tweet

(b) Top three impression tweets
Figure 2 : Confusion matrices for both sentiment scores and stock price movements. (a), (b), and (c) each depict the results of
selecting different numbers of tweets each day to typify that day’s Twitter dataset. We can observe a clear link between stock
price trends and tweet sentiments.

(c) Top six impressions tweets

of times a tweet appears in user timelines, search results, or
other viewing sections of the platform. While Twitter remains
reticent about the exact workings of its algorithm, Busch et.
al [28] explained that user actions affect how often a tweet
is pushed out, referred to as its “impressions”. This implies
that the greater the impression of a tweet, the more people
likely agree with its viewpoint. Therefore, using tweets with a
larger impression provides a better representation of society’s
perspective on that stock. Due to the constraints of the actual
production environment, it is hard to process thousands of
tweets simultaneously. Moreover, many of these tweets are
redundant, such as those without meaningful content or those
that are purely promotional. Therefore, we sorted the each
tweet e based on their impressions k and calculated the
Cram´er’s V coefficient V between the overall sentiment of
distinct daily tweets and price movement. Through the con-
tingency Table I, where each cell value represents an observed
frequency Oij, we can derive the expected frequencies as
follows:

Eij =

Rowi × Columnj
T otalij

,

(3)

where Rowi represents the total count for the ith sentiment
category, while Columni signifies the total count for the jth
movement category from the training set. T otalij encompasses
the sum of all values within the contingency table.

We iterated through all categories of the two variables,
utilizing the following chi-square statistic function to measure
the discrepancy between observed and expected frequencies:

(cid:88)

(cid:88)

χ2 =

i

j

(Oij − Eij)2
Eij

.

(4)

As shown in Figure 2, there is a discernible correlation
between Movement and Sentiment. In our study, for stock s
on day t, we sorted each tweet e based on its impression
count. Subsequently, we sequentially extracted distinct tweets
to construct multiple datasets m. By comparing the Cram´er’s
V for Sentiment and Movement across these datasets, we
ascertained their correlation, as presented in Table II. The
formula for the Cram´er’s V is provided as follows:

(cid:115)

V =

χ2
T otalij × (min(Rowi, Columnj) − 1)

.

(5)

Upon comparison, we observed that the Cram´er’s V for
datasets mk containing all tweets from the training set is
slightly higher. However, incorporating all daily tweets into
the model in practical applications presents a significant com-
putational overhead. Therefore, we opted to select the top six
tweets m6 for each stock daily as our finial data input, given
their commendable Cram´er’s V on a smaller scale.

TABLE II : A varying number of daily tweets are retained for
comparison using Cram´er’s V. For m1, only the tweet with
the highest impression each day is retained. Following this
pattern, mk represents the retention of all tweets.

m1

m2

m3

m4

m5

m6

m7

m8

... mk

V 0.0394 0.0643 0.0747 0.0816 0.0833 0.0877 0.0872 0.0853 ... 0.0917

C. Text preprocessing

In natural language processing (NLP), text preprocessing
has long been recognized as a key step. By streamlining
text into a format more amenable to training, it not only
simplifies the content but also boosts the efficiency of machine
learning algorithms. Shifting focus to the concept of self-
aware mechanism, our main objective is pinpointing sector
identification. Within this context, the embeddings of both
sectors and tweets are crucial. When the stock symbol s is
masked, we aim to identify the sector c a tweet e refers to
based solely on its content.

Tokenization. Tokenization is the process by which text is
divided into smaller units, such as sentences, words, charac-
ters, or subwords. Word tokenization, specifically, refers to the
segmentation of text into individual words. In our research, we
utilized spaCy9—an open-source natural language processing
tool—for word tokenization of every tweet e. Moreover, a

9https://spacy.io/

stock ticker like “AAPL” for Apple might inadvertently be
tokenized into “AA” and “PL”. To circumvent such issues,
we established specific rules to ensure that stock tickers are
consistently recognized as single, intact tokens. After tokeniza-
tion, words are converted to lowercase because stock tickers
won’t overlap with regular vocabulary. Such normalization not
only ensures vocabulary consistency across the dataset but also
augments computational efficiency.

Company Masking. For the subsequent self-awareness
portion of the model architecture, we replace all tokens in
εs that correspond to the name of stock s with a special token
[mask]. Based on the sector classification rules of GISC, we
maintained a dictionary mapping the masked companies to
their respective sectors. For example, after data filtering, we
obtain a tweet with a high impression: “With Vision pro,
$AAPL is about to soar!” Next, we mask the stock ticker,
converting it to: “With Vision pro, [mask] is about to soar!”
Sentence encoding. Words from the sentence are first
tokenized and then mapped to their corresponding word em-
beddings. These embeddings are high-dimensional vectors that
capture the intrinsic semantics of each word. We record the
original length of each tweet, storing this information in the
variable l. This data proves crucial when padding comes
into play, a technique used to append or prepend sequences
with filler values, ensuring uniformity. Given the importance
of maintaining consistent sequence lengths, especially when
working in batches, padding ensures our dataset’s sequences
adhere to a standardized length, adding consistency and struc-
ture to our data processing pipeline.

D. Model Architecture

For blue-chip companies, the categorization into sectors is
primarily based on their core business operations. However,
given the substantial scale of these enterprises, their business
models are inherently diverse and not confined to a single do-
main. In fact, many such corporations have a presence across
multiple industries. Take Apple as an example:
its robust
cash flow and stable user base have increasingly amplified its
influence in the financial sector. The rapid growth of Apple
Card’s high-yield savings account10, amassing over $10 billion
in deposits from users within just four months since its launch
in April 2023, serves as a testament to this diversification.
On the other hand, The rise and fall of a company’s stock
price are often influenced by its respective sector. Whenever
a significant event occurs, the impact varies across different
sectors [29].

Self-aware mechanism. Our methodology for modeling
sector and tweet embeddings takes two distinct approaches.
For sectors, the embedding hc of each sector c is learned as a
free parameter, updated dynamically during backpropagation.
Each embedding hc is the c-th row of the sector embedding
matrix C. Conversely, for tweets, in line with our earlier
discussions on company masking, we then deploy a predictor

10The saving account

jointly launched by Apple and Goldman Sachs,
offering an annual percentage yield significantly above the national average.

network and harness its output f (e) based on the tokenized
content of each tweet e to derive the embedding he.

Given the intrinsic capability of recurrent neural networks
to effectively manage long-term dependencies,
they are a
preferred choice for sequential data processing, as evidenced
by various literature [30]. In our approach, we adopted a
bi-directional LSTM (BiLSTM) for the predictor network to
produce the tweet and sector embeddings. The reason why we
use BiLSTM is because LSTM cannot simultaneously account
for the context on both sides of the masked token—a key
requirement for our sector identification task. To be more
precise, we execute the BiLSTM and utilize the state vector
produced at the masked token position as he. This ensures our
model places emphasis on the sector context surrounding the
masked token rather than the end context of the tweet.

We train three distinct sets of parameters: the word token
embeddings, the parameters within the predictor network, and
the sector embeddings. Utilizing the tweet embedding, denoted
as he, generated by the predictor network, we define our
prediction for the sector c as:

yec =

(cid:80)

exp(hT
c he)
c′∈C exp(hT

c′he)

,

(6)

where c′ represents all possible sectors within the set C. For
our dataset, we can categorize it into 10 distinct sectors.

The sector self aware training is done separately from the
predictions section. To optimize our model, we adjust all
parameters to minimize an objective function derived from
the cross entropy loss, detailed as follows:

l(θ) = −

(cid:88)

(cid:88)

c∈C

e∈εs

log yec.

(7)

As highlighted by Soun et. al [9], maximizing the dot
product between sector and tweet embeddings during the self-
aware phase allows for these embeddings to be seamlessly in-
terchanged within the primary model designed for stock move-
ment prediction. Our expectations from the sector self-aware
mechanism are twofold: Firstly, the tweet embedding, while
principally revealing details about its respective company, also
imbibes information from the sector embedding under which
multiple companies fall. This allows for a richer contextual
understanding of the company in the wider industry setting.
Secondly, Blue-chip stocks, given their expansive reach across
various sectors, possess unique sector embeddings. By virtue
of these companies operating in diverse sectors, amalgamating
information from these sectors not only offers a more com-
prehensive view but can also enhance the accuracy of our
predictions.

Macro attention. Given that stock price movements are
influenced not only by their respective sectors but also by over-
arching macroeconomic market factors, it becomes imperative
to craft a comprehensive market summary for more accurate
predictions.

In our model, a comprehensive market index is constructed
by amalgamating price-related information from diverse sec-
tors and incorporating pertinent macroeconomic factors. This

synthesis results in the macro trend vector at, which encap-
sulates holistic market data for subsequent predictions. Given
embedding vectors hc for each sector c and he for every tweet
e, we derive the daily average tweet embeddings, denoted
by rt ∈ R2k. Utilizing rt as the query vector, the attention
mechanism is employed to integrate price-related attributes
across sectors using their specific embeddings. The specifics
of the attention process are detailed as follows:

at =

(cid:88)

c∈C

αctxct where αct =

exp(rT
t hc)
c′∈C exp(rT

(cid:80)

t hc′)

,

(8)

where xct ∈ Rm×q represents the macroeconomic and price-
related feature of sector c on day t, and αct serves as the
attention weight for aggregation. Figure 1 macro attention part
depicts the Macro trend derivation process. For each day t,
the mean tweet embedding rt, is computed from all tweet
embeddings. This average embedding serves as the query
in the attention mechanism, while the matrix C ∈ Rm×2k
representing sector embeddings acts as the key. The atten-
tion mechanism’s outcome is an aggregated macro feature
matrix, which is comprised of data from FRED and Google
Trends. Then it yields the macro trend vector at ∈ Rq.
In the subsequent predictive phase, this generated vector is
channeled into the Attention GRU. This provides both macro-
level perspectives and a comprehensive understanding of the
sector in which the company is positioned.

Micro attention. Many blue-chip companies operate across
multiple sectors. This diversification allows them to hedge
against downturns in one sector by benefiting from the rise in
another. Hence, stock price movements and volatility for these
companies cannot be solely determined by macro trends. We
propose to make a micro trend vector ict for each sector c
at day t, which combines the stock information and relevant
sectors. The micro trend ict provides cross-sectoral involve-
ment information for each stock s, which is distinct from the
Macro trend vector at that takes into account the influences of
macroeconomics and societal hot topics for all target sectors.
The first attention is to find tweets relevant to each target sector
c. We find relevant tweets by using the embedding hc of sector
c as the query of attention as follows:

wct =

(cid:88)

e∈εt

αehe where αe =

exp(hT

c he)
exp(hT

(cid:80)

e′∈εt

c he′)

.

(9)

let εt represent

Considering a day t,

the collection of
tweets. For each tweet e within this set, its corresponding
embedding is denoted by he. Additionally, αe signifies the
attention weight attributed to the tweet. The vector wct ∈ R2k,
which can be envisioned as a weighted mean of all the tweet
embeddings for day t. This averaging is influenced by the
pertinence of each tweet to the specific target sector c. Then,
we employ a linear layer to project wct onto all possible
companies S:

gst = W1(wct) + b1,

(10)

where W1 ∈ Rn×2k, b1 ∈ Rn. As a result of the attention
function of Equation (10), gst ∈ Rn represent that summarizes
all tweets at day t considering the relevance to sector c. Our
second attention uses the resulting tweet vector to find the
most relevant stocks at the moment. This is done by using gst
as the query vector of attention to aggregate the stock price
features at day t based on the stock embeddings:

ist =

(cid:88)

s∈S

αstxt where αst =

exp(gT
stxt)
s′∈S exp(gT

stxt′)

(cid:80)

.

(11)

The micro trend vector ist ∈ Rn encapsulates all tweets for
day t, integrating the information from companies associated
with the sector. This vector is then combined with the stock
price vector xt ∈ Rn×p which generates from Yahoo’s stock
data and the macro trend vector at, serving as the input to the
Attention GRU for forecasting stock movement and volatility.
Predictions layer. The model is primarily designed to ex-
tract nuanced information from stock market, macroeconomic
factors, and tweet data. Once the macro trend at and micro
trend ist are generated, they are concatenated with the stock
price feature xst ∈ Rp using a linear layer, producing a multi-
level feature. The corresponding function is as follows:

˜xst = W2(xst ⊕ at ⊕ ist) + b2,
where W2 ∈ Rn×(p+q+n), b1 ∈ Rn are the learnable weight
and bias terms, respectively, ⊕ is the concatenation operator
between vectors.

(12)

Attention GRU with temporal distance (AGRUD). Given
the proficiency in handling long-term dependencies, recurrent
neural network is extensively employed for sequential data
processing [30]. The general idea of recurrent unit is to recur-
rently project the input sequence into a sequence of hidden rep-
resentations. At each time-step for stock s, the recurrent unit
learns the hidden representations hst by jointly considering the
input ˜xst and previous hidden representation hs,t−1 to capture
sequential dependency. To capture the sequential dependencies
and temporal patterns in the historical stock features, an GRU
recurrent unit [31] is applied to map {˜xs1, . . . , ˜xst} into
hidden representations {hs1, . . . , hst}. Then, We implement
a distance matrix to apply weights to the hidden states of the
GRU:

h′
st = hst ×

,

(13)

1
∆d

where ∆d represents the distance between a specific day
within the window d and the day t. This approach ensures that
hidden states further away from the current time step receive
progressively lower weights, emphasizing the relevance of
more recent states. Then, the states are combined by attention
as follows:

S
(cid:88)

T
(cid:88)

hatt =

αsth′

st where αst =

(cid:80)S

uT h′
st
(cid:80)T
t′=1 uT h′

,

t=1

s=1

s′t′
(14)
in this context, u is a learnable parameter, commonly referred
to as the attention query. It serves to select the most pertinent

s′=1

time steps by evaluating the outcome of the dot product.
The derived weight αst indicates the degree to which step
t contributes to hatt.

Attention GRU with temporal distance generates the first
output hout1 = h′
st ⊕ hatt by concatenating the hidden state
h′
st of the last time step and the output hatt of the attention,
where ⊕ is the concatenation operator between vectors. The
last hidden state h′
st is used in addition to hatt as the basic
output that summarizes all given features apart from the result
of attention. Building upon the previous output hout1, we
further concatenate hatt and h′
st to produce the second output
hout2 = hout1 ⊕ hatt ⊕ h′

st.

We utilize the attention GRU with temporal distance, as
introduced above, as our primary tool for forecasting stock
movement and volatility. The model take the output ˜xst from
the linear layer as its input. The subsequent formula outlines
the methodology for predicting stock price movements:

v , yst
yst

m = AGRUD(˜xs,t−d, ˜xs,t−d+1, . . . , ˜xst),

(15)

where d is the window size, which is chosen as a hyperpa-
rameter between 5 and 15 in our experiments.

Optimization. We update all parameters to minimize the
following objective function, which is based on the cross
entropy function for stock price movement:

l(θ) = −

(cid:88)

(cid:88)

s∈S

t∈T

log yst
m,

(16)

where θ is the set of learnable parameters of AGRUD, T is
the set of available days in training data , S is the set of target
stocks.

We also train AGRUD to minimize the following objective

function for stock price volatility:
(cid:88)

(cid:88)

L(θ) = −

(cid:2)ˆyst

v log(σ(yst

v )) + (1 − ˆyst

v ) log(1 − σ(yst

v ))(cid:3)

s∈S

t∈T

(17)
where ˆyst
v ∈ {0, 1} is the true label of stock s at day t. As
previously mentioned, the true label for stock price volatility
is expounded upon in equation (2).

V. EXPERIMENT

We conduct experiments to answer the following questions

about the performance of framework:
Q1. Baseline methods. How do other baseline models perform
on our task?
Q2. Tweets quality. Does using our model to extract data
yield better results than performing sentiment analysis using
the entire 7.7 million tweets as input?
Q3. Ablation study. How do the macro and micro trend affect
its performance?

A. Baseline Methods

We evaluate our model’s effectiveness by comparing it with
technical and fundamental analysis models. Technical analysis
models focus on capturing patterns of historical prices and

TABLE III : Classification performance of ECON and baseline
methods, measured with the accuracy (ACC) , the Matthews
correlation coefficient (MCC) and Receiver Operating Char-
acteristic (ROC). The best is in bold, Our ECON shows the
best performance in all evaluation metrics.

Models

XGBoost
ARIMA
ALSTM
Adv-LSTM
DTML
SLOT
ECON

Movement

Volatility

Acc.

49.30
50.71
50.22
51.76
51.95
52.45
53.36

MCC

0.0146
0.0031
0.0453
0.0374
0.0578
0.0617
0.0754

Acc.

52.40
53.61
56.43
58.84
59.79
60.83
62.84

MCC

-0.0127
-0.0032
0.0624
0.0529
0.0703
0.0628
0.0861

AUC

55.61
56.89
59.58
58.93
61.06
60.52
63.54

fundamental analysis models combine historical prices with
other government and financial market information for stock
price movement and volatility prediction.
• Extreme Gradient Boosting (XGBoost) [32] is a high-
performance gradient boosting framework renowned for its
efficiency in handling large datasets, regularization features,
and capability in both classification and regression tasks.
• Autoregressive Integrated Moving (ARIMA) [33] is a fore-
casting model for time series data, consisting of autoregressive
(AR), differencing (I), and moving average (MA) components.
It assumes data linearity and stationarity.
• Long Short-Term Memory (LSTM) [34] is a type of
recurrent neural network (RNN) architecture specifically de-
signed to tackle the vanishing and exploding gradient problems
encountered in traditional RNNs. LSTM units use gate mech-
anisms to regulate the flow of information, making them well-
suited for learning from long-term dependencies in sequences.
• Attention LSTM (ALSTM) [12] is a LSTM network with
an attention mechanism, enhancing sequence modeling by
emphasizing relevant elements.
• DTML [35] is based on Transformer, it learns temporal
correlations and combines the contexts of all target stocks.
• SLOT [9] is based on ALSTM, it captures correlations
between tweets and stocks through self-supervised learning.

To test whether our Twitter filter can still accurately capture
market sentiment even with a significantly reduced Twitter
input:
• AGRUD-A concatenated the sentiment score of all 7.7
million tweets with stock prices and macroeconomic data,
keeping the structure of main predictor AGRUD unchanged.
• AGRUD-F concatenated the sentiment score of data refined
with stock prices and macroeconomic data, keeping the struc-
ture of main predictor AGRUD unchanged.

Evaluation metrics. Following previous work for stock
movement prediction, we adopt the standard measure of Accu-
racy (ACC) and Matthews Correlation Coefficient (MCC) as
evaluation metrics. MCC avoids bias due to data skew. Given
the confusion matrix containing the number of samples clas-
sified as true positive (tp), false positive (f p), true negative
(tn) and false negative (f n), Acc. and MCC are calculated as

TABLE IV : This table illustrates the effectiveness of the tweet
filter. By retaining only the tweets with the highest impres-
sions, the results for predicting stock price movement and
volatility are consistent with those obtained using sentiment
analysis on all tweets.

Models

Tweets

Movement

Volatility

Acc.

MCC

AGRUD-A
AGRUD-F
ECON

7,765,542 52.38
52.21
193,147
53.36
193,147

0.0518
0.0564
0.0754

Acc.

61.36
60.68
62.84

MCC

0.0747
0.0694
0.0861

AUC

61.97
59.93
63.54

even though the number of tweets significantly decreased,
AGRUD-A and AGRUD-F results remained almost the same.
This demonstrates that the filtered tweets effectively represent
people’s genuine intentions. Consequently, this allows ECON
to achieve more accurate forecasting results at a reduced cost
and with greater efficiency.

TABLE V : An ablation study of ECON, where ECON(W/O
AI) is baseline without any tweet and macroeconomic data.
ECON(A) is without the micro trend and ECON(I) is without
macro trend. Both macro and micro trend improve the perfor-
mance of the framework.

follows:

Acc. =

tp + tn
tp + tn + f p + f n

,

(18)

Models

.

(19)

M CC =

tp × tn − f p × f n
(cid:112)(tp + f p)(tp + f n)(tn + f p)(tn + f n)
For stock volatility prediction, given that data points in-
volving stock price changes greater than 5% only constitute
a minor portion of our dataset, using accuracy to evaluate
ECON can be misleading due to model overfitting. When
evaluating the imbalanced data, we use Area Under the Curve
- Receiver Operating Characteristic (AUC-ROC) where AUC
is the area under the ROC curve, which falls in the range
[0,1]. AUC is insensitive to the absolute numbers of positive
and negative samples. It primarily focuses on how the model
distinguishes between positive and negative samples. AUC is
the area under the ROC curve, which falls in the range [0,1].
AUC is calculated as follows:

AU C =

(cid:90) 1

0

T P R(F P R) dF P R,

(20)

where T P R(F P R) represents the True Positive Rate as a
function of the False Positive Rate. The T P R gives the
proportion of actual positives that are correctly identified.
F P R is the False Positive Rate, indicating the proportion of
negative samples that are mistakenly identified as positive.

B. Results

The performances of our ECON and the established base-
lines are detailed in Table III. ECON is observed to be the
superior baseline model in terms of accuracy and MCC for
movement prediction. While DTML shows an outstanding
performance in the AUC and MCC for volatility prediction.
ECON surpasses both of these models by significant margins.
In terms of accuracy, ECON achieves a score of 53.36 and
62.84, outperforming SLOT and DTML by 1.7% and 2.7%
respectively, and outshines in AUC by a margin of 4.0%
and 4.0%. We argue this improvement of price movement
prediction is attributed to use of macro and micro trends
to investigate the influence of multiple sectors on stocks.
On the other hand, leveraging macro insights and placing a
stronger emphasis on recent information have also significantly
contributed to improved volatility forecasting.

The performance results for the tweets filter can be seen
in Table IV. When relying solely on sentiment analysis,

Movement

Volatility

Acc.

52.16
51.52
49.71

53.36

MCC

0.0513
0.0392
0.0012

0.0754

Acc.

57.93
55.58
53.47

62.84

MCC

0.0517
0.0321
0.0399

0.0861

AUC

60.09
57.53
55.40

63.54

ECON(A)
ECON(I)
ECON(W/O AI)

ECON

C. Ablation Study

An ablation study of ECON, where we have constructed
three variations alongside the fully-loaded model. Each variant
is specifically tailored to handle certain types of input data:
ECON(A) solely relies on macro trend, ECON(I) exclusively
processes micro trend while ECON(W/O AI) omits both macro
and micro trends information. As shown in Table V, our
ablation study revealed that incorporating multi-dimensional
perception of stock market significantly enhances the pre-
dictive capabilities of the model for stock movement and
volatility. A noteworthy point is that the result of ECON(A)
demonstrates a significant role of the macro trend in enhancing
the accuracy of volatility prediction.

VI. 