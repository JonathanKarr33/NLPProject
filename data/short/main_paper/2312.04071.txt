INTRODUCTION
ACM Reference Format:
Zijie Huang, Baolin Li, Hafez Asgharzadeh, Anne Cocos, Lingyi Liu, Evan
Cox, Colby Wise, and Sudarshan Lamkhede. 2023. Synergistic Signals: Ex-
ploiting Co-Engagement and Semantic Links via Graph Neural Networks .
In Proceedings of ACM Conference (WWWâ€™24). ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
WWWâ€™24, May 2024, Singapore
Â© 2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Recommender systems at Netflix rely on entity (e.g. video title,
game, actor) embeddings that accurately reflect the similarity be-
tween entity pairs. In many cases these representations are learned
from user co-engagement data [19, 24, 28, 29]: if two entities are
frequently co-engaged by users, they should have similar embed-
dings. A well-known drawback of leveraging user collaborative
signals in isolation to train entity embeddings is that these data
suffer popularity biases and have low coverage for new and unpop-
ular items [21, 42]. At the same time, there may exist rich semantic
information about items (such as genre, content maturity level,
intellectual property, and storyline). This semantic data can pro-
vide an important signal in learning entity representation and also
provides fine-grained interpretability for similarity scores.
Figure 1 provides a concrete example. Based on co-engagement
signals alone, a title similar to â€œTitanicâ€ which was released in
1997 is â€œPride & Prejudice â€, which is also a popular romantic movie
MoreLikeThisYear1997Pride&Prejudice(2005)Samegenre(romantics)HighSeas(2020)Similarstoryline(seas)>=100DegreeMean (EEL)=38.7915DegreeMean (noEEL)=3.5683NumberofNeighbors(Degree)TitleDegreeDistributione!"e#$e%$e&"eâ€™$e($e#"eâ€™"e%"e("Entity-entityLinks (EEL)Semanticrelations(a)Titledegreedistribution(b)SemanticKnowledgeGraph 
WWWâ€™24, May 2024, Singapore
Zijie Huang, Baolin Li, Hafez Asgharzadeh, Anne Cocos, Lingyi Liu, Evan Cox, Colby Wise, and Sudarshan Lamkhede
released in 2005. However, another more recent similar title is â€œHigh
Seasâ€ released in 2020. Compared with â€œPride & Prejudiceâ€, it has
a weaker co-engagement signal with "Titanic" but is semantically
similar in the sense that both stories take place on the sea. Therefore,
semantic information yields a more diverse set of similar titles and
possibly helps us to understand why two movies are similar, e.g.
due to genre or storyline.
Motivated by these observations, we develop a novel method to
learn entity embeddings from both co-engagement and content-
based semantic information. Specifically, we train a Graph Neural
Network (GNN) [20, 33] over the knowledge graph (KG) [3, 22] de-
picted in Figure 2(b). This enables us to capture both co-engagement
and entity-specific semantic information in the GNN embeddings
by modeling entities, semantic concepts (e.g. genres, maturity levels,
storylines), and their relationships jointly.
GNNs [10, 13, 20] have gained significant attention in recent
years due to their superior ability to perform representation learn-
ing on graph-structured data [12, 33, 38]. They have been suc-
cessfully employed to perform industrial tasks such as building
recommender systems [7, 11], modeling user interest [30, 31, 35],
etc. However, there are three unique challenges in training on this
particular KG within Netflix, which existing GNN architecture
designs are not fully aware of.
(1) Relation type class imbalance: most entity nodes have se-
mantic relations (i.e. links to semantic concept nodes), but
entity-entity co-engagement links (EEL) between entity pairs
are sparse as illustrated in Figure 2(a). Moreover, the number
of semantic concepts associated with each entity also varies,
resulting in imbalanced semantic edge distribution as well.
(2) Lack of informative features for semantic concept nodes:
Unlike entities that are usually associated with abundant
textual or visual features, semantic concepts are represented
as short phrases that only contain limited information.
(3) Graph scalability issue: To handle web-scale graph data, we
must design an efficient distributed training framework to
train over the entire graph with millions of nodes and billions
of edges on GPUs.
To this end, we propose SemanticGNN, a large-scale end-to-
end training pipeline for learning entity embeddings through a
relation-aware GNN. We design a two-step training strategy, where
we first run pre-training via a KG completion task to generate
contextualized representations of semantic nodes (e.g., genre), and
second train a novel GNN model via link prediction loss utilizing the
more sparse supervision signals from co-engagement relationships
between entities. Specifically, the KG pre-training stage addresses
challenge (ii) by forming representative embeddings for semantic
concept nodes, in contrast to directly using conceptsâ€™ short phrases
as textual features.
For challenge (i), we design a novel relation-aware GNN that is
able to distinguish the influence of different neighbors of a node
through attention weights. The attention weights are aware of dif-
ferent relation types such as has_genre and has_maturity_level.
In this way, for a newly released title that lacks any co-engagement
data, we are able to distinguish the influence of different semantic
types and thus learn an informative embedding. Distinguishing
relation types also helps us to better represent popular titles: for a
popular title that has abundant co-engagement links, thanks to the
learnable prior weights of different relation types, the model is able
to automatically adjust the influence received from co-engagement
and from semantic edges, thus preventing noisy co-engagement
data from dominating its representation.
To address the scalability concerns in challenge (iii), we design
a Heterogeneity-Aware and Semantic-Preserving (HASP) subgraph
generation scheme to train our model across multiple GPUs â€“ such
a framework has been proven efficient and feasible in an industry
production environment. Our proposed method, SemanticGNN, is
able to generate general-purpose entity representations that out-
perform baseline GNN-based entity embeddings in recognizing
semantic similarity (35% improvement) and co-engagement (21%
improvement). We also show that our method is especially powerful
compared with baselines in the inductive setting and for learning
new title representations. The model has also been successfully
deployed within Netflix.
2 PROBLEM DEFINITION
A semantic knowledge graph describes entities and their associated
semantic concepts such as genre. Formally, we represent a semantic
knowledge graph as G(Vð‘¡ , Vð‘, Eð‘¡ð‘, Eð‘¡ð‘¡ ), where Vð‘¡ , Vð‘ are the sets
of entity nodes and concepts nodes (e.g. core_genre) respectively.
The number of entity nodes is much larger than that of the con-
cept nodes, i.e. |Vð‘¡ | >> |Vð‘ |. There are two relation sets: (1) Eð‘¡ð‘
are the directed entity-concept edges where each edge ð‘’ð‘¡ð‘ points
from an entity node ð‘£ð‘¡ to a concept node ð‘£ð‘ . We denote (ð‘£ð‘¡ , ð‘’ð‘¡ð‘, ð‘£ð‘ )
as a semantic triple such as (Titanic, has_genre, romantic).
We use T = {(ð‘£ð‘¡ , ð‘’ð‘¡ð‘, ð‘£ð‘ )} to denote the set of factual semantic
triples. (2) Eð‘¡ð‘¡ are the undirected entity-entity links (EEL) obtained
from user co-engagement data where if two titles are frequently
co-engaged by users, an EEL would be created to denote their sim-
ilarity. As a consequence of using user co-play data, such EELs
are usually sparse and only cover a small portion of titles (biased
toward popular titles) as shown in Fig 2(a).
Given a semantic knowledge graph G(Vð‘¡ , Vð‘, Eð‘¡ð‘, Eð‘¡ð‘¡ ), we would
like to learn a model that effectively embeds entities to contextual-
ized latent representations that accurately reflect their similarities.
We evaluate the quality of the learned embeddings based on differ-
ent entity pair similarity measurements, described in Section 6.
3 RELATED WORK
3.1 Graph Neural Networks (GNNs)
GNNs constitute a category of neural networks designed to directly
process graph-structured data such as social networks [35], multi-
agent dynamical systems [14], etc. They are applicable to diverse
downstream tasks such as node classification [20, 33], graph clus-
tering and matching [2], etc. Although diverse architectures exist,
a typical update procedure for a single GNN layer involves two
primary operations: (1) Information Extraction from Neighbors. In
this step, information is gathered from each neighboring node. For
instance, GAT [33] computes attention scores between node pairs
based on sender and target node representations. These scores are
then multiplied by the sender nodeâ€™s representation to yield the
extracted information. GCN [20] employs the normalized Laplacian
as the attention weight, treating it as an approximation of spectral
Synergistic Signals: Exploiting Co-Engagement and Semantic Links via Graph Neural Networks
WWWâ€™24, May 2024, Singapore
domain convolution for graph signal processing. (2) Information
Aggregation from Neighbors. Basic aggregation methods, such as
mean, sum, and max, are commonly used. Additionally, more ad-
vanced pooling and normalization functions have been proposed.
ð‘¡ represents the node embedding for the target node ð‘¡
Suppose ð’‰
during the (ð‘™)-th GNN layer. In this case, the update procedure from
the (ð‘™-1)-th layer to the (ð‘™)-th layer can be succinctly described as
ð‘¡ â† Aggregate
âˆ€ð‘  âˆˆð‘ (ð‘¡ )
(cid:16)Extract (cid:16)
, ð‘’ð‘ â†’ð‘¡
(cid:17)(cid:17)
where ð‘ (ð‘¡) denotes the neighbor (source) node sets of the target
node ð‘¡ and ð‘’ð‘ â†’ð‘¡ denotes the edge type between pairs of the tar-
get node and source node. Finally, through the accumulation of
multiple layers (e.g. ð¿ layers), each nodeâ€™s output representation,
denoted as ð’‰
ð‘¡ , incorporates messages from an expanded neighbor-
hood. This results in a broader reception field, fostering high-order
interactions among nodes and resulting in a more contextualized
node representation.
3.2 Knowledge Graph Embeddings (KGE)
Knowledge graph embeddings [1, 4, 15, 32] seek to acquire latent,
low-dimensional representations for entities and relations, which
can be utilized to deduce hidden relational facts (triples). They
measure triple plausibility based on varying score functions. For in-
stance, translation-based models like TransE [4] interpret relations
as simple translations from a head entity to a tail entity. Extending
this concept, TransH [37] projects entities into relation-specific
hyperplanes, accommodating distinct roles of an entity in vari-
ous relations. RotatE [32], in contrast, characterizes relations as
rotations in the complex embedding space, capturing semantic prop-
erties like compositional relations. Later on, ConvE [1] introduces
a learnable neural network in computing the score function, and
some recent studies explore leveraging textual input of entities and
relations to enhance triple prediction tasks, integrating efficient
language models like KG-Bert [39].
3.3 Scaling up GNN Training
Training GNNs on large-scale graphs is a major bottleneck of to-
dayâ€™s GNN training when trying to take advantage of GPU ac-
celeration [6, 36, 43], as large graphs come with large memory
and computation requirements, and requires system solutions to
make these GNNs trainable on GPU memory and distributed across
multiple GPUs [41]. Various system solutions tackle this prob-
lem by distributing the full graph across multiple GPUs and us-
ing inter-GPU communication to pass messages between nodes
across different GPUs: Neugraph [26] and Roc [16] introduce dis-
tributed full-graph training by partitioning the graph across multi-
ple GPUs and optimizing the communication and CPU/GPU mem-
ory management. Several following works have then focused on
approximation and quantization to further improve distributed
training speed [5, 34, 40]. In contrast to the full-graph approach,
Pagraph [23] and DistDGLv2 [44] use random sampling from the
full graph to train on smaller graphs with optimized node caching
and data movement. SALIENT [17] and BGL [25] have further opti-
mized the dataloader and I/O of the sampling-based training process.
However, these approaches are limited in the training of our Se-
manticGNN due to resource constraints and graph heterogeneity
(Sec. 5.1).
Figure 3 shows the overall framework of our proposed Seman-
ticGNN. It follows a two-step training pipeline: 1.) We first pretrain
the semantic KG via KG completion loss using TransE in order to
generate the embeddings for concept nodes. 2.) We then train a
relation-aware GNN over the KG using link prediction loss over
EEL to get entity embeddings that reflect entity similarity. Now we
introduce each component in detail.
4.1 KG Pretraining
Our goal in KG pretraining is to produce high-quality features for
semantic concept nodes, as concept nodes are usually associated
with short phrases, which may not be informative enough to serve
as input features. We choose TransE [4] as our backbone and con-
duct KG pretrating via the standard KG completion task [15]. It can
be replaced with any off-the-shelf KG embedding method based on
different downstream applications and KG structures. Specifically,
let ð’†ð‘¡ , ð’†ð‘ be the learnable embeddings of entity ð‘¡, ð‘ respectively, we
train entity embeddings via the hinge loss over semantic triples
T = {(ð‘£ð‘¡ , ð‘’ð‘¡ð‘, ð‘£ð‘ )} defined as:
(cid:2)ð‘“ (cid:0)ð’†â€²
ð‘¡ , ð’“ð‘¡ð‘, ð’†â€²
(cid:1) âˆ’ ð‘“ (ð’†ð‘¡ , ð’“ð‘¡ð‘, ð’†ð‘ ) + ð›¾ (cid:3)
where ð›¾ > 0 is a positive margin, ð‘“ is the KGE model, and ð’“ð‘¡ð‘ is the
embedding for the relation ð‘’ð‘¡ð‘ . (ð’†â€²
ð‘ ) is a negative sampled
triple obtained by replacing either the head or tail entity of the true
triple (ð’†ð‘¡ , ð’“ð‘¡ð‘, ð’†ð‘ ) from the whole entity pool.
ð‘¡ , ð’“ð‘¡ð‘, ð’†â€²
4.2 Relation-Aware GNN
To handle the imbalanced relation distribution in the constructed
KG, we propose an attention-based relation-aware GNN to learn
contextualized embeddings for entities following a multi-layer mes-
sage passing architecture.
In the ð‘™-th layer of GNN, the first step involves calculating the
relation-aware message transmitted by the entity ð‘£ð‘¡ in a relational
fact (ð‘£ð‘¡ , ð‘’ð‘¡ð‘, ð‘£ð‘ ) using the following procedure:
ð‘ (ð‘Ÿ ) = Msg
ð‘£Concat(ð’‰
ð‘ (ð‘Ÿ ) is the latent representation of ð‘£ð‘ under the relation type
ð‘Ÿ at the ð‘™-th layer, Concat(Â·, Â·) is the vector concatenation function, ð’“
is the relation embedding and ð‘¾ð‘™
ð‘£ is a linear transformation matrix.
Then, we propose a relation-aware scaled dot product attention
mechanism to characterize the importance of each entityâ€™s neighbor
to itself, which is computed as follows:
(cid:205)ð‘£ð‘â€² âˆˆ N (ð‘£ð‘¡ ) exp
(cid:17)ð‘‡
Â· ð›½ð‘Ÿ ,
WWWâ€™24, May 2024, Singapore
Zijie Huang, Baolin Li, Hafez Asgharzadeh, Anne Cocos, Lingyi Liu, Evan Cox, Colby Wise, and Sudarshan Lamkhede
where ð‘‘ is the dimension of the entity embeddings, ð‘¾ð‘™
two transformation matrices, and ð›½ð‘Ÿ is a learnable relation factor
for each relation type ð‘Ÿ . Diverging from conventional attention
mechanisms [2, 33], we incorporate ð›½ð‘Ÿ to represent the overall sig-
nificance of each relation type ð‘Ÿ . This is crucial, as not all relations
equally contribute to the targeted entity depending on the overall
KG structure.
We then update the hidden representation of entities by aggregat-
ing the message from their neighborhoods based on the attention
ð‘¡ + ðœŽ (cid:169)
ð‘£ð‘ âˆˆN (ð‘£ð‘¡ )
where ðœŽ (Â·) is a non-linear activation function, and the residual
connection is used to improve the stability of GNN [9]. Finally, we
stack ð¿ layers to aggregate information from multi-hop neighbors
and obtain the final embedding for each entity ð‘– as ð’‰ð‘– = ð’‰
4.3 Overall Training
Given the contextualized entity embeddings, we train the model
using standard link prediction loss defined over entity-entity links:
log (cid:0)ð‘ (cid:0)ð‘’ð‘– ð‘— (cid:1)(cid:1) + log (cid:0)1 âˆ’ ð‘ (cid:0)ð‘’ð‘– ð‘— â€² (cid:1)(cid:1) ,
ð‘’ð‘– ð‘— âˆˆEð‘¡ð‘¡
ð‘’ð‘– ð‘— â€² âˆ‰Eð‘¡ð‘¡
ð‘ (ð‘’ð‘– ð‘— ) = Sigmoid(ð’‰
Â· ð’‰ð‘— ).
5 SYSTEM DEPLOYMENT
Next, we describe a practical distributed GNN training framework
that addresses the challenge of scalability and practicality in train-
ing GNNs for deployment. Traditional GNN training methodologies
often hit bottlenecks such as memory limitations and computational
inefficiencies, particularly when the task involves large-scale graphs
with complex relations and high-dimensional feature spaces. While
CPU-based training solutions exist, they prove to be prohibitively
slow for graphs of this scale and complexity (taking days to train).
On the other hand, training these large graphs solely on GPUs is
also infeasible due to limitations in GPU DRAM. Our framework
ameliorates these challenges by training on a set of sub-graphs
with preserved semantic information and distributing the computa-
tional workload across multiple GPUs in a cluster. This distributed
approach allows for parallel processing and significantly reduces
per-GPU memory consumption, thereby accelerating the training
process. It does so without sacrificing the modelâ€™s ability to learn
entity embeddings using semantic information, effectively bridging
the gap between computational efficiency and model performance.
5.1 Training Challenges
Deploying the GNN training workflow on a multi-GPU system
presents a formidable challenge, primarily stemming from the strin-
gent limitations of GPU device memory and inter-GPU communica-
tion. Our input graph boasts a substantial number of entity nodes,
each accompanied by high-dimensional node features. Moreover,
Equation 3 highlights another storage-intensive requirement, as
it necessitates the retention of computed attention coefficients for
every pair of nodes connected by an edge. In our graph, these edges
represent user co-play history, resulting in a significant volume of
entity-entity connections. To put this into perspective, even on a
powerful NVIDIA A100 Tensor Core GPU with 40GB of memory,
we can only feasibly train a semantic knowledge graph of a certain
size. Yet, our target graph encompasses 7Ã— more entity nodes and
7Ã— more edges than the largest graph we can train on a single A100.
Furthermore, our knowledge graph is poised for continual expan-
sion, demanding a training framework that remains future-proof
in the face of these escalating memory demands.
Distributed training of our knowledge graph over multiple GPUs
presents a unique set of challenges that are currently unaddressed
by existing off-the-shelf solutions. As discussed in Sec. 3.3, previous
multi-GPU GNN training techniques can be categorized into two
InputSemanticGraphe!"e#$e%$e&"eâ€™$e($e)"e*"ðºð‘ð‘TitleNodesFeatureInputGNNEmbeddingOutputSimilarityLinkPredictionLossð’¥+KGModelð‘“(Â·)ð’¥,KGCompletionLossConceptNodesFeatureVectors(a)PretrainingStage(b)TrainingStageSynergistic Signals: Exploiting Co-Engagement and Semantic Links via Graph Neural Networks
WWWâ€™24, May 2024, Singapore
approaches: (i) the full-graph training approach and (ii) the graph
sampling approach. However, neither of these approaches can be
well applied to our use case as we discuss in the following.
While full-graph training preserves cross-GPU edge connections
through inter-GPU communication during training, it also poses
limitations. Specifically, the number of GPUs required scales with
the graph size, making it a less flexible option for our dynamic
workloads at Netflix. In our setting, we aim for a system that can
train on an arbitrary number of GPUs, regardless of whether there
are, for example, at least 16 GPUs available in the system.
The graph sampling approach hosts the graph in CPU memory
and samples random nodes to create subgraphs, which are then
loaded into the GPU for training. However, this approach is ill-
suited for our use-case due to several reasons: First, our semantic
knowledge graph is highly heterogeneous, with each node type
having different feature sizes and degrees. Existing solutions either
focus on homogeneous graphs or convert heterogeneous graphs
into homogeneous ones [44]. Secondly, the graph sampling and
streaming from the CPU into the GPUs can become a performance
bottleneck during training. Finally, Our application features high-
degree nodes, such as one semantic node connected to tens of
thousands of entity nodes. The sampling strategy would thus either
capture an excessively large subgraph or require us to discard many
neighbors to fit into the GPU memory. Given these challenges, there
is a clear need for a more adaptable and scalable multi-GPU GNN
training solution for heterogeneous graphs.
5.2 HASP Multi-GPU Training Framework
We propose a novel and practical framework, termed Heterogeneity-
Aware and Semantic-Preserving (HASP), for generating subgraphs
from a comprehensive semantic knowledge graph. Unlike tradi-
tional methods such as graph sampling, where nodes are randomly
selected, or graph partitioning, which divides the entire graph into
several subgraphs, HASP is designed to capture the graph hetero-
geneity and preserve the semantics within the generated subgraphs.
The advantages of the HASP approach are manifold. Firstly, each
HASP-generated subgraph is optimized to fit within the memory
constraints of modern GPUs, ensuring efficient training without
memory overflow issues. Secondly, it offers both data parallelism
and flexibility: enabling simultaneous GNN training on these sub-
graphs across multiple GPUs, while also accommodating scenarios
where a single GPU can sequentially process the HASP subgraphs
using a dataloader. Next, we explain the design details of the HASP
generation framework, illustrated in Fig. 4.
Entity node partitioning. In the HASP framework, the entity
nodes within each subgraph are distinct, ensuring no overlap across
subgraphs. To achieve this, a specific subset of entity nodes is allo-
cated to each HASP subgraph. Notably, relationships between entity
nodes are represented by entity-entity links (EELs), indicative of
relationships like user co-play. A challenge arises when dividing
entity nodes: if two nodes connected by an EEL are apportioned to
separate HASP subgraphs, the EEL is effectively bypassed during
GNN training. This diminishes the utility of such relationships in
the learning process. Furthermore, to avoid computational dispari-
ties and ensure efficient distributed training, it is imperative that
each HASP subgraph contains a roughly equal count of entity nodes.
Uneven distribution might lead to GPU workload imbalances and
resource underutilization. Therefore, our entity node partitioning
strategy strives to achieve two primary objectives: (i) minimizing
the elimination of EELs across HASP subgraphs and (ii) ensuring
an equitable distribution of entity nodes across subgraphs.
Our partitioning methodology begins by categorizing entity
nodes of the original graph into two distinct groups: nodes that
possess EELs and nodes that lack any EELs, as illustrated in Fig. 4
(a). For the former group, we engage in a minimum cut graph par-
titioning process on the EEL subgraph, which is composed of all
EELs and their interconnected nodes. The METIS library[18] facili-
tates this operation, delivering ð‘ approximately uniform partitions
of the EEL subgraph with minimized EEL eliminations. Here, ð‘
stands as a representation of our predetermined number of target
partitions, generally set to be a multiple of available GPUs and opti-
mized in line with the GPU memory capacity to ensure each HASP
subgraph can comfortably reside within GPU memory. Following
this partitioning, we then integrate nodes without EELs into these
ð‘ partitions. Allocation begins from the most sparsely populated
partition, ensuring a balanced distribution of entity nodes across all
partitions, thus equalizing computational load. This method allows
us to achieve the dual goals of minimal EEL elimination and equi-
tably distributed entity nodes. This entity node partition process is
demonstrated in Fig. 4 (b). We deliberately avoided partitioning the
entire entity node subgraph, including both the EEL and non-EEL
nodes, in one go to prevent skewed EEL distributions, where some
partitions could be densely populated with nodes interconnected by
EELs while others might be bereft of them, potentially undermining
the generality of our resulting HASP subgraphs.
Semantic node duplication. In our HASP generation process,
each subgraph encompasses an exclusive set of entity nodes. Yet, to
ensure the preservation of all semantic information, every subgraph
1221435321Title (no EEL)Title (EEL)Semantic nodesSemantic edgesEEL(a) Original Graph(b) Title node partition12435Node partition on title nodes with EELRandomly assign non-EEL nodes to each partition12435132(c) Semantic Node Duplication435212121312Subgraph 1Subgraph 2WWWâ€™24, May 2024, Singapore
Zijie Huang, Baolin Li, Hafez Asgharzadeh, Anne Cocos, Lingyi Liu, Evan Cox, Colby Wise, and Sudarshan Lamkhede
Table 1: Evaluation results of baselines and our proposed method SemanticGNN over two ground-truth entity similarities:
HC-Sim (human-curated semantic sims) and Co-Sim (observed co-engagements). We report the relative improvement ratio
compared with Netflix-baseline.
MAP@10 MAP@50 MAP@100 MAP@10 MAP@50 MAP@100
Netflix-baseline
SemanticGNN_no_KG
SemanticGNN
Baseline Methods
Proposed Method
retains a full set of semantic nodes, effectively duplicating them
across all subgraphs. This design choice, illustrated in Fig. 4 (c), is
motivated by the vast difference in quantity between entity and
semantic nodes. To elucidate, consider the graphâ€™s representation
of genres: there might only be 20 distinct genre types, each rep-
resented by a single semantic node. In contrast, each genre could
be associated with tens of thousands of Netflix shows (the en-
tity nodes), highlighting the stark disparity in node counts. This
approach ensures all the semantic information can be used in the
message passing when training the GNN on each individual sub-
User-defined node sampling. In some scenarios, the distinction
between entity nodes and semantic nodes may blur. For instance,
when integrating external show entities into the graph to enrich
semantic information, these entities donâ€™t neatly fit into the tradi-
tional semantic node category. Given their potentially vast quanti-
ties, duplicating these external entities, akin to the way we handle
traditional semantic nodes, becomes impractical. To address this,
our HASP framework introduces a user-defined node sampling
feature. This allows users to specify and randomly sample a set
number of nodes from particular node types. This inclusion offers
context within the generated subgraphs while ensuring we remain
within memory constraints.
In summary, the HASP framework adeptly generates subgraphs
from a semantic knowledge graph. These subgraphs can be subse-
quently trained on an arbitrary number of GPUs or even sequen-
tially on a single GPU. By duplicating semantic nodes across all
subgraphs, HASP ensures the consistent preservation of semantics.
Title nodes, on the other hand, are strategically partitioned across
subgraphs. This ensures their distinctiveness while taking into ac-
count their intricate interconnections, represented as EELs, and
simultaneously achieving a balanced distribution. For complex sce-
narios like integrating external show entities, HASP provides a user-
defined node sampling, allowing customized node inclusion while
staying within memory limits. Users can leverage the HASP frame-
work for efficient GNN training as we have observed an average
improvement of over 50Ã— when trained on 4 A100 GPUs compared
to a naive full-graph training on a large-memory CPU machine. For
inference, given its one-time effort nature, node embeddings can be
generated using a large-memory CPU machine, ensuring all edges
in the knowledge graph are holistically preserved.
6 EXPERIMENTS
6.1 Experiment Setup
Datasets. We constructed our semantic knowledge graph by collect-
ing entities and their associated metadata from Netflixâ€™s catalog.
To empirically evaluate our model performance, we subsample a
portion of it and in total, we have over 125k nodes, where nearly
99% are entities and 1% are semantic concepts. Semantic concepts
describe basic information associated with each entity such as genre
and in total we have roughly 10 different categories of such seman-
tic concepts. For edges, we have both entity-entity edges (EEL)
with limited amounts and abundant entity-semantic edges. The
total number of edges is around 10Ã— compared to the total number
of nodes, where around 90% of them are entity-semantic edges.
When developing the model at Netflix, it handles graph size to
around 1 million entity nodes following a similar structure. We
only report the results on the sampled KG in this paper. In prac-
tice, the deployed model over the full KG shows great performance
gain in varying downstream recommendation tasks, especially for
new/unpopular titles.
6.2 Implementation Details.
Training Details. Our codebase is written in Python 3.7.12, we
use Pytorch 1.12.0 as the training framework and use the Pytorch
Geometric 2.2.0 library to construct the graph. We perform GNN
training on our Ray [27]-based machine learning training platform
using NVIDIA A100 Tensor Core GPUs, each with 40GB GPU mem-
ory. The CUDA version is 11.4.
Evaluation Protocol. We evaluate the quality of the learned entity
embeddings through two offline evaluation benchmarks, which
cover two flavors of entity-entity similarity measurements.
â€¢ Co-engagement Similarities (Co-Sim): Observed pairs of en-
tities in which both entities were engaged by the same user.
â€¢ Human-curated Similarities (HC-Sim): Dataset of similar
entity pairs, annotated by human experts.
Synergistic Signals: Exploiting Co-Engagement and Semantic Links via Graph Neural Networks
WWWâ€™24, May 2024, Singapore
Baselines. We compared against GCN [20], GAT (heto version 1) [33],
GraphSAGE (heto version) [8], and Netflix-baseline model. Net-
flix-baseline model adopts GAT (heto version) + embedding simi-
larity loss for training. The embedding similarity loss is designed as
cosine similarity measurements between the learned embeddings to
other pre-trained embeddings that also reflect title similarities from
other data sources (with limited coverage of titles in the constructed
KG), with negative sampling. It is in general more time-expensive
compared with the link prediction loss in SemanticGNN.
6.3 Main Results
We first examine whether our SemanticGNN has superior perfor-
mance compared with baselines as well as the design rationality
of each component of SemanticGNN. As illustrated in table 1, our
proposed SemanticGNN in general is able to outperform baselines
across evaluation metrics. Our model variant without KG pretrain-
ing has degraded performance compared with SemanticGNN, show-
ing the effectiveness of designing the KG pre-training stage.
