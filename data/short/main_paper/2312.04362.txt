Introduction
In the realm of Question Answering (QA) sys-
tems, traditional approaches have largely focused
on single-question scenarios, overlooking the dy-
namic nature of human information-seeking di-
alogs. However, to create more human-like and
interactive QA systems, understanding context-
dependent and evolving conversations is essen-
tial. To this end conversational question answer-
ing datasets have been introduced (Reddy et al.,
2019; Choi et al., 2018; Campos et al., 2020).
Here, we present PCoQA, an innovative and exten-
sive Persian Conversational Question Answering
dataset tailored explicitly for Question Answering
1https://github.com/HamedHematian/PCoQA
in Context, drawing inspiration from two influen-
tial predecessors, CoQA (Reddy et al., 2019) and
QuAC (Choi et al., 2018). Our dataset contains 870
dialogs, 9,026 question-answer pairs, and corre-
sponding documents, retrieved from the Wikipedia.
We take initiatives from both prominent CoQA and
QuAC datasets to build our dataset. To this end,
like CoQA, both questioner and responder have ac-
cess to the document in order to control the rate of
unanswerable questions. Since questioner’s acces-
sibility to documents increases the odds of string
matching and paraphrasing questions (Choi et al.,
2018), two further measures are taken to diminish
the phenomena. First, the questioner is informed to
ask questions that do not contain lexical matching,
and second, in the post-processing stage, questions
that contain a high level of lexical overlap with the
sentence containing the answer, are paraphrased to
ensure the quality of the dataset.
Our dataset incorporates various linguistic phe-
nomena related to conversations, including co-
references to previous dialog turns, anaphora, and
It introduces new challenges due to a
higher presence of non-factual questions, result-
ing in longer answers. This characteristic is fur-
ther compounded by the inclusion of abstract
topics (since we don’t value pages containing
high number of entities over other pages) in our
dataset, where documents often lack entities or
noun phrases, and answers tend to be explanatory
and lengthy. Finally, we provide various bench-
marks for the dataset, including baseline meth-
ods. Given that our dataset is approximately ×10
smaller than larger datasets like CoQA and QuAC,
and data scarcity poses a challenge, we also explore
the potential of enhancing model performance by
pre-training it on other question-answering datasets.
Our experiments exhibit the effectiveness of pre-
training on boosting the performances.
The rest of the paper is structured as follows. We
first describe the previous datasets and methods in
Section 2. Subsequently, we provide the details of
building the dataset in Section 3, comprising doc-
ument selection, data annotation, post-processing,
dataset validation, dataset analysis, and splitting.
Lastly, in Section 4, our tested models, experiments,
and results are reported.
2 Related Works
Multiple datasets have been introduced for the
task of question answering (Rajpurkar et al.,
2016; Trischler et al., 2017; Dunn et al., 2017;
Kwiatkowski et al., 2019). The field of conversa-
tional question answering aims to extend systems’
capabilities in answering questions within the con-
versational domain. Multiple datasets for this task
have been proposed in English (Choi et al., 2018;
Reddy et al., 2019; Campos et al., 2020). Unlike
QA domain, where multiple datasets in various lan-
guages are available (Carrino et al., 2019; Shao
et al., 2018; Chandra et al., 2021), attempts to build
datasets for CQA in non-English languages have
been limited (Otegi et al., 2020). Otegi et al. (2020)
constructed a CQA dataset in the Basque language.
Multiple methods have been proposed to effectively
model the history in CQA. Qu et al. (2019a) pro-
poses marking history answers in the embedding
layer, and Qu et al. (2019b) extends this work by
considering the order of histories. Another line
of research utilizes question rewriting (Vakulenko
et al., 2021) to address the problem. Kim et al.
(2021) employs consistency training to mitigate the
error propagation problem in rewritten questions,
while Chen et al. (2022) uses reinforcement learn-
ing to rewrite questions based on feedback from a
question-answering module. Despite these signif-
icant efforts, a notable issue in most of the men-
tioned research is the use of ground truth answers as
part of the modeling process. Siblini et al. (2021)
re-implements the works of Qu et al. (2019a,b)
without utilizing the gold answers of history and re-
ports significantly lower performance. Otegi et al.
(2020) adopts pre-training on other resources to
alleviate the impact of low-resource data.
3 Dataset Construction
This section describes the process of constructing
PCoQA dataset. A dialog sample of the dataset
is depicted in Figure A1 whose title is Pride &
3.1 Document Collection
The documents in our dataset are based on
Wikipedia articles. In line with previous question-
answering datasets in Persian (Darvishi et al., 2023;
Ayoubi, 2021), we have chosen Wikipedia as the
primary source for obtaining documents. To build
our documents, we have taken a different approach
compared to CoQA, which selects the initial por-
tion of each article as the final document (Reddy
et al., 2019). Wikipedia articles typically begin
with an abstract that provides general informa-
tion about the article’s topic, and subsequent sec-
tions delve into specific details. While the abstract
is essential for constructing final documents, the
finer-grained information in the subsequent sec-
tions should not be overlooked. To address this
concern and ensure diversity among documents
with consistent contexts, we have devised a unique
process for building our final documents. Initially,
we select two bounds for the minimum and maxi-
mum document lengths, denoted as Dm and DM
respectively. Dm is set to ensure that all documents
contain a minimum context necessary for a mean-
ingful dialog, while DM prevents excessively long
documents that can challenge current network mod-
eling capabilities, as transformers consist our main
models and they receive a limited length as input.
In practice we set Dm = 100 and DM = 1000. In
our approach, we differentiate between the abstract
and other sections, which we represent as A and
Si, respectively, with i being the section number.
The lengths of A and Si are denoted as LA and
LSi, respectively. To ensure consistency in the con-
text of the final documents, we appoint a human
annotator as the document provider. The role of the
document provider is crucial in curating documents
that align with the dataset’s requirements and main-
tain contextual coherence. First, a Wikipedia page
is chosen on random. Unlike Reddy et al. (2019),
we don’t consider any pre-condition, like a good
number of entities, for the selected pages. This
is because we want to maximize diversity. For in-
stance, it’s obvious that pages regarding abstract
phenomena contains a few entities whereas pages
regarding individuals and geographical locations
contain significant amount of entities. Next, It is
decided to whether select the document from the A
or a random Si-s:
• If A is selected as the beginning of the doc-
ument: If LA + (cid:80)
i LSi ≤ Dm, meaning
that the length of page is below the minimum
constraint, the document is discarded and if
A ≥ DM , the A is tailored to ensure the max-
imum length constraint. If A is chosen and
Dm ≤ LA ≤ DM , A is selected as the docu-
ment. To encourage diversity, the document
provider is allowed to append Si-s to the A
in order to elongate the document such that
the maximum length constraint is preserved;
these Si-s are selected in a way so that they
are semantically consistent with each other
• If Sj is selected as the beginning of the docu-
ment: the process follows a similar pattern
as previously described. However, in this
case, the document begins with Sj and is sub-
sequently extended with potentially semanti-
cally consistent sections, as determined by the
document provider.
To illustrate the process, an example involving
the Wikipedia page for "Canada" is shown in Fig-
ure A2. We begin by selecting S11, which corre-
sponds to the "Education System" section of the
page. Since the length of this section, LS11, is
within the bounds defined by Dm, the document
provider proceeds to choose the next two sections,
namely "Economy" and "Culture" These sections
are semantically consistent with the subject of "Ed-
ucation System". The final document is then com-
posed by concatenating these three selected sec-
It is important to emphasize the pivotal role of
the document provider in this process. The docu-
ment provider must carefully oversee the content
of each Si to ensure consistency. For instance, if
Sj is selected as the beginning of the document
and it contains a co-reference to a previous sec-
tion or some of its content is vague due to lack of
previous context, Sj should be omitted from the
selection, and the process should proceed with the
next suitable section.
3.2 Dataset Annotation
To establish dialogs, each document is assigned to
to a questioner and a responder, both of whom hav-
ing access to the title and text of the document. At
the turn of k, questioner asks a question of qk and
the responder returns ak, a span of the document
as the answer. The dialog is continued until the
questioner stops the conversation. The questioners
are informed that they should start the conversation
with general information and continue it to specific
subjects, to match the same process of human infor-
mation seeking in real world. To be specific, they’re
strictly told that they should not ask about specific
concepts regrading a topic unless they’re informed
about that concept in previous dialog turns. Addi-
tionally, questioners are informed to change their
questions if their questions exhibit a substantial
overlap with the potential answer.
3.3 Post-Processing
While our dataset is designed to provide question-
ers with access to documents, it is possible that
string-matching questions may arise (Choi et al.,
2018), despite our efforts to guide questioners to
avoid such issues. Previous studies have indicated
that questions exhibiting high similarity to the sen-
tence containing the answer have a greater likeli-
hood of being answered correctly (Sugawara et al.,
2018). To ensure the dataset’s quality, we have
identified these questions and had them rewritten to
reduce lexical overlap between the rewritten ques-
tion and the sentence that contains the correspond-
ing answer. Each question that shares at least one
similar word with the answer-containing sentence
is subjected to this rewriting process. A question is
rewritten in one of three ways:
• Words were removed due to ellipsis
• Words were replaced by their synonyms
• Words were replaced by their co-references
| overlap |
We quantified the similarity using the formula
| question words | where overlap is the
similarity =
set of shared words between the question and the
sentence containing the answer. Before the rewrit-
ing process, the similarity was measured at 14.2,
and after rewriting, the similarity was reduced to
3.4 Dataset Validation
Following previous research Choi et al. (2018);
Rajpurkar et al. (2016), multiple annotations are
provided for each question in Dev/Test set. This is
due to the fact that each question can have multiple
answers; Therefore, it is indispensable to obtain
accurate and unbiased scores for evaluation. These
annotations are tagged by annotators other than re-
sponders. In line with previous research (Choi et al.,
2018; Rajpurkar et al., 2016), multiple annotations
are assigned to each question in the Dev/Test set.
This practice is essential because a single ques-
tion may have multiple valid answers. It ensures
the acquisition of accurate and unbiased scores for
evaluation purposes. Notably, these annotations are
provided by annotators who are distinct from the
responders. We report the scores of the responders’
answers in Table 2.
3.5 Dataset Analysis
Key statistics for the PCoQA dataset are presented
in Table 1, along with a comparison to similar
datasets such as CoQA and QuAC. In the table,
the expression X/Y represents the average quan-
tity of X per unit of Y . Notably, PCoQA answers
are longer, reflecting the prevalence of non-factual
questions in our dataset. Additionally, our docu-
ments are longer than those in CoQA and QuAC,
necessitating the use of transformers with larger
input sizes, as standard transformers have limited
input capacities. Furthermore, our dataset features
a higher number of questions per dialog compared
to QuAC, underscoring the importance of effective
history representation.
tokens, words / document
tokens, words / question
tokens, words / answer
questions / dialog
unanswerable rate
Table 1: Statistics of the PCoQA Dataset
3.6 Splitting
The dataset is randomly divided into Train, Dev,
and Test sets with the ratio of 70/15/15.
4 Experiments
In this section, we describe the adopted evaluation
metrics, methods, and the results of applying these
methods to the PCoQA dataset.
4.1 Evaluation Metrics
Exact Matching (EM) is the ratio of questions for
which the model has answered correctly. Follow-
ing Choi et al. (2018), three additional metrics of
F1, HEQ-Q, and HEQ-D are considered in this pa-
per. F1 indicates the degree of overlap between the
predicted answer and the gold answer, and HEQ-Q
and HEQ-D are the ratio of questions and dialogs
for which the model outperforms the human re-
spectively (Choi et al., 2018). While HEQ-D is a
stringent metric that requires the model to outper-
form humans for every question within a dialog to
earn a point, it may be overly strict in some cases.
While HEQ-D is a stringent metric that requires
the model to outperform humans for every question
within a dialog to earn a point, it may be overly
strict in some cases. To address this, we introduce
another metric, called HEQ-M. HEQ-M quantifies
the number of dialogs for which the model achieves
a better overall performance compared to human
performance on average. Additionally, we analyze
the F1 score for each dialog turn to gain insights
into the model’s performance at different turns of
the conversation.
Importance of History
In this section, we explore the impact of history
on model performance. Figure 1 illustrates the per-
formance variation of the model concerning the
inclusion of a different number of history ques-
tions. Notably, excluding the history questions
results in a sharp drop in the model’s performance.
The best performance is achieved when using 2
history questions. However, including more than
2 history questions gradually leads to a decline in
performance. This suggests that histories with dis-
tances over 2 are irrelevant and don’t introduce new
information on average, and their inclusion induces
some noise in the model. Thus, we perform the rest
of our experiments with 2 history turns.
These base models serve as the foundation for our
methodology. In our implementation, each model
takes the concatenated question and previous his-
tory questions as the first input and the document
as the second input, which is then fed into the trans-
Baseline Methods ParsBERT and XLM-Roberta
are fine-tuned on PCoQA, constituting our baseline
Pre-Trained Methods ParSQuAD + Pars-
BERT denotes
pre-training ParsBERT on
ParSQuAD (Abadani et al., 2021), a translated
dataset of SQUAD (Rajpurkar et al., 2018) to
Farsi, and then fine-tuning it on PCoQA using
history concatenation. Similarly, ParSQuAD +
XLM-Roberta denotes pre-training XLM-Roberta
on ParSQuAD and then fine-tuning it on PCoQA
using history concatenation. Lastly, QuAC + XLM-
Roberta represents pre-training XLM-Roberta on
QuAC and subsequently fine-tuning it on PCoQA
using history concatenation.
