Introduction

The diffusion model is one of the generative models based on thermal non-equilibrium physics [1].
Compared with other generative models such as Generative Adversarial Network (GAN) [2] and
Variational Autoencoder (VAE) [3], the diffusion model generates more high-quality data samples, i.e.,
the diffusion models can derive more discriminative parametric distribution for a given dataset. Based
on their superior capability in sampling data, the diffusion models are establishing remarkable success
in various domains, such as image and audio generation [4–6] and natural language processing [7].
Those achievements triggered various unsupervised anomaly detection (AD) studies that apply
diffusion models to detect data anomalies [8–11].

Those unsupervised AD studies using diffusion models [8–11] have justified that the diffusion models
can be used for unsupervised AD and can achieve competitive performance compared with other
generative model-based AD methods. The high-quality data sampling capability of the diffusion
models can be a strong advantage in driving unsupervised AD methods. However, applying diffusion
models is still challenging due to high computational costs caused by iterative sampling or the
unstable quality of generated data depending on the number of samples [12].

∗Email to the corresponding author: jinhong@inje.ac.kr

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

 
 
 
 
 
 
This paper introduces the Adversarial Denoising Diffusion Model (ADDM), an end-to-end unsuper-
vised AD method using a diffusion model. The proposed ADDM minimises the noise prediction
error and explicitly minimises adversarial loss about the denoised sample. Compared to other diffu-
sion model-based methods, ADDM can dramatically reduce the number of data sampling (reverse
process) steps by providing an explicit process to improve the quality of denoised data. The ADDM
outperforms other diffusion model-based AD methods [8, 10] with 6.2% better performance in our
experiments using MRI images for brain tumour detection.

2 Preliminaries

The goal of diffusion models [1] is to find the parameterised data distribution pθ(x0) using a given
data x0 ∼ q(x0). To do this, when a data x0 is given, the training of diffusion models conducts a
forward process (a.k.a. diffusion process) q(xt|xt−1), which adds Gaussian noise to the data and
a reverse process (a.k.a. denoising process) pθ(xt−1|xt), which denoises the given noised data by
subtracting predicted noise.

The forward process q(xt|xt−1) is a task to add Gaussian noise to data at a certain time step t ≤ T .
The Gaussian noise is generated by a Gaussian probability N (·) with scheduled variance: β1, . . . , βT .
The entire forward process to generate completely noised sample xT is represented by

q(x1:T |x0) :=

T
(cid:89)

t=1

q(xt|xt−1),

q(xt|xt−1) := N (xt; (cid:112)1 − βtxt−1, βtI).

(1)

The reverse process pθ(xt−1|xt) can be considered as a denoising task. For each time step t, a
diffusion model predicts a noise and subtracts it from the noised data. This task is represented by
Markov Chain so that it can be represented by parametric conditional distribution, as follows:

pθ(x0:T ) := p(xT )

T
(cid:89)

t=1

pθ(xt−1|xt),

pθ(xt−1|xt) := N (xt−1; µθ(xt, t), Σθ(xt, t)).

(2)

The learning of the diffusion model is to find the suitable parametric distribution pθ(x0) representing
a given data. However, the negative log-likelihood of pθ(x0) is not nicely computable, so it is
alternatively optimised by variational lower-bound, which is represented by

Ldm := E [− log pθ(x0)] ≤ Eq

(cid:20)

− log

pθ(x0:T )
q(x1:T |x0)

(cid:21)

(cid:20)

= Eq

− log p(xT ) −

(cid:88)

t≥1

log

pθ(xt−1|xt)
q(xt|xt−1)

(cid:21)
.

(3)

In DDPM [13], it found that the forward and reverse processes are represented by reparameterisation
tricks; therefore, it can replaced by the minimisation of prediction error for a Gaussian noise and noise
prediction obtained by a neural network. As a result, Eq. (3) is more simplified to the l2-distance
between the Generated Gaussian noise ϵ ∼ N (0, I) and predicted noise ϵθ at a certain time step.
Using the notations, αt := 1 − βt and ¯αt := (cid:81)t

s=1 αs, the simplified loss is represented by

(cid:20)

Ex0,ϵ

β2
t
t αt(1 − ¯αt)

2σ2

√

(cid:13)
(cid:13)ϵ − ϵθ(

¯αtx0 +

√

2(cid:21)
1 − ¯αtϵ, t)(cid:13)
(cid:13)

.

(4)

3 Adversarial Denoising Diffusion Model

Unlike GAN [2] or VAE [3], which directly generate data from relatively low-dimensional Gaussian
noise and classify whether the samples are generated or given (in the case of GAN) or minimise
Euclidean distances of generated samples and a given sample (in the case of VAE), to generate data
from complete, from a given noise which having the same dimensionality with the data, diffusion
models repeatedly performed the reverse process, i.e., predicting the noise present in the given data
and remove it.

The diffusion model methodology, that is, generating data through an iterative procedure that gradually
restores data from the same dimensional noise, shows outstanding data sampling performance,

2

Figure 1: Illustration for Learning of Adversarial Denoising Diffusion Model (ADDM). LDDPM denotes
the loss function to minimise the l2-distance between the generated error from Gaussian distribution and the
predicted error by a model. LAdv indicates the adversarial loss between the model-based denoised sample xt−1
and virtually generated noised samples, for the certain time step t − 1.

but compared to GAN and VAE, the sampling process of the diffusion model is cost-efficient.
Additionally, the quality of generated data is varied depending on the number of sampling steps [12].
This issue is because diffusion model learning involves noise prediction, which is not closely related
to the semantic properties of data. When generating noise, x0 is used conditionally (See Eq. (2)),
but the model ultimately learns the minimum prediction error between Gaussian noise and the noise
present in the image (See Eq. (4)). In this study, adversarial learning is proposed to minimise changes
in the diffusion model’s structure and preserve the semantic characteristics of data.

Figure 1 shows the architectural details of the proposed ADDM. The objective function of the
adversarial denoising diffusion model (ADDM) is defined by the summation of the DDPM loss (Same
with Eq. (4)) and the additional adversarial loss using a balancing weight λ, represented by

LADDM := Et,x0,ϵ

(cid:104)(cid:13)
(cid:13)ϵ − ϵθ(

(cid:124)

√

¯αtx0 +
(cid:123)(cid:122)
LDDPM

√

1 − ¯αtϵ, t)(cid:13)
(cid:13)

2(cid:105)

(cid:125)

+ λEt,x0,ϵ[log D(q(xt−1))] + Et,x0,ϵ[log (1 − D(pθ(xt−1|xt))]
,
(cid:123)(cid:122)
(cid:125)
LAdv

(cid:124)

(5)

where D denotes the discriminator to apply the adversarial learning on ADDM. q(xt|xt−1) defines
the t-step noised data obtained by the forward process (Eq. (1)), and pθ(xt−1|xt) indicates the
denoised data produced by the reverse process (Eq. (2)).

LDDPM is a simplified version of Eq. (4). DDPM [13] supposes that the simplified loss function is
more beneficial to the quality of the sample and implementation efficiency; therefore, we keep this
loss format in our study. LAdv is the loss term for the adversarial learning about the denoised data.
LAdv tries to distinguish between the real noise data q(xt|xt−1) and the denoised data pθ(xt−1|xt).

In predicting a noise for the noise removal process for data generation through DDPM, the ADDM
model can learn semantic features of the data and information about the noise to be predicted. Through
this direct learning of semantic features of data, we expect to reduce the number of sampling steps
required to generate sufficient quality data. Besides, the adversarial loss helps to reduce blurriness,
which means it helps to learn high-frequency features to add more local details in denoising the data.
This property will help prove the accuracy of AD methods by reducing the false-positive results.
Through ablation studies, we will demonstrate that the proposed adversarial learning helps AD.

4 Experiment

4.1 Experimental settings

Dataset and Protocol: We utilise the two datasets. The first dataset is the Neurofeedback Skull-
Stripped (NFBS) repository [14]. The repository provides 125 MRI images captured from normal
people, so there are no anomalies in the MRI images. The second dataset contains 22 T1-weighted
MRI scans provided by the Centre for Clinical Brain Sciences from the University of Edinburgh [15].

3

.........!"!#!#$%!&'((!#$%|!#),(!#|!#$%)--(,#/0012GaussianNoise/345Method
AE (Spatial) [17]
VAE (Dense) [17]
f-AnoGAN [18]
Transformer [19]
Pinaya et al. [10] (T = 1000)
AnoDDPM [8] (T = 1000)
ADDM (T = 300)
ADDM (T = 500)
ADDM (T = 1000)

Dice
0.252
0.317
0.128
0.241
0.375
0.383
0.301
0.379
0.403

AUC
0.707
0.734
0.789
0.695
0.815
0.863
0.811
0.861
0.917

IoU
0.162
0.203
0.093
0.193
0.238
0.269
0.213
0.271
0.289

Precision Recall
0.279
0.313
0.080
0.120
0.452
0.468
0.369
0.491
0.508

0.258
0.297
0.362
0.275
0.367
0.373
0.293
0.361
0.392

Table 1: Quantitative performance comparison of unsupervised AD methods based on
generative models. T denotes the number of sampling steps of a diffusion model.

The second dataset provides MRI images containing brain tumours. To follow an experimental
protocol for the unsupervised AD, the training dataset has to be composed of normal samples only.
We train the ADDM using the first dataset and conduct AD experiments using the second dataset. We
refer to the experiment protocol of AnoDDPM [8].

Implementations: The resolution of images is resized to 256 × 256. Adam optimiser [16] is used
for the optimisation algorithm. The balancing weight λ is set by 0.05. The number of epochs and
the batch size are set by 3000 and 4, respectively. To demonstrate the effectiveness of adversarial
learning, we evaluate the AD performance with 300, 500, and 1000 sampling steps (T ). The learning
rate is initialised by 0.0001, and it is decayed by multiplying 0.999 for every 200 epoch.

4.2 Experimental results

Effectivenss of LAdv: We train the ADDM with 300, 500, and 1000 sampling steps. Table 1
contains the quantitative performances of the ADDM depending on T . The ADDM obtains the
best performance with 1000 sampling steps. It produces 0.403 Dice and 0.917 AUC. The overall
performance of the ADDM with 1000 sampling steps is better than the other two ADDMs trained with
300 and 500 sampling steps. However, the ADDMs trained with 300 and 500 sampling steps produce
competitive performance compared with other DDPM-based models [8, 10]. In particular, the ADDM
trained with 500 sampling steps outperforms the Pinaya et al. [10], which is structurally equivalent to
the DDPM with 1000 sampling steps. The experimental results justify that the adversarial learning
on the ADDM improves the robustness of the diffusion models with respect to the sampling step.

Comparison with SOTA methods: We compare the proposed method with various generative
model-based AD methods [8, 17–19]. Table 1 shows the quantitative results on the dataset. Listed
methods have been chosen for performance comparison: Autoencoder (AE), Variational AE, [17],
f-AnoGAN [18], Transformer [19], Pinaya et al. [10], and AnoDDPM [8]. In particular, Pinaya et
al. [10] and AnoDDPM [8] are built based on DDPM [13], so their baseline methods are similar
to the ADDM. Both approaches compile reconstruction-based AD methods for MRI images using
diffusion models. Interestingly, the architectural details of Pinaya et al. [10] are almost identical to
the DDPM [13]. AnoDDPM [8] is built based on the DDPM and uses a new noising approach called
Simplex Noise.

The quantitative results in Table 1 show that the ADDM outperforms other methods. The ADDM
(T =1000) produces 0.917 AUC, which is 6.2% higher performance than the second-ranked method
(AnoDDPM). Moreover, the proposed ADDM trained with 500 sampling steps also achieves com-
petitive performance with the AnoDDPM that requires 1000 sampling steps. This result shows that
the proposed ADDM is more cost-efficient than AnoDDPM. Overall, the experimental results show
that the proposed adversarial loss improves AD performances and outperforms existing SOTA AD
detection methods on MRI images.

5 