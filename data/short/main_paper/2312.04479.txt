INTRODUCTION
In this work, we aim to model pedestrian behaviour in
urban areas so as to create a risk map of the environment
that can then be used by other parts of the AV system to
navigate through such an area. We do this by modelling the
social forces around the car in the environment, identifying
the different possible goals for each pedestrian in the scene
and utilising the map information around each detected
pedestrian at each time step in their trajectory.
In summary, our contributions presented in this work are
as follows: (1) we propose a novel model architecture that
captures social and environmental interactions to accurately
model spatio-temporal pedestrian trajectories over a short
to medium term horizon (2) this architecture incorporates
a novel goal identification module that generates multiple
likely end points for each observed pedestrian partial trajec-
tory. (3) A generative model that beats the state of the art
on two drone datasets proving that the model can generalise
to similar environmental scenarios.
In our work, we take ideas from each of these systems such
as using Transformers to capture temporal trajectories, Graph
Networks to capture varied and heterogeneous interactions
and CVAEs to model the multi-modality of human behaviour.
The exact method in which these components are assembled
together in our architecture will be described in the next
section.
III. THEORETICAL APPROACH
A. Problem formulation
Our problem of multimodally predicting pedestrian trajec-
tories can be formally prescribed as follows. Starting with a
bird’s eye view (BEV) scene of an environment captured in
RGB, semantically segmented into different classes, denoted
by M contains a history of positions of each pedestrian in
this scene. For each scene, there exist a finite number of
goals that can be reached by a pedestrian denoted by G.
Each pedestrian also interacts with every other agent in the
scene, denoted by E.
Given this information, the model aims to predict the
distribution of pedestrian’s trajectory for H timesteps in the
future.
B. Proposed model
Our proposed model can be split into three main compo-
nents, in two phases as depicted in Fig. 1. In the Encoding
Phase, we encode the ego agent’s interactions with the
environment and other agents in the scene to create “Social
Embeddings�. These Embeddings are used along with other
data to gererate multi-modal trajectories in the Generative
Phase of the model. The three main components are the
Graph Attention Encoder (GAE), the Temporal Transfmer
Encoder-Decoder (TT) and the generative CVAE model for
the ultimate output of multi-modal trajectories. Each of these
components and their sub-components are described in detail
below.
Fig. 1: GSGFormer model overview. Consider various VRUs in a
structured urban environment, in BEV as shown. Our model uses
the understanding of the interplay of social forces between different
VRUs and the environment in which the motion is captured. A
map encoder captures the influence of the environment on the VRU
under consideration while the other encoders - for encoding the ego
motion of the VRU, identifying the different goals of this VRU and
those of its neighbours - are fed into a Graph Attention Network
Encoder to capture a “Social Embedding� within the environment.
Social Embeddings for each time step are then passed on to the
Generative Phase of the model, where different contexts from the
Transformer Decoder and the CVAE Encoder are concatenated
along the last dimension of the tensor and passed to the Residual
GMM generator. Finally, positions for a time interval h from current
time t are predicted by integrating these velocities over the time
interval dt.
3) Goal Encoder: This subcomponent is one of our novel
contributions in this paper. We propose a 1D CNN to estimate
the goal position. Initially, The state of the ego agent for
each history time step is embedded using the agent state
encoder. Then the embedding is stacked over the time di-
mension, resulting in a tensor of shape (n×dmodel). The 1D
convolutional neural network extracts the temporal feature
between the n channels(time steps). The map embedding is
concatenated to the output of the goal encoder and passed
to a linear layer. The output of the network is a vector of
size dmodel. The vector then serves as the input condition for
the CVAE-Residual-GMM module detailed further in III-B.6,
facilitating the generation of a range of potential goals.
4) Social Graph Attention Neural Network: To model the
social force acting on the ego agent, we introduce a hetero-
geneous graph representation. This representation comprises
four distinct nodes: the ego agent, neighboring agents, the
semantic map, and the goal populated by their respective
embeddings. Physically, we only consider the effect of these
embeddings unidirectionally on the ego agent at time step t.
Then a heterogeneous graph attention network[35] performs
both node-level and semantic-level attention aggregating the
social information to the ego agent node. The output is
the social force embedding in size of dmodel is embedded
from the aggregated ego agent node. This embedding is then
element-wise combined with the state embedding of the ego
agent, producing the social embedding at time step t.
5) Temporal Transformer Model: Once the social infor-
mation are encoded from the social graph of the ego agent
at each time step, it is necessary to build the dependencies
between graphs across different time steps. We employ a
Transformer framework [12] to model the temporal relation-
ship. The history sequence of social embedding are fed into
the Transformer encoder as the source sequence with a fixed
length n. Since the primary operation in Transformer is the
attention mechanism which is not sensitive to the relative or
the absolute position of the elements in the sequence, we add
the �positional encodings� with the same dimension to each
social embedding in the sequence. We compute positional
encodings through sine and cosine functions, as described
in [12], which allows the model to easily learn to attend by
relative positions. Meanwhile, it enables the model recognize
any missing observations, as each comes with its distinct
positional encoding.
6) CVAE-Residual-GMM Module: We propose a gener-
ative CVAE-Residual-GMM Module to realize the one-to-
many mapping for both goal and waypoints. The module is
composed of two main parts: a Conditional Variational Auto-
Encoder(CVAE) [36] and a Gaussian Mixture Model(GMM)
[37]. Let’s denote the input condition as X and its cor-
responding output (be it goal position or waypoint state)
as Y. The high dimensional
latent variable Z is drawn
from the prior distribution Pθ(Z|X), facilitated by a prior
the
network. Throughout
ground truth is accessible, Z is drawn from the posterior
distribution Qϕ(Z|X, Y) by the recognition network. Y is
finally generated from the distribution Pψ(Y|X, Z) through
the generation network.
the training phase, given that
Different from the vanilla CVAE, the generated Y is the
sum of two parts: the output ˆY from CVAE decoder decoding
the concatenated (X, Z) and the residual ∆Y, which adheres
to a GMM distribution. This distribution is parameterized by
an MLP, defining means, variances, and component weights
based on (X, Z). Notably, every waypoint within a single
trajectory utilizes a consistent Z for each sampling iteration.
7) Training Strategy: During training, the most recent
historical state, alongside the entire future state sequence, is
concurrently fed into the Transformer decoder. A subsequent
masking is applied to speed up the training and to ensure
predictions for time step t rely solely on preceding time
steps. The latent variable Z is drawn both from Pθ(Z|X)
and Qϕ(Z|X, Y).
IV. EXPERIMENTS AND RESULTS
A. Datasets
We train and evaluate our model on three stationary BEV
datasets - the intersection Drone (inD) Dataset [33], the
Stanford Drone Dataset (SDD) [32] and the ETH/UCY
Dataset [38], [39].
inD dataset contains 32 drone recordings of various VRU
trajectories of 5 different classes (pedestrian, bicycle, car,
truck and bus) in 4 different scenes at German intersections.
The raw data is samples in 25Hz. Training and test sets are
split based on previous work done [40] such that scenes 1
and 2 of the dataset are considered as training data, scene 3
to be the validation set and scene 4 as the test set.
SDD is a similar drone dataset, with 60 recordings at 8
unique scenes in the Stanford university campus, each scene
containing a mixture of 6 classes of VRUs (pedestrians,
skateboarders, bicyclists, cars, cars and buses). The raw data
is sampled at 30 Hz. The entire dataset is split in two specific
ways - one proposed in TrajNet [41] and another full split
as in DESIRE [8].
ETH/UCY is a classic dataset widely used for the evalu-
ation for pedestrian trajectory prediction which contain only
data of pedestrians with rich human-human interactions. The
raw data is extracted and annotated at 2.5Hz. We follow the
leave-one-out strategy as presented in previous works [4], [7]
for evaluation.
The first two datasets are more representative of scenarios
that AVs can face in urban environments, especially in shared
spaces than the classic ETH/UCY dataset. The background
RGB frames in these datasets act as the maps for our model.
In each dataset, we only concentrate on predicting trajecto-
ries of the pedestrian VRU class while utilizing the effects of
the other VRU classes on these pedestrians. Another rationale
for utilizing similar datasets is to investigate our model’s
robustness to different scenes and vantages.
B. Experimental Setup
For each of the datasets chosen and all their corresponding
scenes, we perform trajectory preprocessing as provided by
OpenTraj tools [42]. All VRU trajectories are sampled at
2.5Hz and velocities for these trajectories are calculated as
∆X|t+1
/∆t where X represents the sampled position at
t
time t and ∆t is the sampling interval. Each observed partial
trajectory lasts for 8 timesteps with a prediction horizon
h of 12 timesteps thus implying a medium term trajectory
prediction window of 4.8s after an observation window of
3.2s, consistent with the baselines (IV-C) chosen to compare
against our work.
For each trajectory, a distance of 15m (or equivalent) is
chosen as an effective threshold for neighbour interactions
while a crop patch of 10m (or equivalent) is chosen for
map interaction for social embeddings. Parentheses imply an
estimation from pixel space to Cartesian space where direct
measurements are unavailable in the dataset.
The model is set up to be trained on a single NVIDIA
RTX A6000 GPU with 48 GB VRAM.
C. Baselines and Metrics
a) Baselines: We compare our model’s effectiveness
with different baselines for each of the datasets described
in Section IV-A since each dataset has its own merits and
drawbacks. However, we explain some choices of baselines
as below:
Social GAN [7]: One of the earliest generative architec-
tures that focus on multi-modal trajectory prediction. They
apply a an LSTM Encoder-Decoder architecture connected
by a social pooling module which used to aggregate the
social interactions between actors in the scene.
ST-GAT [6]: ST-GAT is a classic work on human trajec-
tory prediction which uses graph attention network to model
human interactions. LSTM is applied to model the temporal
feature of the pedestrians.
Y-Net [11]: This multi-modal trajectory generation model
uses goals, waypoints and segmented maps to predict pedes-
trian motion. However, it does not explicitly model any VRU
Method
D. Implementation details
All input states - ego state, neighbour states, goal states -
are embedded in a space of dmodel = 128. We use a map
patch centered on the ego agent parsed through a pretrained
UNet architecture to extract semantic segments from the
patch and subsequently embed it in a space of 128. The
Graph Attention Network is composed of two input layers
of size 128 in mean aggregation mode with 4 attention heads
for each layer. The single layer Transformer encoder decoder
module has a feed forward dimension set to 256 with 4 heads
of multi-head attention. Each MLP in the CVAE encoder,
decoder and GMM consists of two hidden layers of size
512 → 256 with ReLU activations on each. The dimension of
the latent variable is 16 for the goal and 32 for the waypoint.
The number of mixture components is set as 4 for goal and
20 for waypoint.
The semantic segmentation network for the map encoder
is a single model trained on both the drone datasets, with
manual annotations in 6 classes where unavailable as pre-
sented in Sec. III-B.2.
The model is trained using the Adam optimizer with a
learning rate starting at 0.001 and decaying by a factor of
0.5 every 10 epochs for a total of 50 epochs with a batch
size of 512.
E. Quantitative Results
We organise our results by dataset, with the reported
minADE and minFDE for 20 samples each. Clearly, for all
datasets, lower scores are better for both metrics with the
best result shown in bold. Table II compares metrics for the
baselines with ours. As observed, our model beats the state
of the art in one metric while performing at the state of the
art in the other.
Table III shows the compared metrics for two different
splits for SDD - overlapping trajectories for the entire dataset
at the top with the Trajnet split at the bottom. It can be
seen that our model performs better than the state of the art
when trained on the entire dataset compared to the Trajnet
split. We conjecture that this is due to the Trajnet split not
containing sufficient social interaction information among its
trajectories.
Table I compares metrics of the baselines with our ap-
proach, however as can be seen, our model does not perform
as well on this dataset as the others. This is because of the
relative simplicity of ETH/UCY scenes and the inability of
the Map Encoder module of our approach to reach its full
potential, proving our model performs best on large scale
datasets with environmental diversity.
F. Qualitative Results
Fig. 3 presents the qualitative results of our model, de-
picted here for the inD and SDD datasets. In all presented
cases, we see that our model predicts trajectories accurately
over time - even in complicated situations like fig. 3e where
a turn in the trajectory is captured and predicted as one of the
samples. In contrast, fig. 3f shows a case of another turning
trajectory with a less confident goal sampling, leading to a
chaotic trajectory and thus, risk map, prediction.
(a) Best of 20 result (inD)
(e) Turning trajectory (inD)
(b) Risk map (inD)
(c) Best of 20 result (SDD)
(d) Risk map (SDD)
(f) Chaotic prediction (inD)
A second conclusion we would like to draw from this work
is how small the changes in the state of the art has become.
We would like to draw attention to Table II where the average
prediction is smaller than the average stride of a human and
while we have beaten the state of the art here, there is a
plateauing of scores with changes only in the second decimal.
It is our conclusion that we as a community have reached
the limits of resolution for prediction in certain situations.
ACKNOWLEDGMENT
These
researches have been conducted within the
AI4CCAM project, financed by the EU Project AI4CCAM
with grant number 101076911.
G. Ablation Study
We performed an ablation study, module-by-module, of
our proposed model and verified on the inD dataset for its
effectiveness in prediction tasks with results shown in Table
IV. Beginning by removing all component modules except
a single CVAE layer, trained by the l2 loss, produces an
FDE and ADE as seen. Continuing, we add modules one
after another and compare against the minimum baseline
produced by the single CVAE layer. As seen,we notice a
consistent decline of ADE and FDE with the addition of
each component module thereby showcasing the importance
of each of the four enumerated components in our module
as well as in pedestrian prediction tasks in general.