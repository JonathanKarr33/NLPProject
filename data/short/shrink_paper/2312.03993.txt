INTRODUCTION
In the realm of artistic expression and cultural preservation,
the desire to revitalize a timeless treasure such as Calvin and
Hobbes comics has fueled our motivation to seamlessly blend
nostalgia with modern techniques.
This endeavor not only
aims to breathe new life into the beloved comic strips but also
stands as a testament to the ongoing exploration of advanced
machine learning approaches.
One such avenue of exploration is the utilization of stable
diffusion fine-tuning, a cutting-edge process that holds the
promise of achieving a delicate balance between preserving
the essence of the original artwork and infusing it with con-
temporary flair.
By undertaking this project, we endeavor to
unravel the intricacies of stable diffusion fine-tuning, delving
into its nuances to better comprehend its potential applica-
tions in the domain of artistic style transfer.
Amidst a myriad of deep learning based style transfer ap-
proaches, we chose stable diffusion as our preferred method
due to its unique ability to synthesize artistic styles.
Right Results of our fine-tuned stable diffusion
model performing style transfer on these images into the Calvin and
Hobbes comics style.
our attempt at learning how to perform fine-tuning of stable
diffusion models and revitalize Calvin and Hobbes comics
along the way.
Pages from the beginning and the end with the foreward and
publishing details were discarded since we wanted to focus
on images with the original comic strips.
Black and White Pages
The original Calvin and Hobbes comic strips were syndi-
cated and published in newspapers worldwide every day for
10 years from 1985 to 1995.
The comic strips published from
Monday to Saturday were in black and white and consisted of
four panels in a single row.
The black and white pages
are nearly binary images as the pixels are clustered in two
modes around 3 and 250 in grayscale values.
The black and white images
have a max absolute channel-wise difference of 10 while this
does not hold true for color images as the red, green and blue
channels encode different intensities.
Another motivation for this choise was the fact
that the weekday black and white comic strips tend to follow
a consistent and reliable four panel structure that allows us to
easily extract panels of the same size.
The weekend colored
comic strips tend to be more artistic and do not follow the
same sizes for each panel and thus pose a lot of variation in
the dataset.
Due to variation in num-
ber of pages between the volumes, we ended up with 11,033
black and white panels of the same size.
We explored the idea of using the multi-modal vision-
language framework BLIP2 [1], capable of answering ques-
tions based on any input image, to generate captions for our
input panels.
However, the generated captions were unsatis-
Another approach we tried was to use the multi-modal
GPT-4 [2] that can accept image and text inputs to generate
Fig.
Although this is not an ideal choice, we expected the
diffusion model to generalize sufficiently owing to the large
dataset size.
METHODOLOGY
As described in Section 1, we use a denoising diffusion
probabilistic model [3] to perform a style transfer operation.
To fine-tune this large model effi-
ciently, we use a training technique developed by Microsoft
Research called LoRA [5] which stands for Low Rank Adap-
tation.
More about the network and the training procedure in
the following two subsections.
Network
The Stable Diffusion Model v1.5 consists of many pieces.
Latent diffusion model [4] runs the diffusion process in the
latent space instead of pixel space, making training cost lower
and inference speed faster.
It is motivated by the observation
that most bits of an image contribute to perceptual details and
the semantic and conceptual composition still remains after
aggressive compression.
The de-
sign is equivalent to fuse representation of different modality
into the model with cross-attention mechanism.
Thus, it can efficiently learn
visual concepts, in the form of text, via natural language su-
pervision and perform zero-shot classification (Figure 4)
In the pre-training stage, the image and text encoders are
trained to predict which images are paired with which texts
in a dataset of 400M image-caption pairs.
CLIP is trained to
maximize the cosine similarity of the image and text embed-
dings of image-caption pairs via a multi-modal embedding
We use a training method called Low Rank Adaptation
(LoRA) [5] to fine-tune the diffusion model.
In this technique, weight matrices in existing
layers, typically attention layers, are fine-tuned to a specific
dataset by adding update matrices.
During the fine-tuning process, the original weights are
frozen and the weights in the update matrices are learnt.
The
LoRA framework is flexible as we have control over the rank
of the matrices in the decomposition of the update matrix.
In our experiments,
we set k = 4 and we use one update matrix for each attention
layer in the UNet.
First, this
approach uses far less memory than traditional fine-tuning ap-
proaches since we only need to compute the gradients for the
relatively smaller weight matrices.
For our custom Calvin and Hobbes dataset, we fine tune
the model on 11,000 input images paired with the synthetic
text caption CNH3000 as mentioned earlier.
At each step, a
random image from the dataset and a random denoising time
step for the DDPM are sampled.
Though the network does not train on all available input im-
ages at each time steps, it is able to generalize the denoising
process with a significantly smaller subset.
A little girl with a balloon walking down a street with
buildings in the background in the style of Calvin and
After fine tuning stable diffusion on Calvin and Hobbes
images, we generated some more images using the same
that we replaced Calvin and Hobbes with our keyword,
CNH3000.
For
image to image models, the denoising process starts with an
image with some noise added to it.
The noised im-
age with some prompts can allow us to style transfer an im-
For all these image generation, we used similar prompts as
4.1 with the keyword, CNH3000.
Finally, one of the natural questions in the age of Large
Language Models (LLMs) is whether we can use a model to
generate entire comic strips in the style of Calvin and Hobbes.
CONTRIBUTION
We experimented with starting with noisy sample of edge
maps of images instead of using the original image.
Since
these comics aim to convey a short story within 4 panels, they
are often filled with text.
This is
unideal and does not align with a regular person’s expectation
of the comic’s style.
Removing the text using available open-
source OCR tools like pytesseract is a good starting point.
Alternatively, we could generate better captions for the im-
ages use tools like GPT-4 or Flamingo [11] to incorporate the
text from the images into the captions.
This could aid the dif-
fusion model in understanding the style of the comics better.
Towards improving the dataset, it would also be interesting
to include color images and have the model jointly learn the
style from black and white as well as colored panels simulta-
Thinking about the training mechanisms, we explored
LoRA as described in Section 3.3.
It would be interesting
to explore ideas such as DreamBooth [12], Textual Inversion
[13] and ControlNet [14] since they have shown promising
results for other image-based tasks.
We briefly experimented with applying our style transfer
model on videos as described in Section 4.4.
If we had to assign credit to specific authors,
we would say that Sundar was heavily responsible for dataset
creation and pre-processing, Asvin was responsible for the
fine-tuning of diffusion models and Sloke was responsible for
running extensive experiments with the fine-tuned model.
We would also like to thank Prof. Alan C. Bovik for his in-
sights and guidance in the development of this project.
