The second year of life is marked by
both the emergence of more semantic visual representa-
tions and a better understanding of word meaning.
However, even in suit-
able contexts for word learning like dyadic play sessions,
caregivers utterances are sparse and ambiguous, often re-
ferring to objects that are different from the one to which the
child attends.
We introduce a synthetic dataset of ego-centric images per-
ceived by a toddler-agent that moves and rotates toy ob-
jects in different parts of its home environment while “hear-
ing caregivers’ utterances, modeled as captions.
On one side, there is evidence
that biological organisms learn similar representations for
close-in-time visual inputs [19, 38].
This is called the slow-
ness principle [37] and it supports the building of view-
invariant object representations during, e.g., object manip-
ulations [18, 29].
On the other side, the acquisition of ob-
ject names correlates with a stronger focus on global shape
features relative to texture [13, 17, 28], which spurs per-
formance in name-agnostic categorization tasks [9, 21], see
also [14].
Dyadic play could be particularly important for pro-
moting the emergence of more semantic object representa-
tions, because both learning mechanisms are likely to co-
occur during such play:
the toddler extensively focuses
on/manipulates objects and the caregiver frequently talks
to the toddler about the present objects [26, 31].
How-
ever, naming utterances during such play are sparse [6] and
ambiguous: they typically contain several words unrelated
to the object.
Furthermore, even though toddlers have bi-
ased attention towards held objects [39], they often play
in contexts that admit several other objects in the back-
ground [3, 40], adding another potential source of confu-
In this paper, we investigate two what extent dyadic
play sessions can support the emergence of semantic ob-
ject representations in toddlers.
D) Summary of the learning architecture, see Section 3.2 for details.
Then, we consider a bio-
inspired model of toddlers’ learning that 1) maps close-in-
time visual inputs to similar representations and 2) similarly
aligns the representations of visual inputs and co-occurring
naming utterances (Fig.
The overall model al-
lows us to systematically study the potential impact of the
sparsity and ambiguity of naming utterances on the learned
visual representations.
Related work
Models of toddlers’ object learning Recently, the avail-
ability of datasets of images extracted from toddlers’ head-
mounted cameras [3, 32] has enabled the study of represen-
tations learnt through the senses of toddlers.
However, they only
consider synthetic images based on 9 digits rather than sim-
ulated ego-centric play sessions.
Integrating
constraints from toddlers’ perceputal experience into these
models, like short arms or foveation, can lead to improved
ability to recognize objects in front of novel backgrounds
[2].
However, none of these works studied
whether naming statistics in dyadic play sessions elicit
semantic visual representations.
A recent work showed
that adding sparse labeling on top of contrastive learning
through time enhances visual categorization [2].
To create them, we simulate a toddler that interacts with
objects using the simulation platform ThreeDWorld (TDW)
Image recording At the beginning of each play session,
we place the agent in a random location, with a random
orientation within the Virtual Home Environment [1] (cf.
We approximate an ego-centric view of a seated
toddler by positioning a camera at 0.4 m above the ground.
In addition to the main object, we add 5 to 20 randomly
sampled (without replacement) background objects to the
scene, following the number of toys commonly present in
the field of view of toddlers during play [3].
To place them,
we follow the same procedure as for the main object, but
change the range of distances from the agent to [0.42; 0.875]
m. In contrast to the object being held, we let them fall to
the floor one at a time using the physics engine of TDW.
To simulate natural interactions, in each of the 20 frames
of the play sessions the agent slowly moves and turns the
object along/around the three axes.
We display a high vari-
ability of manipulation sequences by generating all move-
ments with an Ornstein-Uhlenbeck stochastic process for
which we randomly generate a new recall coefficient in
[0.1, 1] at the beginning of each session, for each kind
of manipulation and axis.
Finally, while the agent observes the moving object,
it also executes additional relative small eye movements
around the object in focus (yaw and pitch axis), whose end-
points are sampled from a Normal distribution N ([0, 0], 2×
I).
We also allow the agent to focus its gaze on an object
different from the main one with probability 0.7; however,
we discard these images during training as previous meth-
ods already analyzed the impact of changes of attention on
contrastive through time losses [1, 25].
Assuming that the
pitch/roll rotations rarely go beyond [−20; 20]◦ from their
starting orientation during training, we increase the initial
range of test object orientations to [−20; 20]◦ around the
pitch and roll axis
utterances We
statistics of utterances
developmentally-relevant
a caregiver to a toddler during a play session.
Our study focuses specifically on transcripts
from native English-speaking children residing in North
America and the UK, between the ages of six months
and two years.
After filtering out incomplete utterances and
those containing less than three words, as well as a manual
review, we retained 820 templates.
Self-supervised learning of visual representa-
In this work, we postulate that toddlers learn visual rep-
resentations that 1) slowly change over time and 2) align
with co-occurring linguistic representations.
The first loss aligns embed-
dings of close-in-time images; this entails that seeing an
object from different viewpoints elicits viewpoint invariant
object representations [25].
Since the naming utterance may inform about the category
of the object in focus, this provides weak supervision re-
garding the object category.
We sum up the learning archi-
tecture in Fig.
Thereafter, we
compute embeddings of images z1 = g1(f (x)) using a fea-
ture extractor f and a projection head g1, both implemented
as neural networks [5].
Multimodal contrastive learning For each sampled im-
age xi, we also sample, if provided by the caregiver, its co-
occurring naming utterance li.
The
top part of (2) ensures that co-occurring naming utterances
and images have similar embeddings while the bottom part
prevents all embeddings to collapse into a single vector.
Thus,
we define the (conditional) probability of naming the ma-
nipulated object (versus another object in the background)
as pcorrect = 0.5.
Second, a caregiver approximately pro-
vides, on average, one naming utterance related to the ob-
ject being manipulated per time-extended object manipula-
tion [31] (exactly 2.54
2 = 1.27 in their study).
We assume that consecutive frames
are spaced by one second, making the duration of our play
sessions correspond to the average of 20 seconds reported
in [31].
The ra-
tionale behind using a pre-trained text model lies in our re-
search focus on visual representation learning.
Evaluation To evaluate if the learned representation sup-
ports category recognition, we apply a repeated random
sub-sampling cross-validation to split the non-overlapping
object instances into 2382 train (2/3 of the total) and 1191
test objects (1/3 of the total).
Then, we extract images of
train and test objects to form the train and test datasets,
respectively (see Section 3.1).
To evaluate our representa-
tion with respect to object instance recognition, we extract
novel images of the train objects to build a test dataset.
For
both object instance and category recognition, we freeze the
weights of the vision encoder and train a linear classifier in
an online and supervised fashion on top of the latent visual
representation [4].
2 compares such statistics
with two upper-bound baselines: Oracle refers to super-
vised learning and Ideal refers to a caregiver who always
utters sentences containing the correct object.
We ob-
serve that our model improves category recognition over
not using naming utterances, even though it does not reach
the performance of oracles.
Additionally, we examine how
plausible utterance statistics perform when used to train the
Figure 2.
The model yields similar out-
comes, albeit necessitating significantly more training time,
as it has not yet reached convergence.
Small changes in utterance frequency and am-
biguity drastically impact object representa-
To understand how frequency and ambiguity of utter-
ances affect object representations, we systematically vary
both factors in Fig.
This suggests that toddler’s object learning may be quite
sensitive to caregivers’ presence and the quality of their ut-
terances.
When considered alongside the
accuracy values of classification experiments, this observa-
tion strongly suggests that increased textual attention to the
object name is crucial for learning object representations.
