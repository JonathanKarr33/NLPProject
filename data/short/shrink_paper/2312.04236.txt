Introduction
Stable Diffusion (Rombach et al., 2022) has become increas-
ingly prominent in generating human images.
However, a recurrent issue with this model is
its tendency to produce inaccurate hand images, a problem
we define as the non-standard hand.
The concept of the uncanny valley (Mori, 1970) describes
a sense of unease or discomfort when humanoid figures
closely resemble humans but are not quite lifelike.
In fields like augmented reality (AR), virtual reality (VR),
and gaming, where a seamless and realistic experience is
crucial, such anomalies in hand images can be particularly
unsettling for users.
Accurate hand representation is,
therefore, essential not just for visual appeal, but also for
the functionality and overall user satisfaction in interactive
virtual settings.
But the easier training of the
Diffusion model is a big plus over GANs, which need adver-
sarial training, and VAEs, which need a variational posterior.
The Stable Diffusion uses UNet architecture (Ronneberger
et al., 2015) from image segmentation and shows stable loss
during training and very good performance.
Our contribution is a structured method that
initially identifies ’non-standard’ hand depictions and sub-
sequently corrects them to match the appearance of actual
human hands, referred to here as “standard hands.
Utilizing a specialized dataset and the advanced capabilities
of the YOLOv8 algorithm (LLC, 2023), our approach attains
notable accuracy in detection, with Figure 3 demonstrating
an instance of the detection phase.
Upon identifying anomalies, our system progresses to the
restoration phase.
The process involves a set of defined
Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images
Figure 1.
Based on this skeleton, a template is accurately positioned over the non-standard hand to create the
“control image.
Using this
union mask, the control image, and a descriptive template prompt, we repair the area covered by the mask.
operations: Body Pose Estimation, utilizing Google’s Medi-
aPipe (Lugaresi et al., 2019) to determine the hand’s position
and movement; Control Image Generation, which provides
the Stable Diffusion model with directive images for better
restoration outcomes; ControlNet Inpainting, offering an ini-
tial refinement based on the Control Image; and ultimately,
Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images
(a) Image with both standard and
non-standard-hand.
An illustration of this pipeline
is depicted in Figure 4.
In this report, we present a solution that improves the image
quality produced by Stable Diffusion, particularly the de-
piction of hands.
We create a pipeline to rectify anatomical inaccuracies
in hand images Our results demonstrate the effective-
ness in producing anatomically accurate and realistic
hand images in outputs from Stable Diffusion.
We finetune a detection model to locate and classify
standard and nonstandard hands, and fine-tuned an In-
structPix2Pix model to make high-fidelity adjustments
to the images.
We create a dataset featuring a diverse collection of
hand images, both standard and nonstandard, to facili-
tate comprehensive model training.
Stable Diffusion occasionally generates images with atypi-
cal hands, defined as non-standard hands.
Our method is to
identify these variations and then adjust them to resemble
real-world hands, defined as standard hands.
The following sections provide detailed
insights into each part of the process.
This
dataset should encompass a wide range of images featuring
hands.
Crucially, every hand in these images requires an-
notation.
We begin with HaGRID (HAnd Gesture Recognition Image
Dataset) (Kapitanov et al., 2022) as our foundational dataset.
The pho-
tos were taken indoors under different lighting conditions,
providing a rich assortment of visual details.
This diversity
grants our dataset extensive coverage and high levels of gen-
eralizability, enhancing our model’s capability to process
hands in a wide range of appearances.
We employed the Stable Diffusion
model (Rombach et al., 2022) to recreate the hand areas,
as outlined by their bounding boxes in HaGRID images.
Due to certain limitations of the Stable Diffusion model,
some hands in the redrawn images were classified as non-
standard hands.
From these redrawn images, we manually
selected samples featuring non-standard hands, pairing each
with its corresponding original image, which depicts a stan-
dard hand.
To enable a comprehensive evaluation of
our model, we divided this data into training and testing
sets.
This dataset is specifically prepared for detecting non-
standard hands.
To identify and classify hands as ei-
ther non-standard or standard, we finetuned the YOLOv8
model using the training dataset described above.
After
implementing the trained YOLO model, we annotate the
bounding boxes around the hands and classify them.
Body Pose Estimation
Estimating body pose is crucial for determining the size po-
sition, and chirality of hands in our images.
Notably, its pose landmark detection feature is ca-
pable of detecting the human body skeleton.
This includes
a machine learning model skilled at identifying body land-
marks such as hands, elbows, shoulders, hips, and more in
images or videos, and their structural interconnections, as
shown in Figure 8.
Control Image Generation
ControlNet (Zhang et al., 2023) enhances the Stable Dif-
fusion model by introducing additional input modalities,
including key landmarks, depth maps, and edge maps.
This
integration directs Stable Diffusion to produce images with
more stable and realistic structures.
This
image, tailored to enhance Stable Diffusion’s restoration of
non-standard hands, pushes the model to its limits and im-
proves the output quality.
We select an appropriate Template based on the gesture
and context of the image undergoing restoration.
A background image, identical in size to the redrawn
image but entirely black, is prepared (see Figure 10a).
The Template is initially positioned at the top-left cor-
ner of this background (see Figure 10b).
The angle is calculated as:
Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images
Figure 7.
For a comprehensive view of the process, we compile
samples of the redrawn images, body skeleton, control
image, and union mask into a single visual representa-
tion (see Figure 11c).
These prompts, familiar within the community for gener-
ating high-quality images, reflect prompt engineering do-
main knowledge.
Following the initial
restoration with ControlNet, IP2P enhances the textures,
focusing on giving the hands a more realistic and authentic
appearance to blend seamlessly with the rest of the image.
Initially, the IP2P model is fine-tuned using our training set,
which comprises 9623 pairs of images.
Each pair includes a
real image with a standard hand from the HaGRID dataset
Detecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images
Figure 12.
For restoration, the fine-tuned model is then applied.
