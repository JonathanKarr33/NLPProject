{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nOur\ncontributions include a highly customizable and scalable HVAC and\nbuilding simulator, a rapid configuration method to customize the\nsimulator to a particular building, a calibration method to improve\n1Available online: https://github.com/google/sbsim\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page.\nOverview of the HVAC problem Most office buildings are\nequipped with advanced HVAC devices, like Variable Air Volume\n(VAV) devices, Hot Water Systems (HWS), Air Conditioner (AC)\nand Air Handlers that are configured and tuned by the engineers,\nmanufacturers, installers, and operators to run efficiently with the\ndevice\u00e2\u20ac\u2122s local control loops [15].\nHowever, integrating multiple\nHVAC devices from diverse vendors into a building \u00e2\u20ac\u0153system re-\nquires technicians to program fixed operating conditions for these\nunits, which may not be optimal for every building and every poten-\ntial weather condition.\nIf existing setpoint control policies are not\noptimal under all conditions, the possibility exists that an ML model\nmay be trained to continuously tune a small number of setpoints\nto achieve greater energy efficiency and reduced carbon emission\nRL for HVAC systems We define the state of the office building\n at time  as a fixed length vector of measurements from sensors on\nthe building\u00e2\u20ac\u2122s devices, such as a specific VAV\u00e2\u20ac\u2122s zone air temperature,\ngas meter\u00e2\u20ac\u2122s flow rate, etc.\nThe action on the building  is a fixed-\nlength vector of device setpoints selected by the agent at time ,\nsuch as the boiler\u00e2\u20ac\u2122s supply water temperature setpoint, etc.\nWe\nalso define a custom feedback signal, or reward, that indicates the\nquality of taking an action in the current state, as a weighted sum of\nnegative cost functions for carbon emission, energy consumption,\nand zone-level setpoint deviation.\nTherefore, it is necessary to train in an offline\nsandbox environment that adequately emulates the dynamics of\nthe building before being deployed on the actual building.\n2 RELATED WORK\nConsiderable attention has been paid to HVAC control in recent\nyears, and a growing portion of that has considered how RL and its\nvarious associated algorithms can be leveraged [8, 14, 20, 23, 24].\nThe difficulty here is that data-driven\nmodels often do not generalize well to circumstances outside the\ntraining distribution, especially since they are not physics based.\nOffline RL: The second approach is to train the agent directly\nfrom the historical real world data, without ever producing an in-\nteractive environment [4\u00e2\u20ac\u201c6].\nWhile the real world data is obviously\nof high accuracy and quality, this presents a major challenge, since\nthe agent cannot take actions in the real world and interact with\nany form of an environment, severely limiting its ability to improve\nover the baseline policy that produced the real world data [10].\nOur approach uses a physics-based\nsimulator that achieves an ideal balance between speed and fidelity,\nwhich is sufficient to train an effective control agent off-line.\nFidelity is the simulator\u00e2\u20ac\u2122s ability to\nreproduce the building\u00e2\u20ac\u2122s true dynamics that affect the optimization\nprocess.\nSpeed refers to both simulator configuration time, and\nagent training time, i.e., the time necessary for the agent to optimize\nits policy using the simulator.\nWe\nnote that while RL is the control method proposed in this work, our\ncalibrated simulator is useful as well for other controller methods\nsuch as Model Predictive Control [9].\nTo meet these requirements\nwe designed a lightweight simulator based on Finite Differences\napproximation of heat exchange, with a novel automated procedure\nto go from building floor plans to a custom simulator with little\nmanual effort.\nBuilding Thermal Model We developed our simulation around\nour abstract thermal model for office buildings, shown in Figure 1.\nWe selected water supply temperature \u00cb\u2020 and the air handler\nsupply temperature \u00cb\u2020 as agent actions because they affect the\nbalance of electricity and natural gas consumption, they affect\nmultiple device interactions, and they affect occupant comfort.\nThis method divides each floor of the building into\na grid of three-dimensional control volumes and applies thermal\ndiffusion equations to estimate the temperature of each control\nvolume.\nBy assuming each floor is thermally isolated, (i.e., no heat\nis transferred between floors), we can simplify the three-spatial\ndimensions into a spatial two-dimensional heat transfer problem.\nEach control volume is a narrow volume bounded horizontally,\nparameterized by \u00ce\u201d 2, and vertically by the height of the floor.\nThe\nenergy balance, shown below, is applied to each discrete control\nvolume in the FD grid, and consists of the following components:\n(a) the thermal exchange across each face of the four participating\nfaces control volume via conduction or convection 1, 2, 3, 4,\n(b) the change in internal energy over time in the control volume\n \u00ce\u201d\n\u00ce\u201d , and (c) an external energy source that enables applying\nlocal thermal energy from the HVAC model only for those control\nvolumes that include an airflow diffuser,  .\nWe note that heat transfer using air\ncirculation is effectively the exchange of air mass between control\nvolumes, which we approximate by a randomized shuffling of air\nwithin zones, parameterized by a shuffle probability.\nHVAC Model The HVAC system is modeled as an energy bal-\nance with a lossy thermal exchange between the hot water and air\ncircuits, evaluating temperature differences, and air and water flow\nrates with efficiency parameters.\nSpecial control volumes associated with air diffusers\nallow thermal energy exchange between the HVAC model and the\nFD grid, (cid:164) .\nTo configure the simulator, we require\nfloorplans, and HVAC metadata.\nWe preprocess the detailed floor-\nplan blueprints of the building, and extract a grid that gives us an\napproximate placement of walls.\nWe also employ a user interface\nto label the location of each HVAC device on the floorplan grid.\nGiven floorplans and HVAC layout information, a single\ntechnician was able to generate a fully specified simulation in under\n3 hours that matched the real building in every device and room.\nEach iteration\nstep of our simulator was designed to represent a five minute in-\nterval, and our real world data is also sampled in five minute in-\ntervals.\nWe then initialize the simulator so that its ini-\ntial state matches that of the starting observation, and run the\nsimulator for  steps, replaying the same HVAC policy as was\nused in the real world observations.\nNext, we calculate our sim-\nulation fidelity metric, which is the mean absolute error of the\nmeasured zone air temperatures, at the  th timestep.\n(cid:205)\n(cid:205)\nSimulator Calibration Once we defined our simulation fidelity\nmetric, we can minimize the error by searching over the physical\nparameters that affect the simulation response.\nThe variables tuned\nduring the parameter search included the following: (a) the forced\nconvection coefficient quantifying outside wind and air currents\nagainst the building exterior surfaces; (b) the thermal conductivity,\nheat capacity, and density of exterior and interior walls; and (c)\nthe shuffle probability that controls how likely an air volume will\nbe exchanged with another within the zone to approximate the\ninternal air circulation and interior forced convection.\n4 EXPERIMENT RESULTS\nWe now demonstrate the results of how our simulator, when tuned\nand calibrated, is able to make real-world predictions.\nExperiment Setup To test out our simulator, we obtained teleme-\ntry recordings from our pilot building, a commercial office building\nlocated in Mountain View, California.\nWe obtained floor plan blueprints and used them to\nconfigure a customized simulator for the building, a process that\ntook a single human less than three hours to complete.\nThe first telemetry interval,\n(a), was used to tune the simulator for calibration.\nThe second and\nthird telemetry intervals, (b) and (c), were used for validation.\nThe parameters varied, the sweep range limits, and the\nvalues found that minimized the calibration metric are shown in\nTable 1.\nHowever, the external convection coefficient\nwas higher than under normal weather conditions, and likely is\ncompensating for radiative losses and gains, which were not directly\nCalibration Results Table 2 shows the  -step prediction fi-\ndelity, as MAE, over a six-hour prediction window ( = 72).\nThis was not used in the tuning process, but gives us\nsome insight into how well the calibration process is performing.\nAs indicated in Table 2, our tuning procedure drifts only 0.6\u00e2\u2014\u00a6 on\naverage over a 6 hour period on the tuning set, and we get good\n\fRLEM \u00e2\u20ac\u212223, November 11\u00e2\u20ac\u201c12, 2023, Istanbul, Turkey\nGoldfeder and Sipple\ngeneralization: almost identical error for the same time period of a\ndifferent day.\nIt should be noted that an uncalibrated model had a\nmuch larger error of 2.1\u00e2\u2014\u00a6.\nFor the different\ntime of day scenario, we observe a larger drift of 1.2\u00e2\u2014\u00a6, and the me-\ndian is shifted as well, indicated that tuning on one time of day is\nnot as useful when making predictions on another.\nAt each time step, we plot the real world\ntemperatures in a blue boxplot and the simulator ones in orange.\nRed re-\ngions indicate the simulator was warmer than the validation data,\nblue regions indicate cooler, and white regions had no temperature\ndifference.\nThe ring of blue around the building indicates that our\nsimulator is too cold on the perimeter, which implies that the heat\nexchange with the outside is happening more rapidly than it would\nin the real world.\nThe inside of the building remains red, which\nmeans that despite the simulator perimeter being cooler than the\nreal world, the inside is warmer.\nWe suspect these errors are largely due to assuming thermal isola-\ntion between floors and assuming no radiative exchange with the\noutside.\n"}, {"role": "assistant", "content": "ABSTRACT\nModern commercial Heating, Ventilation, and Air Conditioning\n(HVAC) devices form a complex and interconnected thermody-\nnamic system with the building and outside weather conditions,\nand current setpoint control policies are not fully optimized for\nminimizing energy use and carbon emission. Given a suitable train-\ning environment, a Reinforcement Learning (RL) model is able to\nimprove upon these policies, but training such a model, especially\nin a way that scales to thousands of buildings, presents many real\nworld challenges. We propose a novel simulation-based approach,\nwhere a customized simulator is used to train the agent for each\nbuilding. Our open-source simulator1 is lightweight and calibrated\nvia telemetry from the building to reach a higher level of fidelity. On\na two-story, 68,000 square foot building, with 127 devices, we were\nable to calibrate our simulator to have just over half a degree of drift\nfrom the real world over a six-hour interval. This approach is an\nimportant step toward having a real-world RL control system that\ncan be scaled to many buildings, allowing for greater efficiency and\nresulting in reduced energy consumption and carbon emissions.\n\nKEYWORDS\nHVAC Optimization, Simulation, Reinforcement Learning\n\nACM Reference Format:\nJudah Goldfeder and John Sipple. 2023. A Lightweight Calibrated Simulation\nEnabling Efficient Offline Learning for Optimal Control of Real Buildings.\nIn Proceedings of RLEM \u00e2\u20ac\u212223. ACM, New York, NY, USA, 5 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n\n1 "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nAcknowledg-\ning the inherent unpredictability of precise stock prices [2],\nour research leverages these blue-chip stocks to forecast future\ntrends in stock price movement and volatility [3], [4].\nMoreover, the existing\nmodels are primarily focused on predicting whether trends will\nchange [10], but overlook the significance of the magnitude of\nsuch changes.\nIn reality, the extent of these changes constitutes\nan important part of stock behavior.\nIn this paper,\nthe aforementioned limita-\n3GICS is a method for assigning companies to a specific economic sector\nand industry group that best defines its business operations.\n2A blue chip stock is stock issued by a large, well-established, financially-\n5Although Twitter has recently been re-branded to \u00e2\u20ac\u0153X\u00ef\u00bf\u00bd, this article contin-\nsound company with an excellent reputation.\nBy skillfully combining the strengths of both technical and\nmodel, we achieve state-of-the-art performance in predicting\nstock movements.\nLet S denote a set of target\nstocks for which predictions are sought.\nIn addition, we identify a set \u00ce\u00b5 consisting of tweets,\neach referencing at least one stock in S. These historical\nprices incorporate the opening, highest, lowest, closing prices\nand adjusted closing price7 for each stock.\nHowever, in order to simplify the expla-\nnation of our proposed method, we focus on a specific time\nlag for both movement and volatility prediction.\nThis allows our models to learn from the genuine\nchanges in a stock\u00e2\u20ac\u2122s value that occur due to market conditions,\nrather than being swayed by artificial fluctuations resulting\nfrom corporate actions.\nECON STRUCTURE\nWe present an overview of data ingestion, text preprocess-\ning, and model architecture, as illustrated in Figure 1.\nIn our research, we consistently selected the\ntop six tweets, those garnering the highest daily impressions,\nas our primary input for text preprocessing.\nIn our research, we undertook a sen-\ntiment analysis on a dataset comprising 7.7 million tweets.\nThis optimized model can produce one of three sentiment\noutcomes for each tweet: positive, neutral, or negative.\nFor\nevery tweet e associated with a blue-chip stock s, we compared\nthe predicted sentiment pe with the same day\u00e2\u20ac\u2122s binary adjusted\nprice movement ys\nm gauges the\nprice fluctuation between days t \u00e2\u02c6\u2019 1 and t. Same as problem\nsetup, if the stock price ascends in this interval, ys\nm is labeled\n\u00e2\u20ac\u0153Up\u00ef\u00bf\u00bd, and conversely, if it declines, it\u00e2\u20ac\u2122s labeled \u00e2\u20ac\u0153Down\u00ef\u00bf\u00bd.\nIt\u00e2\u20ac\u2122s\npertinent to note that our dataset only incorporated tweets from\nthe trading hours of day t. To aggregate daily sentiment, if a\nday witnesses a predominance of negative sentiments in tweets\nconcerning a specific stock, we designate that day as \u00e2\u20ac\u0153nega-\ntive\u00ef\u00bf\u00bd.\nIn contrast, if the tweets are primarily characterized by\npositive sentiments, the day is classified as \u00e2\u20ac\u0153positive\u00ef\u00bf\u00bd.\nWithin this context, the embeddings of both\nsectors and tweets are crucial.\nWord tokenization, specifically, refers to the\nsegmentation of text into individual words.\nIn our research, we\nutilized spaCy9\u00e2\u20ac\u201dan open-source natural language processing\ntool\u00e2\u20ac\u201dfor word tokenization of every tweet e. Moreover, a\n9https://spacy.io/\n\fstock ticker like \u00e2\u20ac\u0153AAPL\u00ef\u00bf\u00bd for Apple might inadvertently be\ntokenized into \u00e2\u20ac\u0153AA\u00ef\u00bf\u00bd and \u00e2\u20ac\u0153PL\u00ef\u00bf\u00bd.\nTo circumvent such issues,\nwe established specific rules to ensure that stock tickers are\nconsistently recognized as single, intact tokens.\nFor the subsequent self-awareness\nportion of the model architecture, we replace all tokens in\n\u00ce\u00b5s that correspond to the name of stock s with a special token\n[mask].\nBased on the sector classification rules of GISC, we\nmaintained a dictionary mapping the masked companies to\ntheir respective sectors.\nThese embeddings are high-dimensional vectors that\ncapture the intrinsic semantics of each word.\nWe record the\noriginal length of each tweet, storing this information in the\nvariable l. This data proves crucial when padding comes\ninto play, a technique used to append or prepend sequences\nwith filler values, ensuring uniformity.\nGiven the importance\nof maintaining consistent sequence lengths, especially when\nworking in batches, padding ensures our dataset\u00e2\u20ac\u2122s sequences\nadhere to a standardized length, adding consistency and struc-\nture to our data processing pipeline.\nIn our approach, we adopted a\nbi-directional LSTM (BiLSTM) for the predictor network to\nproduce the tweet and sector embeddings.\nThe first attention is to find tweets relevant to each target sector\nc. We find relevant tweets by using the embedding hc of sector\nc as the query of attention as follows:\nlet \u00ce\u00b5t represent\nConsidering a day t,\nthe collection of\ntweets.\nThe vector wct \u00e2\u02c6\u02c6 R2k,\nwhich can be envisioned as a weighted mean of all the tweet\nembeddings for day t. This averaging is influenced by the\npertinence of each tweet to the specific target sector c. Then,\nwe employ a linear layer to project wct onto all possible\ncompanies S:\ngst = W1(wct) + b1,\nwhere W1 \u00e2\u02c6\u02c6 Rn\u00c3\u20142k, b1 \u00e2\u02c6\u02c6 Rn.\nAs a result of the attention\nfunction of Equation (10), gst \u00e2\u02c6\u02c6 Rn represent that summarizes\nall tweets at day t considering the relevance to sector c. Our\nsecond attention uses the resulting tweet vector to find the\nmost relevant stocks at the moment.\nThis is done by using gst\nas the query vector of attention to aggregate the stock price\nfeatures at day t based on the stock embeddings:\nThe micro trend vector ist \u00e2\u02c6\u02c6 Rn encapsulates all tweets for\nday t, integrating the information from companies associated\nwith the sector.\nThis vector is then combined with the stock\nprice vector xt \u00e2\u02c6\u02c6 Rn\u00c3\u2014p which generates from Yahoo\u00e2\u20ac\u2122s stock\ndata and the macro trend vector at, serving as the input to the\nAttention GRU for forecasting stock movement and volatility.\nThe model is primarily designed to ex-\ntract nuanced information from stock market, macroeconomic\nfactors, and tweet data.\nOnce the macro trend at and micro\ntrend ist are generated, they are concatenated with the stock\nprice feature xst \u00e2\u02c6\u02c6 Rp using a linear layer, producing a multi-\nlevel feature.\nV. EXPERIMENT\nWe conduct experiments to answer the following questions\nabout the performance of framework:\nQ1.\nDoes using our model to extract data\nyield better results than performing sentiment analysis using\nthe entire 7.7 million tweets as input?\nA. Baseline Methods\nWe evaluate our model\u00e2\u20ac\u2122s effectiveness by comparing it with\ntechnical and fundamental analysis models.\nThe best is in bold, Our ECON shows the\nbest performance in all evaluation metrics.\n\u00e2\u20ac\u00a2 Long Short-Term Memory (LSTM) [34] is a type of\nrecurrent neural network (RNN) architecture specifically de-\nsigned to tackle the vanishing and exploding gradient problems\nencountered in traditional RNNs.\nLSTM units use gate mech-\nanisms to regulate the flow of information, making them well-\nsuited for learning from long-term dependencies in sequences.\n\u00e2\u20ac\u00a2 SLOT [9] is based on ALSTM, it captures correlations\nbetween tweets and stocks through self-supervised learning.\nTo test whether our Twitter filter can still accurately capture\nmarket sentiment even with a significantly reduced Twitter\n\u00e2\u20ac\u00a2 AGRUD-A concatenated the sentiment score of all 7.7\nmillion tweets with stock prices and macroeconomic data,\nkeeping the structure of main predictor AGRUD unchanged.\nBy retaining only the tweets with the highest impres-\nsions, the results for predicting stock price movement and\nvolatility are consistent with those obtained using sentiment\nanalysis on all tweets.\nFor stock volatility prediction, given that data points in-\nvolving stock price changes greater than 5% only constitute\na minor portion of our dataset, using accuracy to evaluate\nECON can be misleading due to model overfitting.\nIt primarily focuses on how the model\ndistinguishes between positive and negative samples.\nECON is observed to be the\nsuperior baseline model in terms of accuracy and MCC for\nmovement prediction.\nECON surpasses both of these models by significant margins.\nWe argue this improvement of price movement\nprediction is attributed to use of macro and micro trends\nto investigate the influence of multiple sectors on stocks.\nAs shown in Table V, our\nablation study revealed that incorporating multi-dimensional\nperception of stock market significantly enhances the pre-\ndictive capabilities of the model for stock movement and\nvolatility.\nA noteworthy point is that the result of ECON(A)\ndemonstrates a significant role of the macro trend in enhancing\nthe accuracy of volatility prediction.\n"}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dPredicting stock market is vital for investors and\npolicymakers, acting as a barometer of the economic health. We\nleverage social media data, a potent source of public sentiment, in\ntandem with macroeconomic indicators as government-compiled\nstatistics, to refine stock market predictions. However, prior\nresearch using tweet data for stock market prediction faces\nthree challenges. First, the quality of tweets varies widely. While\nmany are filled with noise and irrelevant details, only a few\ngenuinely mirror the actual market scenario. Second, solely\nfocusing on the historical data of a particular stock without\nconsidering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly,\nsimply forecasting the direction of price movement without\nassessing its magnitude is of limited value, as the extent of the rise\nor fall truly determines profitability. In this paper, diverging from\nthe conventional methods, we pioneer an ECON (A Framework\nLeveraging Tweets, Macroeconomic Indicators, and Historical\nPrices to Predict Stock Movement and Volatility). The framework\nhas following advantages: First, ECON has an adept tweets filter\nthat efficiently extracts and decodes the vast array of tweet\ndata. Second, ECON discerns multi-level relationships among\nstocks, sectors, and macroeconomic factors through a self-aware\nmechanism in semantic space. Third, ECON offers enhanced\naccuracy in predicting substantial stock price fluctuations by\ncapitalizing on stock price movement. We showcase the state-\nof-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements\nand volatility.\n\nIndex Terms\u00e2\u20ac\u201dstock market prediction, tweets extraction, time\n\nseries forecasting, sentiment analysis, macroeconomic factors\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nIntroduction\nThe domain gap manifests when a model, trained on a\nsource domain with annotated samples, is deployed in a\ntarget domain that has a distribution shift compared to the\nsource domain.\nUnsupervised domain adaptation (UDA)\nconsiders a target domain with unlabeled data and aims to\nmitigate the domain gap by aligning the source and target\ndomain distributions using the knowledge learned from the\nsource domain.\nMost of the current UDA methods conduct\ndomain adaptation by either minimizing the source-target\ndistribution discrepancy [39, 47], or by adversarially align-\ning the feature spaces of the source and target data [7, 9].\nHowever, such models need access to the source data dur-\ning adaptation, and therefore cannot be applied to cases\nwhere the source domain is not available or when the source\ndata is sensitive or confidential.\nTo address these concerns,\nsource-free domain adaptation (SFDA) [20, 45] was pro-\nposed, where domain adaptation to the target distribution\ntakes place with a source-pretrained model using only the\nunlabelled target data.\nWhile the vast majority of existing UDA literature deals\nwith closed-set domain adaptation, where the target do-\nmain and source domain share the same classes, a more\nrealistic scenario is open-set domain adaptation (OSDA)\n[22, 30] where the target domain contains samples belong-\ning to novel classes that are absent in the source domain.\nIn the OSDA setting, closed-set UDA solutions would en-\nforce alignment of the source and target feature spaces un-\nder the unknown category mismatch, leading to negative\ntransfer [6] and deteriorating performance.\nThe majority of\nthe existing OSDA methods [22, 30] utilize domain adver-\nsarial learning techniques to align the source domain with\nonly the known classes in the target domain, leaving out\nthe target-unknown classes.\nSuch methods fail to properly\nlearn the features for the unknown classes, and hence no\nclear decision boundary between the known classes and the\nunknown class in the target domain is realized.\nUDA methods de-\n\fsigned to work in both closed and open-set settings [15, 31],\nhave attempted to conduct self-supervised learning (SSL)\nto discover latent target domain features without explicit\ndistribution matching.\nIn this work, we introduce Unknown Sample Discovery\n(USD) as a source-free OSDA (SF-OSDA) method that uti-\nlizes an ensemble-based pseudolabeling strategy for the tar-\nget data, and generates known and unknown target sub-\nsets based on Jensen-Shannon distance (JSD) between the\npseudolabels and the predictions from a teacher model.\nUSD uses 2-component Gaussian Mixture Model (GMM)\nto model the target domain JSD, where the distribution with\nthe lower mean JSD is considered to be of the known class\nsamples and that with the greater mean JSD is taken as that\nconsisting of unknown class samples.\nThe adapted target model infers new target sam-\nples in one one of the known classes or the unknown class,\nwithout operating on the entire target dataset first to iden-\ntify known and unknown samples.\n\u00e2\u20ac\u00a2 USD utilizes curriculum adaptation to progressively learn\nthe known class feature space first, and the unknown class\nfeature space later, thus enabling robust alignment of the\nentire target space with the source domain.\nUnsupervised domain adaptation\nDomain gap originates from the distribution shift between\nthe source domain where a deep network model is trained,\nand the target domain where the model is deployed [39].\nDistant supervision for SFDA [19] iteratively\nassigned pseudo-labels to the target data and used them to\nlearn a domain invariant feature space and obtain the tar-\nget class centroids.\n[20] introduced SHOT\nwhich adapts the source-pretrained feature encoder to the\ntarget domain via self-training with information maximiza-\ntion [13, 34] and self-supervised clustering for pseudola-\n\fbeling, while transferring the source hypothesis (classifier\nmodel) to the target.\nOpen set domain adaptation\nIn addition to aligning the source and target subspaces, a\ncritical step in OSDA is detecting target samples from novel\nor unknown categories that are absent in the source do-\nmain.\n[30] adversarially aligned the source domain and\nknown target subdomain, where the unknown target sam-\nples were identified based on a preset threshold.\n[20] and [46] are SF-\nUDA methods that also conduct SF-OSDA by separating\nthe known and unknown samples based on clustering the\nsample entropies into two clusters, and taking the cluster\nwith lower mean entropy as the known subset.\nThe source model is trained by minimizing the standard\nFigure 1.\nThe stu-\ndent model receives pseudolabels for the target samples (see Fig-\nure 1) and is optimized using a combination of triplet, weak-strong\nconsistency, information maximization (IM) and cross-entropy\nlosses.\nTo account for the\nnovel class samples in the target domain, the source classi-\nfier hs is expanded in the student and teacher models to in-\nclude an additional trainable output node for the unknown\nclass.\nThe known class nodes in the target classifiers hS\nt , for the student and teacher respectively, are ini-\ntialized with hs, and remain frozen during adaptation.\nThe\nunknown class nodes in hS\nt and the feature extractors\nt are adapted using only the unlabeled target samples.\nKnown-unknown sample separation\nThe first step for target adaptation is to reliably separate the\nknown class samples and the novel class samples in the tar-\nget data.\nis taken from the student model f S\nt = arg max\nThe index corresponding to the maximum averaged soft-\nmax output is taken as the hard pseudolabel \u00cb\u2020yi\nt for each tar-\nget sample xi\nt. These pseudolabels are however only over\nthe Cs known classes, and therefore the samples need to be\nsplit into known class subset X K\nand unknown class sub-\nt .\nEntropies for all samples are calculated at the\nbeginning of each epoch and then normalized in the range\nof [0, 1] by dividing the each sample entropy by log Cs.\nThe cluster with the higher mean entropy or un-\ncertainty is considered to be the one containing unknown\nsamples, while the other cluster with lower mean entropy is\ntaken as containing known class samples.\nwhere, KL(a, b) is the Kullback-Leibler divergence be-\ntween a and b, and pi\nt)) is the output soft-\nmax probability for target sample xi\nt from the target teacher\nWe consider the unknown class samples in the target do-\nmain as noisy samples when predictions are made over only\nthe known Cs classes.\nIn comparison to entropy or cross-\nentropy loss, JSD is symmetric by design and ranges be-\ntween 0 and 1.As shown in Figure 1, when plotted against\nthe number of samples, JSD produces a bimodal histogram.\nWe consider the samples belonging\nto the distribution with the lower-mean Gaussian as samples\nfrom one of the known classes, and consider those samples\non the higher-mean Gaussian as coming from the unknown\ntarget class.\nThe remaining target samples are in-\ncluded in the unknown subset X U\nupdated accordingly, where the known subset retain their\nearlier assigned pseudolabel from among the Cs classes,\nand the unknown subset of target samples get the new un-\nknown class pseudolabel |Ct|.\nTeacher-student co-training and regularization\nUSD simultaneously adapts the student and teacher tar-\nget models, such that the student model parameters \u00ce\u00b8S\nupdated based on the minibatch gradient descent, and the\nteacher network parameters \u00ce\u00b8T\nt are updated as temporally\nensembled version of the student network [36] at the end of\neach epoch as follows.\nThe output zia\n)]a of the teacher model\non an weakly augmented known class sample is taken as the\nanchor, and the corresponding output zi+\nthe strongly augmented version of the same sample from the\nstudent model is taken as the positive instance.\nWhen loss\ntK on the known sample subset increases, \u00ce\u00b3 marginally\ndecreases to accommodate further adaptation on the known\nsamples in the subsequent iterations.\nBut if Lce\ntK decreases\nby a large margin, \u00ce\u00b3 decreases accordingly to progressively\nadapt to the unknown samples in the following iterations.\nTo encourage individually precise and globally diverse\npredictions, USD further minimizes the information maxi-\nmization (IM) [20] loss as formulated in [37, 38].\nAlgorithm 1: Pseudocode for USD\nInput: Source trained model fs and nt unlabled\ntarget data samples xi\nOutput: Target adapted student model f S\nInitialization: Teacher target model f T\nt and student\nt , are both initialized\ntarget model f S\nwith parameters \u00ce\u00b8s from fs\n1 for epoch = 1 to E do\nConduct M = 6 weak-strong augmentations\nand assign ensemble averaged pseudolabels \u00cb\u2020yi\nusing eq.\nThe threshold\n\u00ce\u00b4t for known-unknown sample separation is set at 0.8, and\nthe momentum parameter m for temporal ensembling is set\naccording to schedule in [44] with a maximum at 0.9995.\nEvaluation metrics\nThe mean-per-class accuracy OS over all known classes\nand the unified unknown class for all the target data may\nbe considered as a metric for evaluating OSDA.\nA better metric is therefore to calculate the\nmean-per-class accuracy OS* over only the known classes,\nand the accuracy UNK for the unknown class, and then take\nthe harmonic mean HOS of the two for fair evaluation over\nthe known and the unknown classes.\nSimilar\nto Office-31, SHOT adapts better to the known classes, but\nfails to competently identify unknown samples, while AaD\nperforms worse on the known classes and better on the un-\nknown samples.\nSHOT severely suffers from negative transfer\nin the unknown class, while AaD fails to learn the target-\nknown feature space.\nNonetheless, if the\nthreshold is set too high (such as 0.9), too few samples may\nbe denoted as known samples, and this could lead to inferior\nperformance.\ncontributes to the adaptation, and leav-\ning out any one of them hurts performance.\nWe observe that\ncurriculum guidance considerably benefits adaptation and\nthe final average HOS increases by > 1.5% (from 64.2%\nto 65.9%) when such guidance is included.\nBoth the\nweakly and strongly augmented samples are fed through the\nstudent network, and losses Ltrip\nare calculated over\nthe student model outputs between the weak and strong aug-\nmentations.\nAs\nseen in Figure 4, in the absence of co-training, the student\nmodel adapts faster, but its performance drops from its peak\nduring the course of adaptation due to error accumulation.\nWe see that such clustering is not better than tak-\ning the hard predictions from the student model as pseudola-\nbels.\n"}, {"role": "assistant", "content": "Abstract\n\nOpen Set Domain Adaptation (OSDA) aims to adapt\na model trained on a source domain to a target domain\nthat undergoes distribution shift and contains samples from\nnovel classes outside the source domain. Source-free OSDA\n(SF-OSDA) techniques eliminate the need to access source\ndomain samples, but current SF-OSDA methods utilize only\nthe known classes in the target domain for adaptation, and\nrequire access to the entire target domain even during in-\nference after adaptation, to make the distinction between\nknown and unknown samples. In this paper, we introduce\nUnknown Sample Discovery (USD) as an SF-OSDA method\nthat utilizes a temporally ensembled teacher model to con-\nduct known-unknown target sample separation and adapts\nthe student model to the target domain over all classes using\nco-training and temporal consistency between the teacher\nand the student. USD promotes Jensen-Shannon distance\n(JSD) as an effective measure for known-unknown sample\nseparation. Our teacher-student framework significantly re-\nduces error accumulation resulting from imperfect known-\nunknown sample separation, while curriculum guidance\nhelps to reliably learn the distinction between target known\nand target unknown subspaces. USD appends the target\nmodel with an unknown class node, thus readily classifying\na target sample into any of the known or unknown classes in\nsubsequent post-adaptation inference stages. Empirical re-\nsults show that USD is superior to existing SF-OSDA meth-\nods and is competitive with current OSDA models that uti-\nlize both source and target domains during adaptation.\n\n1. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nDeep learning (DL) models especially temporal convolution net-\nworks (TCN)[2][3] are widely used for MBTS data mining due to\ntheir good performance and low resource consumption, which uses\ndilated causal convolutions to capture long-term temporal depen-\ndency.\nHowever, the performance of existing models is limited due to\nthe ignorance of the distribution gap across modalities and under-\nfitted MS feature extraction modules.\nHence, to further improve\nthe performance, this paper presents a multi-scale and multi-modal\nbiomedical time series contrastive learning (MBSL) network which\nincludes the following main parts.\n1) Inter-modal grouping tech-\nnique to address distribution gap across modalities.\nMBTS also features diverse structured patterns, such as the\nseasonally dominant PPG and trend dominant SpO2, suited for fre-\nquency and time domain modeling respectively[9].\nTherefore, the\ncommonly used parameter-sharing encoder for all modalities[3][8]\nmay limit the capability of the encoders to represent different modal-\nities.\nAn intuitive approach is to customize a specific encoder for\neach modality to extract heterogeneous features, but this approach\nwould be inefficient and lack robustness[10].\nTo achieve a more ca-\npable and robust model, inter-modal grouping (IMG) is proposed\nto group MBTS into different groups with minimum intra-model\ndistances, thus data in the same group manifest similar distribu-\ntions, which is simple to represent with a shared encoder.\nThen\ndistinct encoders are trained for each group to handle multi-modal\ndata distribution gaps.\nThis\npaper proposes a lightweight multi-scale data transformation using\nmulti-scale patching and multi-scale masking, where different patch\nlengths are defined to obtain input tokens with semantically mean-\ningful information at different resolutions, and the different mask\nratios are adaptively defined according to the scale of features to be\nextracted.\nInappropriate data\naugmentation can alter the intrinsic properties of MBTS, leading to\nthe formation of false positive pairs.\nThis issue causes many time\nseries contrastive learning models to encode invariances that might\nnot align with the requirements of downstream tasks.\nThe intuition behind the idea is that different modalities commonly\nreflect the physiological state so the modal-invariant semantic is use-\nful information for BMA.\nThe contributions are summarized as fol-\n* Hao Wu is the correspondence author.\nPersonal use of this material is permitted.\nPermission from IEEE must be obtained for all other uses, in any current or future\nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or\nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n\fFig.\nxi\nrepresents the i-th input series, v1\ni represents the raw data and embedding of the first group of the i-th sample, respectively.\nWe propose multi-scale patching and multi-scale masking for\nextracting features at various resolutions and adaptively en-\nhancing the capacity of the model, respectively.\nInter-modal Grouping\nAs mentioned before, the distribution gap severely impairs the gen-\neralization ability of the network.\nIntuitively, MBTS is divided into distinct grouped modal-\nities based on their inter-modal distances.\nOur approach\naligns with the intuition that similar data should be processed simi-\nIn detail, we consider a set of M modalities of the data, repre-\nsented as X1, .\nImproved multi-scale Temporal Dependency Extraction\nRecently, TCN has been proven to be a highly effective network ar-\nchitecture for BMA.\nHowever, real-world MBTS is very complex,\na unified receptive field (RF) within each layer of TCN is often not\npowerful enough to capture the intricate temporal dynamics.\nWhat\u00e2\u20ac\u2122s\nmore, the effective RFs of the input layers are limited, causing tem-\nporal relation loss during feature extraction.\nTo adaptively learn a\nmore comprehensive representation, we designed an MTDE mod-\nule, which uses MS data transformation (patching and masking) and\nMS feature extraction encoder.\nMulti-scale patching and Multi-scale Masking\nMulti-scale patching Different lengths of patches are used to trans-\nform the time series from independent points to tokens with semantic\ninformation within adaptive subseries, allowing the model to capture\nfeatures of different scales.\nIn particular, by aggregating more sam-\nples, long-term dependence with less bias can be obtained instead of\nshort-term impact.\nSimultaneously, patching can efficiently alleviate\nthe computational burden caused by the explosive increase in mul-\ntiple TCN branches by reducing the sequence length from L to L\nwhere P is the patch length.\nMulti-scale Masking Masking random timestamps helps im-\nprove the robustness of learned representations by forcing each\ntimestamp to reconstruct itself in distinct contexts, which can be\nformulated as xmaski = xi \u00e2\u02c6\u2014 m, where m \u00e2\u02c6\u02c6 0, 1.\nFor instance, for coarser-grained features,\nlarger MR is preferred so that ample semantic information can still\nbe retained while introducing strong variances,\nthereby enhanc-\ning the robustness.\nW represents the length of the sig-\nnal.\nThe best results are in bold and the second best are underlined.\nThe best results are in\nbold and the second best are underlined.\nExperimental Setup\nThe size of the hidden layers and output layer is 32 and 64 respec-\ntively.\nThe temperature \u00cf\u201e of the contrastive learning is 0.1.\nThe two latest\ncontrastive learning works, i.e.\nTFC[8]and TS2Vec[3], and one lat-\nest non-contrastive self-supervised learning work TimeMAE[12] are\nused as baseline models, and we also compare them with SOTA un-\nder the corresponding datasets.\nMBTS regression\nWe evaluate MBSL on open-source time series regression datasets:\nRespiratory rate (RR) detection[15], providing 53 8-minute PPG\ndata segment (125 HZ) recordings, and exercise heart rate (HR)\ndetection[16], including 3196 2-channel PPG and 3-axis accelera-\ntion sampled at a frequency of 125Hz.\nEvaluation method: 1) RR The PPG signal and its Fourier\ntransform in frequency domain signal are used as multivariate bi-\nological time series.\nThe average mean absolute\nerror (MAE) and the standard deviation (SD) across all patients are\nused to evaluate the performance of the model [6].\nThen MS time series pass through parallel TCN\nencoders with different kernel sizes and layers and then concatenated\nat the end.\nCross-modal Contrastive Loss\nPrevious research on contrastive representation learning mainly\ntackled the problem of faulty positive pairs and noise in data.\nthe process of generating positive pairs by data augmentation, in-\ncorrect positive samples are also introduced, leading to suboptimal\nperformance.\nIn this work, the same segments from different modalities are\nchosen as positive pairs instead of generating them by data augmen-\ntation, thus avoiding the risk of introducing faulty positive pairs.\nBe-\nsides, this is beneficial to learn the semantic information shared by\nmodalities because signals from different modalities of the same seg-\nments can intuitively reflect the subject\u00e2\u20ac\u2122s physiological state.\nHence,\nduring reducing the loss of contrastive learning, effective informa-\ntion among different modalities of the same segment will be max-\nimized while redundant information across different segments will\nbe ignored.\nThe best results are in bold\nand the second best are underlined.\nWithout patching and masking, MS feature extraction is\nlimited, leading to poor performance.\nUsing moderate mask ratios\n(0.1) and patch lengths (10) also reduces effectiveness, highlighting\nthe need for varied data transformations for different scale feature\nextraction.\nElim-\ninating or replacing this loss with standard instance contrastive loss\nleads to reduced model performance, underscoring the effectiveness\nof the cross-modal contrastive loss.\n"}, {"role": "assistant", "content": "ABSTRACT\n\nMulti-modal biomedical time series (MBTS) data offers a holistic\nview of the physiological state, holding significant importance in\nvarious bio-medical applications. Owing to inherent noise and dis-\ntribution gaps across different modalities, MBTS can be complex\nto model. Various deep learning models have been developed to\nlearn representations of MBTS but still fall short in robustness due\nto the ignorance of modal-to-modal variations. This paper presents\na multi-scale and multi-modal biomedical time series representa-\ntion learning (MBSL) network with contrastive learning to migrate\nthese variations. Firstly, MBTS is grouped based on inter-modal dis-\ntances, then each group with minimum intra-modal variations can\nbe effectively modeled by individual encoders. Besides, to enhance\nthe multi-scale feature extraction (encoder), various patch lengths\nand mask ratios are designed to generate tokens with semantic in-\nformation at different scales and diverse contextual perspectives re-\nspectively. Finally, cross-modal contrastive learning is proposed to\nmaximize consistency among inter-modal groups, maintaining use-\nful information and eliminating noises. Experiments against four\nbio-medical applications show that MBSL outperforms state-of-the-\nart models by 33.9% mean average errors (MAE) in respiration rate,\nby 13.8% MAE in exercise heart rate, by 1.41% accuracy in hu-\nman activity recognition, and by 1.14% F1-score in obstructive sleep\napnea-hypopnea syndrome.\n\nIndex Terms\u00e2\u20ac\u201d bio-medical time series, multi-modal, represen-\n\ntation learning, contrastive learning\n\n1. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nMedical report generation (MRG), a typical vision-language\ntask, aims to describe medical images with professional and\naccurate sentences that encompass detection sites, abnormal\nsituations, etc.\nThe automation of MRG will reduce the heavy\nworkload of physicians during diagnosis and treatment.\nFor MRG, challenges arise from the diverse range of im-\nage modalities, high anisotropy, complex implicit content,\nand the abundance of professional medical terms in the ac-\ncompanying text.\nSimilar to the success trend of VLP mod-\nels tailed for natural images [3, 4], M-VLP models benefit the\nlearning of transferable medical image representations.\nFor\nexample, BiomedCLIP [2] is endowed with medical image-\ntext contrast ability by pre-training on 15 million biomedical\nfigure-caption pairs extracted from PubMed Central.\nHow-\never, these M-VLP models still suffer from the inability of\nMRG due to the lack of a vision-conditioned language model,\nwhich is addressed by the subsequent studies [5, 6] that com-\npose foundation models (FM), i.e.\nWhile the integration of VLP models and LLMs presents\na solid basis for generating texts from vision data, there are\nstill several critical steps to developing specialist models for\nMRG.\nLoRA [8] and Prefix-tuning [9], adjusting pre-trained mod-\nels for downstream tasks at low cost to continuously learn\nthe specific knowledge and achieve domain adaptation.\nAn-\nother one is enhancing the task-specific medical knowledge\nof pre-trained models [1], which is requisite to produce tar-\ngeted descriptions rather than general ones.\nHowever, exist-\ning works [6] mainly focus on the PEFT of LLMs and neglect\nthe adaptation of the vision part.\n2) Within MAKEN, we\nintroduce adapter tuning to calibrate medical image represen-\ntations and design a medical knowledge enhancement loss\nto reinforce the assimilation of medical terms.\nIt is composed of a medical image encoder with adapters, a feature extractor\nnamed Q-Former, and LLM.\nThe parameters of the image encoder and the LLM were fixed during the training.\non ImageCLEFmedical 2023 [10] validate the superiority of\nour proposed MAKEN against strong competitors and the\ncomprehensive quality of the generated text.\nBased\non the advanced multimodal foundation model BLIP-2 [4],\nwe introduce two core designs: the adapter tuning module for\nthe vision FM and the medical knowledge enhancement loss\nthat augments the weights associated with medical norms.\nThe initial encoding of the input medical image I using\nthe vision encoder of BiomedCLIP [2], which was pre-trained\nthoroughly in medical-domain vision-language processing.\nThe lightweight vision encoder employs the structure of ViT-\nB/16 [11] model, and its parameters remain frozen during our\ntraining.\nWe explore the decoder-based OPT 2.7B model [12]\nfor the frozen large language model.\nIt plays\na pivotal role in bridging the gap between the image and text\nfeature domains.\nThe prefix tokens condition the decoder-based LLM to gener-\nate text that is closely related to the input medical image.\nWe\nintroduce the medical knowledge enhancement loss, collab-\norating with language model loss to train the entire network.\nAdapter Tuning with Vision Encoder\nWe incorporate parameter-efficient\nlow-rank adapters be-\ntween each layer of the vision encoder to preserve the rich\nprefetched domain-specific knowledge of medical images,\nwhile continually learning the effective visual features for the\nspecific MRG task.\nThe structure of vision adapter attached is illustrated in\nFig.\nWe performed\nmedical entity recognition and obtained statistical data for\neach text in the training data.\n, tK are the\nindices of medical terms in Ti.\nshould pertain to a precise nominal phrase or word in a med-\nical entity.\nLmke( \u00cb\u2020Ti) = \u00e2\u02c6\u2019\nj) log P\u00ce\u00b8( \u00cb\u2020wj = wi\n\u00cb\u2020Ti represent the text predicted by LLM, \u00cb\u2020wj denote the word\npredicted at the corresponding position, and N represent the\nlength of the entire text.\nThe attention weights G(wi\nmedical term in M Wi are derived from the word frequency.\nExperimental Settings\nWe evaluate our MAKEN on the dataset provided by Image-\nCLEFmedical 2023 [10], which is an updated and extended it-\neration of the Radiology Objects in COntext (ROCO) dataset.\nEvaluation\nwas performed on the validation set due to an undisclosed\nground truth for the test set.\nData preprocessing on ImageCLEFmedical 2023 mainly\ncontains removing the disruptive phrases (e.g.\n\u00e2\u20ac\u0153grey matter of 1 cm \u00c3\u2014\n1.3 cm), and non-ASCII codes in text data, which cannot be\nprecisely generated.\nAs presented in Table 1, we extracted nominal words from\nUMLS terms in the training set text and presented the top\n10 medical term nouns based on their frequency of occur-\nrence.\nAfter filtering out words with frequencies less than\n5, we identified a total of 3196 medical terms in the train-\ning set.\nThe statistics were utilized in our proposed medical\nterm-based attention (Sec 2.3), enhancing feature extraction\nand contributing to accurate results.\nThe global scaling factor \u00ce\u00b1 and MKE loss weight \u00ce\u00b2 were set\nat 0.2 and 0.5, respectively.\nWe analyzed the top\n5 teams\u00e2\u20ac\u2122 results on the validation set, executing exhaustive\ncomparisons.\nDomain-specific fine-tuning was executed on\nthe general image-text generation models, such as BLIP-2 [4]\nand CARE [13], which were integrated for comparison.\nCompared results of MAKEN, closeAI [5] and\nground truth medical text in ImageCLEFmedical 2023.\nexperiment.\nIt is worth noting that\nthe general cutting-edge models, even after fine-tuning, still\nstruggle to perform well in the medical domain.\nIt is also worth mentioning that CSIRO\u00e2\u20ac\u2122s approach [17]\nachieved the highest BERTScore of 0.6425 but relatively\nlow other metrics with 0.1615, 0.2446, 0.2025 for BLEU-1,\nROUGE-1 and CIDEr, respectively, due to their reinforce-\nment learning design to this specific metric.\nWe did not include CSIRO\u00e2\u20ac\u2122s results in Ta-\nble 2 for fair comparisons since all the evaluations are based\non the validation set, which was not reported in [17].\nFor the\nmedical image above with a concise caption, both MAKEN\nand closeAI\u00e2\u20ac\u2122s approach [5] correctly generated the com-\nmon description (\u00e2\u20ac\u0153sagittal T2-weighted MRI).\nRemarkably,\nMEKEN provided precise details, identifying the specific\nregions (\u00e2\u20ac\u0153spinal cord from c3 to c5) where the abnormal\nmanifestations (\u00e2\u20ac\u0153high signal) occurred based on the medical\nimage.\nIn the case below, involving a complex and lengthy\ntext, predicting accurate findings in the report proved to be\nconsiderably challenging for the models.\nMoreover, hallucinations occurred,\nas there was no evidence indicating a pathological region in\nthe \u00e2\u20ac\u0153upper lobe.\nIn contrast, MAKEN accurately predicted\nthe pathological type, aligning with indicators in the image\n(\u00e2\u20ac\u0153pleural effusion and \u00e2\u20ac\u0153pleural mass to \u00e2\u20ac\u0153undifferentiated\ntumor), with precise adjuncts.\nThese instances indicate that\nour proposed method effectively learned information from\nshort and long texts.\nAs observed, individually removing the adapter tuning and\nMKE loss led to performance degradation, and deleting both\nyielded the poorest results.\nFurthermore, we emphasized the significance of data clean-\ning in training a more robust model.\n"}, {"role": "assistant", "content": "ABSTRACT\n\nMedical report generation demands automatic creation of\ncoherent and precise descriptions for medical images. How-\never, the scarcity of labelled medical image-report pairs poses\nformidable challenges in developing large-scale neural net-\nworks capable of harnessing the potential of artificial intel-\nligence, exemplified by large language models. This study\nbuilds upon the state-of-the-art vision-language pre-training\nto customize general\nand fine-tuning approach, BLIP-2,\nlarge-scale foundation models.\nIntegrating adapter tuning\nand a medical knowledge enhancement loss, our model sig-\nnificantly improves accuracy and coherence. Validation on\nthe dataset of ImageCLEFmedical 2023 demonstrates our\nmodel\u00e2\u20ac\u2122s prowess, achieving the best-averaged results against\nseveral state-of-the-art methods. Significant improvements in\nROUGE and CIDEr underscore our method\u00e2\u20ac\u2122s efficacy, high-\nlighting promising outcomes for the rapid medical-domain\nadaptation of the vision-language foundation models in ad-\ndressing challenges posed by data scarcity.\n\nIndex Terms\u00e2\u20ac\u201d Medical Report Generation, Adapter\nTuning, Knowledge Enhancement, Vision-Language Foun-\ndation Models\n\n1. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nIn the realm of artistic expression and cultural preservation,\nthe desire to revitalize a timeless treasure such as Calvin and\nHobbes comics has fueled our motivation to seamlessly blend\nnostalgia with modern techniques.\nThis endeavor not only\naims to breathe new life into the beloved comic strips but also\nstands as a testament to the ongoing exploration of advanced\nmachine learning approaches.\nOne such avenue of exploration is the utilization of stable\ndiffusion fine-tuning, a cutting-edge process that holds the\npromise of achieving a delicate balance between preserving\nthe essence of the original artwork and infusing it with con-\ntemporary flair.\nBy undertaking this project, we endeavor to\nunravel the intricacies of stable diffusion fine-tuning, delving\ninto its nuances to better comprehend its potential applica-\ntions in the domain of artistic style transfer.\nAmidst a myriad of deep learning based style transfer ap-\nproaches, we chose stable diffusion as our preferred method\ndue to its unique ability to synthesize artistic styles.\nRight Results of our fine-tuned stable diffusion\nmodel performing style transfer on these images into the Calvin and\nHobbes comics style.\nour attempt at learning how to perform fine-tuning of stable\ndiffusion models and revitalize Calvin and Hobbes comics\nalong the way.\nPages from the beginning and the end with the foreward and\npublishing details were discarded since we wanted to focus\non images with the original comic strips.\nBlack and White Pages\nThe original Calvin and Hobbes comic strips were syndi-\ncated and published in newspapers worldwide every day for\n10 years from 1985 to 1995.\nThe comic strips published from\nMonday to Saturday were in black and white and consisted of\nfour panels in a single row.\nThe black and white pages\nare nearly binary images as the pixels are clustered in two\nmodes around 3 and 250 in grayscale values.\nThe black and white images\nhave a max absolute channel-wise difference of 10 while this\ndoes not hold true for color images as the red, green and blue\nchannels encode different intensities.\nAnother motivation for this choise was the fact\nthat the weekday black and white comic strips tend to follow\na consistent and reliable four panel structure that allows us to\neasily extract panels of the same size.\nThe weekend colored\ncomic strips tend to be more artistic and do not follow the\nsame sizes for each panel and thus pose a lot of variation in\nthe dataset.\nDue to variation in num-\nber of pages between the volumes, we ended up with 11,033\nblack and white panels of the same size.\nWe explored the idea of using the multi-modal vision-\nlanguage framework BLIP2 [1], capable of answering ques-\ntions based on any input image, to generate captions for our\ninput panels.\nHowever, the generated captions were unsatis-\nAnother approach we tried was to use the multi-modal\nGPT-4 [2] that can accept image and text inputs to generate\nFig.\nAlthough this is not an ideal choice, we expected the\ndiffusion model to generalize sufficiently owing to the large\ndataset size.\nMETHODOLOGY\nAs described in Section 1, we use a denoising diffusion\nprobabilistic model [3] to perform a style transfer operation.\nTo fine-tune this large model effi-\nciently, we use a training technique developed by Microsoft\nResearch called LoRA [5] which stands for Low Rank Adap-\ntation.\nMore about the network and the training procedure in\nthe following two subsections.\nNetwork\nThe Stable Diffusion Model v1.5 consists of many pieces.\nLatent diffusion model [4] runs the diffusion process in the\nlatent space instead of pixel space, making training cost lower\nand inference speed faster.\nIt is motivated by the observation\nthat most bits of an image contribute to perceptual details and\nthe semantic and conceptual composition still remains after\naggressive compression.\nThe de-\nsign is equivalent to fuse representation of different modality\ninto the model with cross-attention mechanism.\nThus, it can efficiently learn\nvisual concepts, in the form of text, via natural language su-\npervision and perform zero-shot classification (Figure 4)\nIn the pre-training stage, the image and text encoders are\ntrained to predict which images are paired with which texts\nin a dataset of 400M image-caption pairs.\nCLIP is trained to\nmaximize the cosine similarity of the image and text embed-\ndings of image-caption pairs via a multi-modal embedding\nWe use a training method called Low Rank Adaptation\n(LoRA) [5] to fine-tune the diffusion model.\nIn this technique, weight matrices in existing\nlayers, typically attention layers, are fine-tuned to a specific\ndataset by adding update matrices.\nDuring the fine-tuning process, the original weights are\nfrozen and the weights in the update matrices are learnt.\nThe\n\fLoRA framework is flexible as we have control over the rank\nof the matrices in the decomposition of the update matrix.\nIn our experiments,\nwe set k = 4 and we use one update matrix for each attention\nlayer in the UNet.\nFirst, this\napproach uses far less memory than traditional fine-tuning ap-\nproaches since we only need to compute the gradients for the\nrelatively smaller weight matrices.\nFor our custom Calvin and Hobbes dataset, we fine tune\nthe model on 11,000 input images paired with the synthetic\ntext caption CNH3000 as mentioned earlier.\nAt each step, a\nrandom image from the dataset and a random denoising time\nstep for the DDPM are sampled.\nThough the network does not train on all available input im-\nages at each time steps, it is able to generalize the denoising\nprocess with a significantly smaller subset.\nA little girl with a balloon walking down a street with\nbuildings in the background in the style of Calvin and\nAfter fine tuning stable diffusion on Calvin and Hobbes\nimages, we generated some more images using the same\nthat we replaced Calvin and Hobbes with our keyword,\nCNH3000.\nFor\nimage to image models, the denoising process starts with an\nimage with some noise added to it.\nThe noised im-\nage with some prompts can allow us to style transfer an im-\nFor all these image generation, we used similar prompts as\n4.1 with the keyword, CNH3000.\nFinally, one of the natural questions in the age of Large\nLanguage Models (LLMs) is whether we can use a model to\ngenerate entire comic strips in the style of Calvin and Hobbes.\nCONTRIBUTION\nWe experimented with starting with noisy sample of edge\nmaps of images instead of using the original image.\nSince\nthese comics aim to convey a short story within 4 panels, they\nare often filled with text.\nThis is\nunideal and does not align with a regular person\u00e2\u20ac\u2122s expectation\nof the comic\u00e2\u20ac\u2122s style.\nRemoving the text using available open-\nsource OCR tools like pytesseract is a good starting point.\nAlternatively, we could generate better captions for the im-\nages use tools like GPT-4 or Flamingo [11] to incorporate the\ntext from the images into the captions.\nThis could aid the dif-\nfusion model in understanding the style of the comics better.\nTowards improving the dataset, it would also be interesting\nto include color images and have the model jointly learn the\nstyle from black and white as well as colored panels simulta-\nThinking about the training mechanisms, we explored\nLoRA as described in Section 3.3.\nIt would be interesting\nto explore ideas such as DreamBooth [12], Textual Inversion\n[13] and ControlNet [14] since they have shown promising\nresults for other image-based tasks.\nWe briefly experimented with applying our style transfer\nmodel on videos as described in Section 4.4.\nIf we had to assign credit to specific authors,\nwe would say that Sundar was heavily responsible for dataset\ncreation and pre-processing, Asvin was responsible for the\nfine-tuning of diffusion models and Sloke was responsible for\nrunning extensive experiments with the fine-tuned model.\nWe would also like to thank Prof. Alan C. Bovik for his in-\nsights and guidance in the development of this project.\n"}, {"role": "assistant", "content": "ABSTRACT\n\nThis project report summarizes our journey to perform sta-\nble diffusion fine-tuning on a dataset containing Calvin and\nHobbes comics. The purpose is to convert any given input\nimage into the comic style of Calvin and Hobbes, essentially\nperforming style transfer. We train stable-diffusion-v1.5 us-\ning Low Rank Adaptation (LoRA) to efficiently speed up the\nfine-tuning process. The diffusion itself is handled by a Vari-\national Autoencoder (VAE), which is a U-net. Our results\nwere visually appealing for the amount of training time and\nthe quality of input data that went into training.\n\nIndex Terms\u00e2\u20ac\u201d Diffusion, Style Transfer, Low Rank\n\nAdaptation\n\n1. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nIntroduction\nLanguage models\ntransformer-based architectures (Brown et al.,\n2020; Chowdhery et al., 2023; OpenAI, 2023) can\ngenerate coherent and contextually relevant texts\nfor various use cases.\nDespite their impressive\nperformance, these models occasionally produce\nerroneous or overconfident outputs, leading to\nconcerns about their calibration (Dawid, 1982;\nDeGroot and Fienberg, 1983) which measures how\nfaithful a model\u00e2\u20ac\u2122s prediction uncertainty is.\nSuch a\nproblem is pressing as users adapt them using a\nrecent paradigm called in-context learning (Brown\net al., 2020) to construct performant predictors,\nin safety-critical\nespecially for applications\ndomains (Bhatt et al., 2021; Kadavath et al., 2022;\nPan et al., 2023).\nWe provide an in-depth evaluation and analysis\nof how well these models are calibrated - that is,\nthe alignment between the model\u00e2\u20ac\u2122s confidence in\nits predictions and the actual correctness of those\npredictions.\nThis token-level calibration assess-\nment will enable us to measure the discrepancy\nbetween the model\u00e2\u20ac\u2122s perceived and actual perfor-\nmance through a Bayesian uncertainty lens, pro-\nviding a valuable metric for assessing the model\u00e2\u20ac\u2122s\naccuracy and reliability.\nWe find that LMs including GPT-2 (Radford\net al., 2019) and LLaMA (Touvron et al., 2023a)\nare poorly calibrated and there exists a calibration-\naccuracy trade-off (Fig.1), i.e.\nCru-\ncially, this calibration degradation worsens as the\nmodel size increases or when fine-tuning occurs\nusing specialized data, such as curated instructions\n(Dubois et al., 2023), dialogues (Zheng et al., 2023),\nor human preference data (Ziegler et al., 2019).\nThough previous work (Braverman et al., 2020)\nshows the entropy of each generation step is drift-\ning and can be recalibrated via scaling techniques\n(Platt et al., 1999) such as temperature scaling (Guo\net al., 2017), we show that the miscalibration issue\nin ICL can not be easily addressed using such well-\nestablished recalibration approaches that rely on\nadditional validation data.\nMoreover, we study the trade-off in reasoning\ntasks that involve the generation of explanations\n(Camburu et al., 2018; Nye et al., 2021; Wei et al.,\n2022) before the answer, showing that the model\ncan produce confidently wrong answers (using\nconfidence histograms and reliability plots) when\nprompted with explanations on Strategy QA (Geva\net al., 2021), Commonsense QA (Talmor et al.,\n2018), OpenBook QA (Mihaylov et al., 2018),\nWorld Tree (Jansen et al., 2018).\nWe carefully\ndesign our human assessment to observe that, with\nthe increase in model sizes and the quantity of ICL\nexamples, there is a corresponding rise in the pro-\n\f(a) Demonstration of In-context Learning\n(b) The accuracy and calibration of LLaMA-30B\nportion of confidently predicted examples among\nthose incorrectly forecasted.\nIn-context learning has been expected to learn\nmodels by gradient descent in their forward pass\n(Von Oswald et al., 2023), which might hopefully\nyield calibrated predictions (B\u00c5\u201aasiok et al., 2023)\nif the models are getting close to local optimality\nwith respect to test loss through meta-optimization.\nWe design controlled ex-\nperiments to illustrate task learning properties of\nICL, showing that when examples in the prompt\ndemonstrate consistent task properties, the learning\nperformance, and calibration would be improved.\nUncertainty\nquantification in NLP, which often adopts the\nBayesian principle to sophisticated methods tai-\nlored for neural networks, aims to enhance the re-\nliability of model predictions.\nThis may involve\nnon-trivial designs as directly interpreting language\nmodel predictions via probabilities (Kadavath et al.,\n2022) and linguistic expressions (Lin et al., 2022;\nMielke et al., 2022; Zhou et al., 2023) may in-\nadvertently lead to over-reliance on the model\u00e2\u20ac\u2122s\nuncertainties (Si et al., 2023), thus complicating\nthe establishment of trustworthy common ground\nbetween humans and models (Bu\u00c3\u00a7inca et al., 2021).\nCalibration is a safety\nproperty to measure the faithfulness of machine\nlearning models\u00e2\u20ac\u2122 uncertainty, especially for error-\nprone tasks using LLMs.\n(Braverman et al., 2020)\nassesses the long-term dependencies in a language\nmodel\u00e2\u20ac\u2122s generations compared to those of the un-\nderlying language and finds that entropy drifts as\nmodels such as GPT-2 generate text.\nThe intricacy\nof explanations on complementary team perfor-\nmance poses additional challenges due to the over-\nreliance on explanations of users regardless of their\ncorrectness (Bansal et al., 2021).\n(Mielke et al.,\n2022) gives a framework for linguistic calibra-\ntion, a concept that emphasizes the alignment of a\nmodel\u00e2\u20ac\u2122s expressed confidence or doubt with the ac-\ntual accuracy of its responses.\nThe process involves\nannotating generations with <DK>, <LO>, <HI>\nfor confidence levels, then training the confidence-\ncontrolled model by appending the control token\n<DK/LO/HI> at the start of the output, followed\nby training a calibrator to predict these confidence\nlevels, and finally predicting confidence when gen-\nerating new examples.\n(Shih et al., 2023) proposes a simple amor-\nLanguage Model:Books were sent to each other by students.\nTo enhance\nthe estimation of uncertainty in language models,\n(Kuhn et al., 2023) developed a method that aggre-\ngates log probabilities across semantically equiva-\nlent outputs.\nThis approach utilizes bidirectional\nentailment through a model to identify outputs that\nare semantically similar, thereby refining the un-\ncertainty estimation process.\n(Cole et al., 2023)\nidentifies the calibration challenge in ambiguous\nQA and distinguishes uncertainty about the answer\n(epistemic uncertainty) from uncertainty about the\nmeaning of the question (denotational uncertainty),\nproposing sampling and self-verification methods.\n(Kalai and\nVempala, 2023) shows the trade-off between cal-\nibration and hallucination but they didn\u00e2\u20ac\u2122t study it\nin an ICL setting and how the predicted answer\u00e2\u20ac\u2122s\naccuracy would impact those two safety aspects.\nfor binary classification tasks, we filter\nand extract probabilities P (\u00e2\u20ac\u0153Yes) and P (\u00e2\u20ac\u0153No),\nbased on which we calculate the following statistics\nfor studying the confidence and calibration of LMs:\nConfidence and feature norm.\nNote that auto-regressive LLMs are\ntrained via maximizing the negative log-likelihood\nobjective L = \u00e2\u02c6\u2019Et[log P\u00ce\u00b8 (wt|w<t)] on massive\nEmpirical estimate of the expected calibration\nerror (ECE) In the realm of probabilistic classi-\nfiers, calibration is a crucial concept.\nA classifier,\ndenoted as P\u00ce\u00b8 with parameters \u00ce\u00b8 and operating over\nC classes, is said to be \"canonically calibrated\"\nwhen, for every probability distribution p over the\nC classes and for every label y, the probability that\nthe label is y given the classifier\u00e2\u20ac\u2122s prediction is p\nmatches the component of p corresponding to y.\nThe total number of samples\nis represented by n, and the dataset consists of n\nTable 1: Accuracy and Calibration of LLaMA-30B model with three sizes across four text classification datasets\nand four reasoning datasets\nText Classification\nStrategy QA\nCommonsense QA 0.354\nOpenBook QA\nReasoning with Scratchpad\nindependent and identically distributed samples,\n{(xi, yi)}n\ni=1.\nNotably, the reasoning task\nperformance can be greatly improved in general\nvia prompting methods like scratchpad (Nye et al.,\n2021; Wei et al., 2022) that enables models to gen-\nerate natural language explanations before predict-\ning an answer.\nWe experi-\nment with three strategies in applying temperature\nscaling methods (Guo et al., 2017) to fix miscali-\n1.\nWe learn one temperature for each n-shot ICL,\ni.e., we learn different temperatures for differ-\nent shot numbers in ICL;\n2.\n6, none of the above strate-\ngies achieves satisfactory calibration performance,\nwhich is in contrast to the well-studied super-\nvised learning setting where scaling the confidence\nscores (via temperature scaling) can effectively re-\nduce calibration errors (Guo et al., 2017).\nThe fact\nthat applying a post-processing calibration method,\nsuch as temperature scaling, as used in most previ-\nous work, cannot directly resolve the miscalibration\nissue suggests that ICL might have substantially\ndifferent properties compared to making predic-\ntions via classical supervised learning models, thus\nfuture investigations are needed to address such\nmiscalibration issues.\nWe show that vicuna\nand alpaca are both more accurate but less cali-\nbrated than their LLaMA counterpart backbones,\nthe margin is especially large for reasoning tasks\nand vicuna.\nOur results provide evidence that though fine-\ntuned on carefully curated datasets can greatly im-\nprove question-answering performance, especially\nfor hard tasks like reasoning problems, attention\nmay need to be paid when assessing the calibration\nof those models\u00e2\u20ac\u2122 predictions.\nIn our study\ninvestigating the impact of various prompt strate-\ngies, we employ three distinct approaches: Repeat-\ncontext: In this strategy, we construct the prompt\nas w0 = [x1, x1, ..., x1, y1], where we repetitively\ninclude only the context x1 a total of n times,\nexcluding the label y1 from repetition.\nThe results presented in Ta-\nble 3 unveil essential insights: (1) The inclusion of\nlabels within the prompt contributes to a reduction\nin uncertainty and facilitates more effective rea-\nsoning.\nA confidence histogram, on the other hand,\ndisplays the distribution of the model\u00e2\u20ac\u2122s prediction\nconfidences across all classes, showing how often\nthe model predicts certain probabilities.\n4.4 Ablation Studies\nFor case studies, we research how miscalibration\ncan impact the selective classification of LLMs,\nwhere models are supposed to abstain from uncer-\n\fTable 2: Norm of representation, entropy, and confidence of LLaMA-30B model across three text classification\n(a) Classification Accuracy\n(b) Calibration error\ntain predictions in high-stakes settings.\nAs a result, the entropy decreases\nand ECE increases, showing that token-level cali-\nbration might have an inverse scaling relationship\nwith model sizes.\nTo better understand the miscalibration is-\nsue of ICL, we conduct fined-grained experiments\nto take a closer look at ICL properties: we measure\nthe norm of the representation vectors2 for differ-\nent number of shots in ICL, to better understand\nhow the representation vectors are changing when\nincreasing the number of shots in ICL.\nMeanwhile,\nwe also measure the confidence and entropy of the\nprediction for yn, and the results are summarized\nin Table 2.\nMean-\nwhile, more ICL samples lead to smaller entropy\nand higher confidence in most cases.\nples of LLaMA and plot the distribution of wrongly\npredicted samples and the confidence scores via\nthresholding.\nSimilar to previous observations, as\nmodel sizes and the number of ICL examples scale\nup, LMs would generate more confident samples\n(Fig.\nWe also observe that most of the wrong\npredictions are highly confident.\nWe manually ex-\namine the correctness of explanations on common-\nsense QA, and found its high correlations with pre-\ndicted answer accuracy, which is the opposite of\ntoken-level explainability that tends to get worse\nwhen the accuracy improves.\nAs LMs continue to\nevolve and gain more capabilities such as having\nlong enough context windows that can include the\nwhole training set as in-context examples for some\ndownstream tasks, our result can be pedagogical\nwhen users would like to examine their uncertainty\nthrough prediction probabilities.\nIn this work, we observe that ex-\nisting scaling recalibration methods cannot fully re-\nsolve the miscalibration issues of ICL, so better un-\nderstanding and mitigation strategies are needed.\nOur\nfindings indicate that in multi-choice or multi-class\nclassification tasks, even though the calibration of\nanswer tokens may deteriorate in high-performance\nsettings, there may be a positive correlation be-\ntween accuracy and the correctness of explanations\nin reasoning tasks.\n"}, {"role": "assistant", "content": "Abstract\n\nModern auto-regressive language models are\ntrained to minimize log loss on broad data by\npredicting the next token so they are expected\nto get calibrated answers when framing a prob-\nlem as next-token prediction task. We study\nthis for in-context learning (ICL), a widely\nused way to adapt frozen large language mod-\nels (LLMs) via crafting prompts, and investi-\ngate the trade-offs between performance and\ncalibration on a wide range of natural language\nunderstanding and reasoning tasks. We conduct\nextensive experiments to show that such trade-\noffs may get worse as we increase model size,\nincorporate more ICL examples, and fine-tune\nmodels using instruction, dialog, or reinforce-\nment learning from human feedback (RLHF)\non carefully curated datasets. Furthermore, we\nfind that common recalibration techniques that\nare widely effective such as temperature scal-\ning provide limited gains in calibration errors,\nsuggesting that new methods may be required\nfor settings where models are expected to be\nreliable.\n\n1\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nAs an example, the state-of-the-art language model\nMegatron-Turing NLG with 530 billion parameters achieves\nan accuracy score of 87.15% in the task of LAMBADA next\nword prediction [3].\nOwing to the discrepancies of model and device config-\nurations, device placement poses a huge solution search space\nwhen the number of operators or devices increases.\nUnfortunately, learning-\nbased approaches requires a training process that takes several\nhours or even several days to deliver a placement solution\nfor a single DNN [9].\nThe placement solution has to be searched all over again, if\nthe DNN is deployed to a different cluster [11].\nUpon analyzing and experimenting the released imple-\nmentations of existing algorithmic approaches, we observed\nthat the above methods inherently suffer from the following\ndrawbacks: (1) Solution optimality: Given model and device\nconfigurations, heuristic-based algorithms quickly yield the\nplacement plan.\nHowever, the placement result is sub-optimal,\nleaving ample room for reducing the makespan.\nNevertheless, current algorithmic solutions fail to leverage\nruntime optimization of a computation graph when coarsening\nthe graph to reduce the solution search space.\n(3) Device\nheterogeneity & Constraints: Studies revolving around ex-\nact algorithms either attempt to produce near-optimal device\nplacement decisions on a small number of homogeneous\ndevices [4], [12] or do not sufficiently consider the device\ncomputation and communication constraints.\nTo address the aforementioned limitations, we propose\nMOIRAI, an algorithmic solution that considers DNN graph\noptimization and caters an optimal solution over heterogeneous\ndevices.\nUnlike the aforesaid\napproaches, in which the device heterogeneity is ignored, we\ncraft the MILP to outline computational differences, memory\nconstraints, and communication capability of the devices.\nMOIRAI outperforms the heuristic-based solution up\nto 1.87\u00c3\u2014 in placement optimality in terms of the reduction\nof the makespan.\nCompared to the learning-based solution\ncounterparts, MOIRAI reduces the solution generation time\nfrom hours to minutes while reducing the makespan up to\n4.28\u00c3\u2014.\n1 depicts the taxonomy of distributed deep learning\nconcepts.\n[6] propose to search the placement policy\nfor a DNN with reinforcement learning method.\nIt adjusts the scheduling\nstrategy towards the objective of reducing the per-step training\ntime of a DNN until convergence, which requires 17-27 hours\nof 80 to 160 4-GPU servers to produce the near-optimal place-\nment.\nIn the face of the cues that all previous methods\ncan only generate a near-optimal placement for a DNN over\na set of computing devices, Placeto [9] encodes computation\ngraphs with graph embeddings and offers a policy network to\ndisentangle the placement of a family of computation graphs\nwith reinforcement learning.\nInspired by\nthe traditional parallel job scheduling, Baechi [11] utilizes\nthree heuristics to portray the task precedence, communication\noverhead, and memory constraints of the placement policy,\nwhich generally requires less than 3 minutes to find a place-\nment strategy.\nGETF [33] represents the DNN as a DAG and\nextends the conventional Earliest Time First (ETF) algorithm\nto incorporate related machines.\nSecondly, the memory capacity of each\ndevice is non-uniform, suggesting that the number of DNN\noperators that can be hosted by each device varies.\nNetwork attachments may reside at or\nabove the data link layer and can have various types of\ninterfaces, such as WiFi, Bluetooth, or even application defined\ninterfaces.\nB. Graph Coarsening\nWe profile operator processing time of four widely em-\nployed DNNs on four devices and show their processing time\ndistribution in Fig.\nWe note that modern DNNs typically\nconsist of a number of operators with short computation\ntime, increasing the difficulty to address the device placement\nproblem.\nThe memory access latency is often orders of mag-\nnitude greater than the computation time, and thus operator\nfusion extensively speeds up the DNN inference.\nA fusion rule contains a sequence of\nordered operator types, each of which is a string.\nTypically, the connections of DNN operators can be\ncategorized into three types in a computation graph, which are\nillustrated in Fig.\nAlthough the first add, relu vertex\npair is certified to conforming partial r3 rule via the function\nis sub rule(), the operator connection denoted by the edge\nbetween add and relu vertex is essentially part of the multi-\noutput connection of the operator add, which is examined by\nfunction is valid conn().\nThe function then moves forward\nand binds the other two add, relu vertex pair on the upper\nbranch, generating two new vertices with the bound tag and\nthe add \u00e2\u2014\u00a6 relu type.\nNext,\nGCOF() merges the operator conv2 \u00e2\u2014\u00a6 bn and operator add \u00e2\u2014\u00a6\nrelu at the end of the upper branch in accordance with rule\nr3 and multi-inputs connection.\nC. Input Profiling\nOur method takes compute time of each operator, transmis-\nsion time of data flow, precedence relation among operators,\nand configurations of devices as its inputs.\nThe operator\ndependency is manifested by the computation graph and\nthe configurations of devices can be acquired by querying\noperating system interfaces, whereas the operator processing\nand the data transmission time requires proper analysis.\nScrutinizing the existing research on\nmeasuring DNN operator processing time, we notice that\nthere are fundamentally three commonly employed methods\nto profile the operator processing time, that is manual testing,\noperational intensity, and prediction model.\nThough manual\ntesting reveals the actual operator execution time, it is labor\nintensive to approach the data.\nD. Problem Modeling\nUp to this point, we have presented how to obtain a coars-\nened graph to reduce the solution search space and prepared\nthe necessary inputs for our algorithm.\nSubsequently, the weight of\nthe link is directly applied to the new node corresponding to\nit.\nPresented with the DNN computation graph termed\nas G = (N, L), the device placement for operators in the DNN\nis achieved by solving the following MILP.\nThe data flow\nof a DNN, defined by the input and output of its operators,\nnaturally establishes the execution precedence relationships\nof the operators.\nWe ensure that each operator is\nassigned to only one device by equation (4c).\nConventionally,\nthe cumulative\nmemory footprint of the operators allocated to a device should\nnot surpasses the memory size of the device, known as\nmemory constraints.\nFor each device,\nwe obtain the memory footprint of each operator through the\nAPIs (e.g., torch.profiler) and constrain the total memory\nof the operators placed on a device not to exceed the memory\ncapacity of the device.\nNaturally, when two adja-\ncent operators are placed on two distinct devices, a commu-\nnication overhead is incurred by the data flow between the\noperators.\nLastly, we address the contention\nof data transmission, when there are multiple outputs waiting\nto be transferred on the same communication channel.\nThe result\nshows that MOIRAI reduces the end-to-end inference latency\nup to 2.98\u00c3\u2014, 1.77\u00c3\u2014, 1.33\u00c3\u2014 compared to Placeto, m-SCT, and\nGETF respectively.\nNext, we are interested in evaluating the impact of the\ngraph coarsening method of MOIRAI on reducing the end-\nto-end inference latency.\nIn the inter-server\nsetting, MOIRAI outperforms Placeto, m-SCT, and GETF\nup to 3.15\u00c3\u2014, 1.9\u00c3\u2014, and 1.25\u00c3\u2014 respectively in reduction of\nthe end-to-end latency.\nIn the intra-server setting, MOIRAI\nsurpasses Placeto, m-SCT, and GETF up to 4.28\u00c3\u2014, 1.74\u00c3\u2014,\n1.34\u00c3\u2014 respectively in reduction of the end-to-end latency.\nConsidering the vast search space and limitations of the Gurobi\noptimizer, both GETF and our algorithm need minutes to\ngenerate placement.\nHowever, since this process is offline, and\nthe optimal placement solutions of MOIRAI are superior to\nthose of the m-SCT, we believe that our approach still holds an\nadvantage in the placement generation.\n10, compared to using the\noriginal computation graph to generate placement results, the\ngraph coarsening method has reduced the end-to-end inference\nlatency by up to 5.7% in the inter-server setting and up to 3.8%\nin the intra-server setting.\nFrom TABLE V, we observe that the\ngraph coarsening method has a significant effect on reducing\nthe placement generation time, with an average time reduction\nto 71.87% of the placement generation time with the original\ncomputation graphs.\nInterestingly, a point that\nattracts our attention is: we observe that as DNN models be-\ncome larger, although the computation resource demand rises,\nlarge models provide greater search space, which increases\nthe parallelism of the model, making full use of computation\nresources, and achieving a better inference acceleration with\n"}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dThe escalating size of Deep Neural Networks (DNNs)\nhas spurred a growing research interest in hosting and serving\nDNN models across multiple devices. A number of studies\nhave been reported to partition a DNN model across devices,\nproviding device placement solutions. The methods appeared\nin the literature, however, either suffer from poor placement\nperformance due to the exponential search space or miss an\noptimal placement as a consequence of the reduced search\nspace with limited heuristics. Moreover, these methods have\nignored the runtime inter-operator optimization of a computation\ngraph when coarsening the graph, which degrades the end-to-\nend inference performance. This paper presents MOIRAI that\nbetter exploits runtime inter-operator fusion in a model to\nrender a coarsened computation graph, reducing the search\nspace while maintaining the inter-operator optimization provided\nby inference backends. MOIRAI also generalizes the device\nplacement algorithm from multiple perspectives by considering\ninference constraints and device heterogeneity. Extensive experi-\nmental evaluation with 11 large DNNs demonstrates that MOIRAI\noutperforms the state-of-the-art counterparts, i.e., Placeto, m-\nSCT, and GETF, up to 4.28\u00c3\u2014 in reduction of the end-to-end\ninference latency. MOIRAI code is anonymously released at\nhttps://github.com/moirai-placement/moirai.\n\nIndex Terms\u00e2\u20ac\u201dModel parallelism, device placement, operator\n\nfusion, mixed integer linear programming.\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nTo the best of\nour knowledge, how to incorporate information from clus-\ntering into the learning process of label prediction has been\nlittle explored in academia.\nFor instance, FaceNet (Schroff, Kalenichenko, and\nPhilbin 2015) solves this by adopting the triplet metric loss\nCopyright \u00c2\u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org).\n2016) jointly optimizes the\nclassification task by minimizing the Softmax loss and opti-\nmizing the clustering task by reducing the distance between\nthe feature and its class center.\n2021) points out that the\nmagnitude of the face feature can be regarded as the indica-\ntor of face quality and can be used for face quality cluster-\ning.\nFurthermore, face quality can guide the label prediction\nby adaptively adjusting the margin in the label classification\nloss.\nHowever, face quality is not the only influence factor on\nface recognition in real life scenarios and the hard samples\nare mainly due to large variations in pose, age, and occlu-\nsion.\nAnd these faces with large variations are usually harder\nto cluster, hence we should also pay attention to these sam-\nples with large variations instead of merely considering face\nRecently, contrastive learning has been employed in the\nclustering task and performs well on various benchmarks (Li\net al.\nIn this work, we try to explore the prob-\nlem through the joint optimization task of face label pre-\ndiction and supervised contrastive face clustering.\nFrom the geometric perspective, faces tend to\nbe more easily pulled to the cluster center and more eas-\nily classified if the face cluster has a larger concentration.\nThus, in the first aspect, we extend the ArcFace loss func-\ntion with a cluster-guided angular margin to adaptively tune\nthe classification decision boundary according to the hard\nlevel of clustering, and the loss is named as Cluster-Guided\nArcFace(CG-ArcFace).\nThe optimization of clus-\ntering helps the feature extractor to learn the prototype of\neach class at the feature level explicitly, which helps to im-\nprove the robustness of the feature extractor to hard samples\nwith large variations in pose, age, occlusion, etc.\nCompared\nwith the MagFace, we add an explicit clustering learning\nprocess to pull faces with different variations towards the\ncluster center.\n\u00e2\u20ac\u00a2 We further propose to jointly optimize both clustering\nand recognition tasks via a supervised contrastive face\nclustering approach and an additional cluster-aligning\n\u00e2\u20ac\u00a2 Extensive qualitative and quantitative results on popu-\nlar facial benchmarks prove the effectiveness of our ap-\nproach and the superiority over the existing methods to\nface recognition.\nThey help pull face instances to-\nwards the learnable class center with the same identity, re-\nsulting in a discriminative face representation.\nSince a fixed\nmargin in the margin-based loss leads the network to treat\neach sample equally without considering their importance\ndegree, mining-based strategies are adopted to pay more at-\ntention to hard samples.\nThey emphasize hard samples by adopting a\npreset constant (MV-Softmax) or an adaptive variable (Cur-\nricularFace) as the weights of negative cosine similarities.\n2021) proposes a structure-\npreserved sampling strategy to train the edge classification\nIn our method, we try to cluster the faces in a supervised\ncontrastive learning way, motivated by the works (Khosla\net al.\n2020), the concentration\nis measured by the Equation 1, where a smaller \u00cf\u2022 indicates\nlarger concentration.\nThen we sample\nM classes among all the K classes, and the sampled cluster\ncenters and cluster concentrations are denoted as \u00ce\u00a6 and C re-\nspectively.\nBy defining the\nangle \u00ce\u00b8j between fi and j-th learnable class center Wj \u00e2\u02c6\u02c6 Rd\nj fi = \u00e2\u02c6\u00a5Wj\u00e2\u02c6\u00a5\u00e2\u02c6\u00a5fi\u00e2\u02c6\u00a5cos\u00ce\u00b8j, the objective of ArcFace can\nbe formulated as Equation 2:\nLArcF ace = \u00e2\u02c6\u2019\nes\u00c2\u00b7cos(\u00ce\u00b8yi +m)\nes\u00c2\u00b7cos(\u00ce\u00b8yi +m) + (cid:80)\nwhere m > 0 denotes the additive angular margin and s\nis the scaling parameter.\n2021) that utilizes the face quality to ad-\njust the margin, we directly explore the clustering structure\nknowledge and adjust the uniform margin based on the clus-\nter concentration.\nThen \u00ce\u00bb(\u00cf\u2022yi) is defined as Equa-\n\u00ce\u00bb(\u00cf\u2022yi ) =\n\u00cf\u2022yi \u00e2\u02c6\u2019 \u00cf\u2022min\n\u00cf\u2022max \u00e2\u02c6\u2019 \u00cf\u2022min\nSupervised Contrastive Clustering\nExcept for adjusting the classification margin based on the\ncluster concentration, which is a manner of joint learning\nof classification and clustering, we furthermore propose to\njointly optimize both tasks via a supervised contrastive clus-\ntering process and an additional clustering-aligning process.\nGiven an instance feature, we\n\fcan obtain the positive cluster center according to its label\nand obtain some other cluster centers as negative samples.\nWe regard it as a supervised contrastive manner because we\nextend the traditional self-supervised batch contrastive ap-\nproach to the fully-supervised setting with reference to Sup-\nCon (Khosla et al.\nSup-\nCon first proposes to conduct contrastive learning in a su-\npervised way, and the positive sample is selected from the\nimages with the same class label.\nOur work inherits the idea\nof supervised learning from SupCon, and the main differ-\nence is that we choose the cluster centers rather than images\nto form positive and negative samples and adopt a class-\nadaptive temperature for face clustering.\nPCL proposes to\nutilize class prototypes to bridge contrastive learning and\nclustering while the class prototypes in their work are gen-\nerated using the Expectation-Maximization(EM)-based al-\ngorithm, which is also an unsupervised learning way.\nThe\nmain difference between our work and PCL is that our clus-\nter centers are generated in a supervised manner and make\nfull usage of the face labels.\nLClu\u00e2\u02c6\u2019Ali will help\nalign the cluster center Cyi and the corresponding learn-\nable class center Wyi, and this may promote the classi-\nfier optimization using the clustering results.\nThe mo-\nmentum mc for updating the Cluster-Center Bank is set to\n0.9.\nSince our proposed classification loss, the\nCluster-Guided ArcFace, is based on the original ArcFace\nloss, we select the model with ArcFace classification head\nand ResNet-100 backbone as the baseline model.\nPerformance Improvement of Individual Losses Firstly,\nwe study the effect of each loss function on the baseline sep-\nsetting1: We replace the ArcFace loss with the proposed\nCluster-Guided ArcFace LCG\u00e2\u02c6\u2019ArcF ace to utilize the clus-\nter concentration of each class to adjust the decision margin\nadaptively.\nThe setting1 model significantly outperforms the\nbaseline model on all the benchmarks except for the LFW\nbenchmark, where the performance is almost saturated.\nEs-\npecially, the setting1 model has a significant improvement\non the benchmarks with hard face samples, including CFP-\nFP and CPLFW with large pose variations, AgeDB and\nCALFW with age variations, IJB-B(C) containing images\nand frames from videos, and the MegaFace containing mas-\nsive samples and a high degree of variability in scale, pose\nand occlusion.\nThe comparisons between the setting1 and the base-\nline have proved the correctness of our assumption and the\nvalidity of the Cluster-Guided ArcFace.\nsetting2: We add the supervised Cluster Contrastive loss\nLClu\u00e2\u02c6\u2019Con to the baseline model to verify the effect of the\njoint optimization of both clustering and recognition task.\nJoint\nlearning of class prototypes helps to improve the robustness\nof feature extraction of hard samples with large variations in\npose, age, scale, occlusion, etc.\nThe Cluster-Aligning loss can enhance the perfor-\nmance because the alignment between learnable class cen-\nters in the classifier and cluster centers helps the learning\nof the classifier to be robust to the variations of faces.\nThe comparisons between the settings (setting4, setting5)\nand setting1 demonstrate that both the supervised Cluster\nContrastive loss and the Cluster-Aligning loss can further\npromote the performance of the Cluster-Guided ArcFace.\nFurthermore, the comparisons between setting6 and the set-\ntings(setting4, setting5) prove that the supervised Cluster\nContrastive loss and the Cluster-Aligning loss can promote\neach other.This is because the cluster aligning procedure\nhelps to constrain the consistency of the cluster centers and\nthe learnable centers, which should be consistent in theory.\nComparisons with the Triplet Loss and Center Loss\nThe supervised Cluster Contrastive loss has shown con-\nsistency in improving the performance of face recogni-\ntion via clustering the face features iteratively.\nIn compar-\nison, some metric losses, such as the Triplet loss (Schroff,\nKalenichenko, and Philbin 2015) and the Center loss (Wen\net al.\nFor a fair comparison, we only replace the supervised\nCluster Contrastive loss with the Center Loss and the Triplet\nloss and still utilize the Cluster-Guided ArcFace loss and\nthe Cluster-Aligning loss.\nThe results in Table 3 show that\nthe proposed supervised Cluster Contrastive loss performs\nbetter than the Center Loss and the Triplet Loss in the\nfive benchmarks, especially in the ones with more diffi-\ncult samples(CFP-FP, AgeDB, IJB-B and IJB-C).\nWe compare it\nwith the typical learnable temperature \u00cf\u201e0, widely adopted in\nthe infoNCE (van den Oord, Li, and Vinyals 2018) loss, on\nvarious benchmarks.\nThe vertical dashed line indicates the expec-\ntation of cosine similarity distribution.\nFigure 2(a) and Figure 2(b), the margin between positive\npairs and negative pairs on the CFP-FP dataset is enlarged\nfrom 0.463 to 0.524, which shows that our paradigm can im-\nprove the face representation of the samples with large pose\nvariations.\nSimilarly, the margin between positive pairs and\nnegative pairs on the AgeDB dataset is enlarged from 0.521\nto 0.547, shown in Figure 2(c) and Figure 2(d).\nWe utilize three clustering methods for evaluation:\nK-means (Lloyd 1982), DBSCAN (Ester et al.\nFollowing the IJB-B clustering\nprotocol, we evaluate on three largest sub-tasks where the\nnumbers of identities are 512, 1024, and 1845.\n"}, {"role": "assistant", "content": "Abstract\n\nFace clustering tasks can learn hierarchical semantic infor-\nmation from large-scale data, which has the potential to help\nfacilitate face recognition. However, there are few works on\nthis problem. This paper explores it by proposing a joint op-\ntimization task of label classification and supervised con-\ntrastive clustering to introduce the cluster knowledge to the\ntraditional face recognition task in two ways. We first extend\nArcFace with a cluster-guided angular margin to adjust the\nwithin-class feature distribution according to the hard level\nof face clustering. Secondly, we propose a supervised con-\ntrastive clustering approach to pull the features to the cluster\ncenter and propose the cluster-aligning procedure to align the\ncluster center and the learnable class center in the classifier\nfor joint training. Finally, extensive qualitative and quantita-\ntive experiments on popular facial benchmarks demonstrate\nthe effectiveness of our paradigm and its superiority over the\nexisting approaches to face recognition.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nIn many cases these representations are learned\nfrom user co-engagement data [19, 24, 28, 29]: if two entities are\nfrequently co-engaged by users, they should have similar embed-\ndings.\nA well-known drawback of leveraging user collaborative\nsignals in isolation to train entity embeddings is that these data\nsuffer popularity biases and have low coverage for new and unpop-\nular items [21, 42].\nAt the same time, there may exist rich semantic\ninformation about items (such as genre, content maturity level,\nintellectual property, and storyline).\nHowever, there are three unique challenges in training on this\nparticular KG within Netflix, which existing GNN architecture\ndesigns are not fully aware of.\nMoreover, the number\nof semantic concepts associated with each entity also varies,\nresulting in imbalanced semantic edge distribution as well.\n(2) Lack of informative features for semantic concept nodes:\nUnlike entities that are usually associated with abundant\ntextual or visual features, semantic concepts are represented\nas short phrases that only contain limited information.\n(3) Graph scalability issue: To handle web-scale graph data, we\nmust design an efficient distributed training framework to\ntrain over the entire graph with millions of nodes and billions\nof edges on GPUs.\nWe design a two-step training strategy, where\nwe first run pre-training via a KG completion task to generate\ncontextualized representations of semantic nodes (e.g., genre), and\nsecond train a novel GNN model via link prediction loss utilizing the\nmore sparse supervision signals from co-engagement relationships\nbetween entities.\nSpecifically, the KG pre-training stage addresses\nchallenge (ii) by forming representative embeddings for semantic\nconcept nodes, in contrast to directly using concepts\u00e2\u20ac\u2122 short phrases\nas textual features.\nIn this way, for a newly released title that lacks any co-engagement\ndata, we are able to distinguish the influence of different semantic\ntypes and thus learn an informative embedding.\nDistinguishing\nrelation types also helps us to better represent popular titles: for a\npopular title that has abundant co-engagement links, thanks to the\nlearnable prior weights of different relation types, the model is able\nto automatically adjust the influence received from co-engagement\nand from semantic edges, thus preventing noisy co-engagement\ndata from dominating its representation.\nTo address the scalability concerns in challenge (iii), we design\na Heterogeneity-Aware and Semantic-Preserving (HASP) subgraph\ngeneration scheme to train our model across multiple GPUs \u00e2\u20ac\u201c such\na framework has been proven efficient and feasible in an industry\nproduction environment.\nWe also show that our method is especially powerful\ncompared with baselines in the inductive setting and for learning\nnew title representations.\n3.2 Knowledge Graph Embeddings (KGE)\nKnowledge graph embeddings [1, 4, 15, 32] seek to acquire latent,\nlow-dimensional representations for entities and relations, which\ncan be utilized to deduce hidden relational facts (triples).\nLater on, ConvE [1] introduces\na learnable neural network in computing the score function, and\nsome recent studies explore leveraging textual input of entities and\nrelations to enhance triple prediction tasks, integrating efficient\nlanguage models like KG-Bert [39].\nHowever, these approaches are limited in the training of our Se-\nmanticGNN due to resource constraints and graph heterogeneity\n(Sec.\nWe first pretrain\nthe semantic KG via KG completion loss using TransE in order to\ngenerate the embeddings for concept nodes.\nWe then train a\nrelation-aware GNN over the KG using link prediction loss over\nEEL to get entity embeddings that reflect entity similarity.\n4.1 KG Pretraining\nOur goal in KG pretraining is to produce high-quality features for\nsemantic concept nodes, as concept nodes are usually associated\nwith short phrases, which may not be informative enough to serve\nas input features.\nIt can\nbe replaced with any off-the-shelf KG embedding method based on\ndifferent downstream applications and KG structures.\n, , \u00e2\u20ac\u00b2\n4.2 Relation-Aware GNN\nTo handle the imbalanced relation distribution in the constructed\nKG, we propose an attention-based relation-aware GNN to learn\ncontextualized embeddings for entities following a multi-layer mes-\nsage passing architecture.\nIn the -th layer of GNN, the first step involves calculating the\nrelation-aware message transmitted by the entity  in a relational\nfact ( , ,  ) using the following procedure:\n ( ) = Msg\nConcat(\n ( ) is the latent representation of  under the relation type\n at the -th layer, Concat(\u00c2\u00b7, \u00c2\u00b7) is the vector concatenation function, \nis the relation embedding and \n is a linear transformation matrix.\nThen, we propose a relation-aware scaled dot product attention\nmechanism to characterize the importance of each entity\u00e2\u20ac\u2122s neighbor\nto itself, which is computed as follows:\n(cid:205)\u00e2\u20ac\u00b2 \u00e2\u02c6\u02c6 N ( ) exp\n(cid:17)\n\u00c2\u00b7  ,\n\fWWW\u00e2\u20ac\u212224, May 2024, Singapore\nZijie Huang, Baolin Li, Hafez Asgharzadeh, Anne Cocos, Lingyi Liu, Evan Cox, Colby Wise, and Sudarshan Lamkhede\nwhere  is the dimension of the entity embeddings, \ntwo transformation matrices, and  is a learnable relation factor\nfor each relation type  .\nDiverging from conventional attention\nmechanisms [2, 33], we incorporate  to represent the overall sig-\nnificance of each relation type  .\nWe then update the hidden representation of entities by aggregat-\ning the message from their neighborhoods based on the attention\n +  (cid:169)\n \u00e2\u02c6\u02c6N ( )\nwhere  (\u00c2\u00b7) is a non-linear activation function, and the residual\nconnection is used to improve the stability of GNN [9].\n5 SYSTEM DEPLOYMENT\nNext, we describe a practical distributed GNN training framework\nthat addresses the challenge of scalability and practicality in train-\ning GNNs for deployment.\nOur framework\nameliorates these challenges by training on a set of sub-graphs\nwith preserved semantic information and distributing the computa-\ntional workload across multiple GPUs in a cluster.\nIt does so without sacrificing the model\u00e2\u20ac\u2122s ability to learn\nentity embeddings using semantic information, effectively bridging\nthe gap between computational efficiency and model performance.\nIn our graph, these edges\nrepresent user co-play history, resulting in a significant volume of\nentity-entity connections.\nTo put this into perspective, even on a\npowerful NVIDIA A100 Tensor Core GPU with 40GB of memory,\nwe can only feasibly train a semantic knowledge graph of a certain\nsize.\nFurthermore, our knowledge graph is poised for continual expan-\nsion, demanding a training framework that remains future-proof\nin the face of these escalating memory demands.\nSecondly, the graph sampling and\nstreaming from the CPU into the GPUs can become a performance\nbottleneck during training.\nGiven these challenges, there\nis a clear need for a more adaptable and scalable multi-GPU GNN\ntraining solution for heterogeneous graphs.\nA challenge arises when dividing\nentity nodes: if two nodes connected by an EEL are apportioned to\nseparate HASP subgraphs, the EEL is effectively bypassed during\nGNN training.\nFurthermore, to avoid computational dispari-\nties and ensure efficient distributed training, it is imperative that\neach HASP subgraph contains a roughly equal count of entity nodes.\nTherefore, our entity node partitioning\nstrategy strives to achieve two primary objectives: (i) minimizing\nthe elimination of EELs across HASP subgraphs and (ii) ensuring\nan equitable distribution of entity nodes across subgraphs.\nFor the former group, we engage in a minimum cut graph par-\ntitioning process on the EEL subgraph, which is composed of all\nEELs and their interconnected nodes.\nHere, \nstands as a representation of our predetermined number of target\npartitions, generally set to be a multiple of available GPUs and opti-\nmized in line with the GPU memory capacity to ensure each HASP\nsubgraph can comfortably reside within GPU memory.\nWe deliberately avoided partitioning the\nentire entity node subgraph, including both the EEL and non-EEL\nnodes, in one go to prevent skewed EEL distributions, where some\npartitions could be densely populated with nodes interconnected by\nEELs while others might be bereft of them, potentially undermining\nthe generality of our resulting HASP subgraphs.\nIn our HASP generation process,\neach subgraph encompasses an exclusive set of entity nodes.\nYet, to\nensure the preservation of all semantic information, every subgraph\n1221435321Title (no EEL)Title (EEL)Semantic nodesSemantic edgesEEL(a) Original Graph(b) Title node partition12435Node partition on title nodes with EELRandomly assign non-EEL nodes to each partition12435132(c) Semantic Node Duplication435212121312Subgraph 1Subgraph 2\fWWW\u00e2\u20ac\u212224, May 2024, Singapore\nZijie Huang, Baolin Li, Hafez Asgharzadeh, Anne Cocos, Lingyi Liu, Evan Cox, Colby Wise, and Sudarshan Lamkhede\nTable 1: Evaluation results of baselines and our proposed method SemanticGNN over two ground-truth entity similarities:\nHC-Sim (human-curated semantic sims) and Co-Sim (observed co-engagements).\nTo elucidate, consider the graph\u00e2\u20ac\u2122s representation\nof genres: there might only be 20 distinct genre types, each rep-\nresented by a single semantic node.\nGiven their potentially vast quanti-\nties, duplicating these external entities, akin to the way we handle\ntraditional semantic nodes, becomes impractical.\nBy duplicating semantic nodes across all\nsubgraphs, HASP ensures the consistent preservation of semantics.\nFor complex sce-\nnarios like integrating external show entities, HASP provides a user-\ndefined node sampling, allowing customized node inclusion while\nstaying within memory limits.\nUsers can leverage the HASP frame-\nwork for efficient GNN training as we have observed an average\nimprovement of over 50\u00c3\u2014 when trained on 4 A100 GPUs compared\nto a naive full-graph training on a large-memory CPU machine.\nFor\ninference, given its one-time effort nature, node embeddings can be\ngenerated using a large-memory CPU machine, ensuring all edges\nin the knowledge graph are holistically preserved.\nIn prac-\ntice, the deployed model over the full KG shows great performance\ngain in varying downstream recommendation tasks, especially for\nnew/unpopular titles.\nIt is in general more time-expensive\ncompared with the link prediction loss in SemanticGNN.\n6.3 Main Results\nWe first examine whether our SemanticGNN has superior perfor-\nmance compared with baselines as well as the design rationality\nof each component of SemanticGNN.\n"}, {"role": "assistant", "content": "ABSTRACT\n\nGiven a set of candidate entities (e.g. movie titles), the ability to\nidentify similar entities is a core capability of many recommender\nsystems. Most often this is achieved by collaborative filtering ap-\nproaches, i.e. if users co-engage with a pair of entities frequently\nenough, their embeddings should be similar. However, relying on\nco-engagement data alone can result in lower-quality embeddings\nfor new and unpopular entities. We study this problem in the con-\ntext recommender systems at Netflix. We observe that there is\nabundant semantic information such as genre, content maturity\nlevel, themes, etc. that complements co-engagement signals and\nprovides interpretability in similarity models. To learn entity sim-\nilarities from both data sources holistically, we propose a novel\ngraph-based approach called SemanticGNN. SemanticGNN models\nentities, semantic concepts, collaborative edges, and semantic edges\nwithin a large-scale knowledge graph and conducts representation\nlearning over it. Our key technical contributions are twofold: (1)\nwe develop a novel relation-aware attention graph neural network\n(GNN) to handle the imbalanced distribution of relation types in\nour graph; (2) to handle web-scale graph data that has millions of\nnodes and billions of edges, we develop a novel distributed graph\ntraining paradigm. The proposed model is successfully deployed\nwithin Netflix and empirical experiments indicate it yields up to\n35% improvement in performance on similarity judgment tasks.\n\nKEYWORDS\n\nFigure 1: Similar entity generation in Netflix.\n\nFigure 2: (a) Title degree distribution with/without entity-\nentity engagement links (EEL). The majority of titles are\nwithout EEL. (b) Constructed semantic knowledge graph for\nlearning title representations.\n\nRepresentation Learning, Graph Neural Networks, Knowledge Graphs\n\n1 "}]}
