{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nRobotics, as a technology,\naims to minimize the necessity for human involvement in\nvarious work processes.\nWhile speech recognition involves the\nsystem comprehending spoken words, it does not necessarily\ngrasp their meaning[1].\nThe\ninitial step involves processing a voice command through\nthe Android platform, where the conversion of voice to text\noccurs within the system.\nThis platform\nis chosen for its simplicity compared to other development\nplatforms, and the Google speech recognizer is employed for\nan effective system response.\nThe Bluetooth\ntechnology utilized for automation is cost-effective compared\nto alternative communication platforms like Zigbee, GSM\n(Global System for Mobile Communication), and General\nPacket Radio Service (GPRS), with a simple installation\nprocess.\nFurthermore, the integration of a mo-\ntor driver and relay driver module with Arduino enhances\nthe effectiveness of driving output\nloads.\nThis paper also\nemphasizes real-time surveillance and automation concepts,\nincorporating an obstacle detection and avoidance mechanism\nfacilitated by the ultrasonic sensor HCSR04.\nThe ultrasonic\nsensor measures the distance between the target and the robot\u00e2\u20ac\u2122s\nactual position.\nThis is achieved by transmitting an ultrasonic\nwave that reflects off a target, and the time taken for the echo\nto return helps calculate the distance.\nThe robot\u00e2\u20ac\u2122s movement\nis determined based on the distance measurement by the\nultrasonic sensor, and this concept is implemented through\nprogramming in the Arduino IDE (Integrated Development\nEnvironment) and loading it onto the Arduino Atmega 328.\nThe subsequent sections of this paper are organized to pro-\nvide a comprehensive understanding of the development of\nthis technology.\nIn Section III,\nthe simulation of this concept has been executed using Pro-\nteus simulation software, and the results are presented.\nThe\nprogramming concept of the robot is elucidated in Section IV,\ndetailing the software aspects essential for its functionality.\nMoving on to Section V,\nthe design architecture of this\ntechnology is thoroughly depicted, offering insights into the\noverall structural framework.\nSimilarly, Section VI is dedicated\nto the hardware implementation of this concept.\nIt provides a\ndetailed account of how the theoretical concepts are translated\ninto a physical system, including the integration of components\nsuch as motor drivers, relay modules, and Arduino boards.\nTECHNICAL COMPONENTS\nThe first essential technical component for this project is\nthe Arduino Uno (Figure1), a microcontroller based on the\nAtmega328[2].\nThe inclusion of a 16 MHz ceramic\nresonator[1] and the incorporation of all necessary circuitry\nwithin a single module contribute to the operational simplicity\nof the Arduino Uno.\nIn\npractice, the module operates within a range of 50 meters[1].\n4: PCB design artwork view[1]\nE. Ultrasonic Sensor HCSR04\nThe interfacing mechanism between an Arduino and the\nultrasonic sensor HCSR04 involves measuring the distance\nbetween a target and the real position of the sensor.\nThis\nis achieved by calculating the time duration between the\ntransmission and reception of an ultrasonic wave.\nThe Android application (Figure7) serves as the primary\ninterface for voice signals and has been developed using\nthe MIT App Inventor[3] platform, employing a drag-and-\ndrop programming approach.\nThe application operates based\non Bluetooth and internet connectivity and was designed\nwith the utilization of the Google Voice Recognizer module.\nSubse-\nquently, this message is transmitted to an electronic hardware\nthrough Bluetooth connectivity.\nThis approach allows for a responsive and\nconditional control mechanism where the system\u00e2\u20ac\u2122s behavior is\n\fgrammed sequence.\nThe simulation of the concept is presented\nin Figure8.\nWhen serial data is available, the program reads the bytes from\nBluetooth, initiates string processing, and checks the received\nstring against predefined voice commands.\nFigure9 illustrates the code for ultrasonic sensor\ninterfacing with an Arduino, while Figure10 depicts the code\nfor string processing.\nSimilarly, Figure11 showcases the code\nfor voice command execution, and Figure12 introduces the\nconcept of automation, demonstrating the control of light-\ning and horn mechanisms through voice commands.\nIn this\nprogram, the voice is defined as a string, and parameters\nsuch as distance, duration, and safety distance are declared\nas integer, long, and integer, respectively.\nThe ultrasonic sensor is employed for obstacle detection and\navoidance, a crucial condition applied to the robot.\nDistance measurement is facilitated\nby the pulse received from digital pin 6 of an Arduino,\nutilized as the echo pin in our project.\nUpon detecting an obstacle, the\nrobot halts immediately, moves backward, and takes a left\ndirection, as per the code depicted in Figure9.\nThe robot\nceases its movement within 10 centimeters of obstacles, and\nafter completing subsequent movements, the safety distance\ncondition is removed from the programming loop.\nIn real-\nworld operation, when the user provides a voice command\nthrough an Android application and the string check condition\nbecomes true, the program executes based on the function\ncall within that particular condition.\nFigures13 (design architecture) and14\n(block diagram) depict the different sections of the project,\nencompassing two main components: an Android application\nand a robot equipped with a wheel mechanism, light, and\nbuzzer.\nIn this methodology, wireless communication is estab-\nlished through serial communication, where one bit at a time\nis transmitted and can be processed by a controller.\nHARDWARE IMPLEMENTATION\nThis section provides a detailed account of how the theo-\nretical concepts are translated into a physical system, encom-\npassing the integration of components like motor drivers, relay\nFig.\nUsing this platform, a\ncircuit can be drawn, and the programming file in hexadecimal\nformat\u00e2\u20ac\u201dgenerated after compiling the program in the Arduino\nIDE\u00e2\u20ac\u201dcan be uploaded.\nThe communication\nplatform is established through the virtual terminal, and a\ncircuit has been identified that operates according to a pro-\n\fFig.\nIn the hardware testing phase,\nrigorous evaluation and validation were conducted to ensure\nFig.\nThis involved\ntesting the responsiveness of the motor drivers, assessing the\nreliability of the relay modules, and confirming the proper\nintegration of Arduino boards within the system.\nThe objective\nof this phase was to identify and address any potential issues\nbefore proceeding to the full-scale hardware implementation.\n14: Block Diagram[1]\ncess of hardware testing, ensuring the reliability of individual\ncomponents, followed by the successful physical development\nof the robot.\nThe figures provide a visual representation of both\nthe testing and implementation phases, offering a complete\nunderstanding of the hardware journey in the realization of\nthe voice recognition robot concept.\n13: Design Architecture[1]\nFigure15 illustrates the hardware testing process, showcasing\nthe systematic evaluation of individual components and their\ncollective functionality.\nMoving forward, the hardware implementation\nphase involved the physical development of the robot, inte-\ngrating the tested components into a cohesive and functional\nsystem.\nThe construction included mounting the motor drivers,\nrelay modules, and Arduino boards onto the robot chassis,\naligning with the design architecture presented in earlier\nsections.\nFigure16 visually represents the completed hardware\nimplementation, illustrating the integrated components within\nthe physical structure of the robot.\nThe design carefully\nincorporates voice recognition technology, highlighting how\nthe hardware components seamlessly interact with the software\nThe integration of voice recognition technology into the\nphysical robot marks a crucial advancement, bringing the con-\ncept to life.\n"}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dVoice recognition technology enables the execution\nof real-world operations through a single voice command. This\npaper introduces a voice recognition system that involves con-\nverting input voice signals into corresponding text using an\nAndroid application. The text messages are then transmitted\nthrough Bluetooth connectivity, serving as a communication\nplatform. Simultaneously, a controller circuit, equipped with a\nBluetooth module, receives the text signal and, following a coding\nmechanism, executes real-world operations. The paper extends\nthe application of voice recognition to real-time surveillance\nand automation, incorporating obstacle detection and avoidance\nmechanisms, as well as control over lighting and horn functions\nthrough predefined voice commands. The proposed technique not\nonly serves as an assistive tool for individuals with disabilities\nbut also finds utility in industrial automation, enabling robots to\nperform specific tasks with precision.\n\nIndex Terms\u00e2\u20ac\u201dVoice Recognition, Android Applications, Blue-\ntooth Connectivity, Real-time Surveillance, Automation, Obstacle\nDetection, Assistive Technology, Industrial Automation\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nThe second year of life is marked by\nboth the emergence of more semantic visual representa-\ntions and a better understanding of word meaning.\nHowever, even in suit-\nable contexts for word learning like dyadic play sessions,\ncaregivers utterances are sparse and ambiguous, often re-\nferring to objects that are different from the one to which the\nchild attends.\nWe introduce a synthetic dataset of ego-centric images per-\nceived by a toddler-agent that moves and rotates toy ob-\njects in different parts of its home environment while \u00e2\u20ac\u0153hear-\ning caregivers\u00e2\u20ac\u2122 utterances, modeled as captions.\nOn one side, there is evidence\nthat biological organisms learn similar representations for\nclose-in-time visual inputs [19, 38].\nThis is called the slow-\nness principle [37] and it supports the building of view-\ninvariant object representations during, e.g., object manip-\nulations [18, 29].\nOn the other side, the acquisition of ob-\nject names correlates with a stronger focus on global shape\nfeatures relative to texture [13, 17, 28], which spurs per-\nformance in name-agnostic categorization tasks [9, 21], see\nalso [14].\nDyadic play could be particularly important for pro-\nmoting the emergence of more semantic object representa-\ntions, because both learning mechanisms are likely to co-\noccur during such play:\nthe toddler extensively focuses\non/manipulates objects and the caregiver frequently talks\nto the toddler about the present objects [26, 31].\nHow-\never, naming utterances during such play are sparse [6] and\nambiguous: they typically contain several words unrelated\nto the object.\nFurthermore, even though toddlers have bi-\nased attention towards held objects [39], they often play\nin contexts that admit several other objects in the back-\nground [3, 40], adding another potential source of confu-\nIn this paper, we investigate two what extent dyadic\nplay sessions can support the emergence of semantic ob-\nject representations in toddlers.\nD) Summary of the learning architecture, see Section 3.2 for details.\nThen, we consider a bio-\ninspired model of toddlers\u00e2\u20ac\u2122 learning that 1) maps close-in-\ntime visual inputs to similar representations and 2) similarly\naligns the representations of visual inputs and co-occurring\nnaming utterances (Fig.\nThe overall model al-\nlows us to systematically study the potential impact of the\nsparsity and ambiguity of naming utterances on the learned\nvisual representations.\nRelated work\nModels of toddlers\u00e2\u20ac\u2122 object learning Recently, the avail-\nability of datasets of images extracted from toddlers\u00e2\u20ac\u2122 head-\nmounted cameras [3, 32] has enabled the study of represen-\ntations learnt through the senses of toddlers.\nHowever, they only\nconsider synthetic images based on 9 digits rather than sim-\n\fulated ego-centric play sessions.\nIntegrating\nconstraints from toddlers\u00e2\u20ac\u2122 perceputal experience into these\nmodels, like short arms or foveation, can lead to improved\nability to recognize objects in front of novel backgrounds\n[2].\nHowever, none of these works studied\nwhether naming statistics in dyadic play sessions elicit\nsemantic visual representations.\nA recent work showed\nthat adding sparse labeling on top of contrastive learning\nthrough time enhances visual categorization [2].\nTo create them, we simulate a toddler that interacts with\nobjects using the simulation platform ThreeDWorld (TDW)\nImage recording At the beginning of each play session,\nwe place the agent in a random location, with a random\norientation within the Virtual Home Environment [1] (cf.\nWe approximate an ego-centric view of a seated\ntoddler by positioning a camera at 0.4 m above the ground.\nIn addition to the main object, we add 5 to 20 randomly\nsampled (without replacement) background objects to the\nscene, following the number of toys commonly present in\nthe field of view of toddlers during play [3].\nTo place them,\nwe follow the same procedure as for the main object, but\nchange the range of distances from the agent to [0.42; 0.875]\nm. In contrast to the object being held, we let them fall to\nthe floor one at a time using the physics engine of TDW.\nTo simulate natural interactions, in each of the 20 frames\nof the play sessions the agent slowly moves and turns the\nobject along/around the three axes.\nWe display a high vari-\nability of manipulation sequences by generating all move-\nments with an Ornstein-Uhlenbeck stochastic process for\nwhich we randomly generate a new recall coefficient in\n[0.1, 1] at the beginning of each session, for each kind\nof manipulation and axis.\nFinally, while the agent observes the moving object,\nit also executes additional relative small eye movements\naround the object in focus (yaw and pitch axis), whose end-\npoints are sampled from a Normal distribution N ([0, 0], 2\u00c3\u2014\nI).\nWe also allow the agent to focus its gaze on an object\ndifferent from the main one with probability 0.7; however,\nwe discard these images during training as previous meth-\nods already analyzed the impact of changes of attention on\ncontrastive through time losses [1, 25].\nAssuming that the\npitch/roll rotations rarely go beyond [\u00e2\u02c6\u201920; 20]\u00e2\u2014\u00a6 from their\n\fstarting orientation during training, we increase the initial\nrange of test object orientations to [\u00e2\u02c6\u201920; 20]\u00e2\u2014\u00a6 around the\npitch and roll axis\nutterances We\nstatistics of utterances\ndevelopmentally-relevant\na caregiver to a toddler during a play session.\nOur study focuses specifically on transcripts\nfrom native English-speaking children residing in North\nAmerica and the UK, between the ages of six months\nand two years.\nAfter filtering out incomplete utterances and\nthose containing less than three words, as well as a manual\nreview, we retained 820 templates.\nSelf-supervised learning of visual representa-\nIn this work, we postulate that toddlers learn visual rep-\nresentations that 1) slowly change over time and 2) align\nwith co-occurring linguistic representations.\nThe first loss aligns embed-\ndings of close-in-time images; this entails that seeing an\nobject from different viewpoints elicits viewpoint invariant\nobject representations [25].\nSince the naming utterance may inform about the category\nof the object in focus, this provides weak supervision re-\ngarding the object category.\nWe sum up the learning archi-\ntecture in Fig.\nThereafter, we\ncompute embeddings of images z1 = g1(f (x)) using a fea-\nture extractor f and a projection head g1, both implemented\nas neural networks [5].\nMultimodal contrastive learning For each sampled im-\nage xi, we also sample, if provided by the caregiver, its co-\noccurring naming utterance li.\nThe\ntop part of (2) ensures that co-occurring naming utterances\nand images have similar embeddings while the bottom part\nprevents all embeddings to collapse into a single vector.\nThus,\nwe define the (conditional) probability of naming the ma-\nnipulated object (versus another object in the background)\nas pcorrect = 0.5.\nSecond, a caregiver approximately pro-\nvides, on average, one naming utterance related to the ob-\nject being manipulated per time-extended object manipula-\ntion [31] (exactly 2.54\n2 = 1.27 in their study).\nWe assume that consecutive frames\nare spaced by one second, making the duration of our play\nsessions correspond to the average of 20 seconds reported\nin [31].\nThe ra-\ntionale behind using a pre-trained text model lies in our re-\nsearch focus on visual representation learning.\nEvaluation To evaluate if the learned representation sup-\nports category recognition, we apply a repeated random\nsub-sampling cross-validation to split the non-overlapping\nobject instances into 2382 train (2/3 of the total) and 1191\ntest objects (1/3 of the total).\nThen, we extract images of\ntrain and test objects to form the train and test datasets,\nrespectively (see Section 3.1).\nTo evaluate our representa-\ntion with respect to object instance recognition, we extract\nnovel images of the train objects to build a test dataset.\nFor\nboth object instance and category recognition, we freeze the\nweights of the vision encoder and train a linear classifier in\nan online and supervised fashion on top of the latent visual\nrepresentation [4].\n2 compares such statistics\nwith two upper-bound baselines: Oracle refers to super-\nvised learning and Ideal refers to a caregiver who always\nutters sentences containing the correct object.\nWe ob-\nserve that our model improves category recognition over\nnot using naming utterances, even though it does not reach\nthe performance of oracles.\nAdditionally, we examine how\nplausible utterance statistics perform when used to train the\nFigure 2.\nThe model yields similar out-\ncomes, albeit necessitating significantly more training time,\nas it has not yet reached convergence.\nSmall changes in utterance frequency and am-\nbiguity drastically impact object representa-\nTo understand how frequency and ambiguity of utter-\nances affect object representations, we systematically vary\nboth factors in Fig.\nThis suggests that toddler\u00e2\u20ac\u2122s object learning may be quite\nsensitive to caregivers\u00e2\u20ac\u2122 presence and the quality of their ut-\nterances.\nWhen considered alongside the\naccuracy values of classification experiments, this observa-\ntion strongly suggests that increased textual attention to the\nobject name is crucial for learning object representations.\n"}, {"role": "assistant", "content": "Abstract\n\n1. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nThe\nset of individual ledgers can be viewed as a distributed ledger, denoting the\n*Corresponding Author\nG. Diamantopoulos et al.\nA consensus protocol is used to aid in validating\nand ordering the transactions and through it, a special set of nodes called block\nproducers, vote on the order and validity of recent transactions, and arrange\nthem in blocks.\nDespite the potential of Blockchain in many different domains, factors such\nas low scalability and high latency have limited the technology\u00e2\u20ac\u2122s adoption, es-\npecially in time-critical applications, while in general, blockchain suffers from\nthe so-called trilemma trade-off that is between decentralisation, scalability, and\nsecurity [28].\nThe most notable factor affecting the performance of the blockchain, ex-\ncluding external factors we cannot control such as the system architecture, net-\nwork, and workload, is the consensus protocol, with system parameters such\nas block time, and block interval getting a close second.\nThe trilemma trade-\noff in combination with blockchains time-varying workloads makes the creation\nof robust, general consensus protocols extremely challenging if not impossible,\ncreating a need for other solutions [8].\nAdditionally, blockchain systems support and are influenced\nby dynamic changes to the system parameters (including the consensus proto-\ncol) during runtime.\nThus there is a need for dynamic management of blockchain\nDigital Twins and DDDAS have been utilised in autonomic management of\ncomputational infrastructures [17,21,7,2] and the last few years have witnessed\nseveral efforts to bring together Blockchain and Digital Twins.\nIn our previous work [6], we presented a Digital Twin architecture for the\ndynamic management of blockchain systems focusing on the optimisation of the\ntrilemma trade-off and we demonstrated its use to optimise a blockchain system\nfor latency.\nWe explore\nhow DDDAS feedback loop principles can support the design of info-symbiotic\nlink connecting the blockchain system with the simulation and analytic environ-\nment to dynamically manage the trilemma.\nAs part of the loop, we contribute to\na control mechanism that uses Reinforcement Learning agent(RL) and combined\nwith our existing simulation framework.\nThe combination overcomes the limita-\n\fDynamic Data-Driven Digital Twins for Blockchain Systems\ntions of just relying on RL while relaxing the computational overhead required\nwhen relying solely on simulation.\nThe rest of the paper is structured as follows: Section 2 discusses the utilisa-\ntion of Digital Twins for the management of Blockchain systems and provides an\noverview of a Digital Twin framework for this purpose.\n2 Digital Twins for Blockchain Systems\nFor this paper, we consider a generic permissioned blockchain system illustrated\nas \u00e2\u20ac\u2122Physical System\u00e2\u20ac\u2122 in Fig.\nSpecifically, when it\u00e2\u20ac\u2122s the turn of a block pro-\nducer to produce a new block, it first gathers the oldest transactions in the pool,\nverifies them and packs them into a block, signs the block with its private key\nand initiates the consensus protocol.\nThe consensus protocol acts as a voting\nmechanism for nodes to vote on the new state of the system, the new block in\nthis case, and as mentioned earlier, is the main factor affecting the performance\nof the blockchain.\nIt is pertinent to note that producing blocks in a round robin\nfashion is simple to implement albeit inefficient due to \"missed cycles\" caused\nby invalid blocks or offline nodes [1].\nOther alternative implementations are pos-\nsible, such as having every block producer produce a new block or leaving the\nselection up to the digital twin.\nAlthough consensus protocols have been studied for many years, due to their\nuse in traditional distributed systems for replication, blockchains larger scale,\nin combination with the unknown network characteristics of the nodes, make\nthe vast majority of existing work incompatible.\nRecent works have focused on\nG. Diamantopoulos et al.\nadapting some of these traditional protocols for the permissioned blockchain,\nachieving good performance but so far no one has managed to create a protocol\nachieving good performance under every possible system configuration [9].\nWith\nseveral specialized consensus protocols available, a dynamic control system is a\nnatural solution for taking advantage of many specialised solutions while avoid-\ning their shortcomings.\nThe idea of trying to take advantage of many consensus protocols is not new,\nsimilar concepts already exist in the literature, in the form of hybrid consensus\nalgorithms [13,18] which combine 2 protocols in one to get both benefits of both.\nAlthough fairly successful in achieving their goal of getting the benefits of two\nconsensus protocols, hybrid algorithms also combine the shortcomings of the\nalgorithms and are usually lacking in performance or energy consumption.\nIn\ncontrast, a dynamic control system allows for the exploitation of the benefits of\nthe algorithms involved, with the cost of additional complexity in the form of\nthe selection mechanism.\nPBFT acts as a robust protocol capable of effi-\nciently achieving consensus when byzantine behaviour is detected in the system\nwhile BigFoot is a fast alternative when there are no signs of byzantine behaviour\nTo achieve the above, we employed a Digital Twin (DT) coupled with a\nSimulation Module to conduct what-if analysis, based on which, an optimiser\nwould compute the optimal consensus protocol for the next time step.\nUsing a\nDT can overcome the shortcomings of relying on an RL agent alone since the\nsimulation element and what-if analysis allow for the exploration of alternative\nfuture scenarios [25].\n3 The DDDAS Feedback Loop\nThe system described in the previous section closely follows the DDDAS paradigm,\nwith the Digital Twin containing the simulation of the physical system, the node\nfeeding data to the Digital Twin acting as the sensors, and the optimiser updat-\ning the system closing the feedback loop.\nThese block producers can act as the sensors of\nthe physical system tasked with periodically sending state data to the digital\ntwin.\nUsing the list of historical\ntransactions, we can develop a model of the workload used in the simulation.\nAlthough using queries to request the state of block producers requires a mecha-\nnism to overcome the Byzantine node assumption, blocks contain a large amount\n\fDynamic Data-Driven Digital Twins for Blockchain Systems\nFig.\nEach new block contains an ex-\ntra data field in which the full timestamped history of the consensus process is\nstored and can be used to infer the state of the Block producers.\nSpecifically,\nthrough blocks, we can learn the state of block producers (offline/online), based\non whether the node participated in the consensus protocol or not, as well as\ndevelop a model of the block producers and their failure frequency.\nAs the optimisation tasks get more complicated, with mul-\ntiple views taken into account (figure 1), smart contract simulation, harder to\npredict workloads, and especially once the decision-making process gets decen-\ntralised and replicated over many block producers, conducting what-if analysis\nbecomes taxing on the hardware.\nDepending on the case i.e energy aware sys-\ntems or systems relying on low-powered/battery-powered nodes might not be\nable to justify such an expensive process or worst case, the cost of optimisation\ncan start to outweigh the gains.\n3.1 Augmenting Reinforcement Learning with Simulation\nIn this paper, we propose the use of a Reinforcement Learning (RL) agent in\ncombination with simulation and what-if analysis to overcome the individual\nshortcomings of each respective technique.\nBy using the simulation module\nto augment the training with what-if generated decisions the agent can learn\na more complete model of the system improving the performance of the agent.\nAdditionally, what-if analysis can be used when the agent encounters previously\nunseen scenarios, avoiding the risk of bad decisions.\nBy splitting the DDDAS into multiple feedback loops,\nwe can allow for finer control of both the data needed and the frequency of\nthe updates.\n4 Experimental setup and Evaluation\nExperimental setup To illustrate the utilisation of RL-based optimisation and\nanalyse the impact of using simulation to enhance the RL agent, a prototype\nActionbcActionsimAgentStatebcStatebcBlockchainSimulatorDDDAS Feedback loopWhat-if Simulation for Data AugmentationStatesimScenario GeneratorRewardbcView: LatencyView: DecentralizationView: SecurityRewardsimDigital Twin\fDynamic Data-Driven Digital Twins for Blockchain Systems\nimplementation of the system presented in figure 3 has been developed focusing\non latency optimisation.\n3: An example instantiation of RL-based control: Latency View\nFor the experiments, a general permissioned blockchain system like the one\ndescribed as \"Physical System\" in Fig.\nTwo consensus algorithms were implemented specifically,\nPBFT and BigFoot which complement each other as shown in [6].\nThe scenario\ngenerator created instances of the above blockchain system by randomly gener-\nating values for the following parameters: (a) Transactions Per Second (T P S)\nwhich denotes the number of transactions the nodes generate per second; (b) T\nwhich denotes the size of the transactions; (c) Node State which signifies when\nand for how long nodes go offline; and (d) Network State which denotes how\nthe network state fluctuates over time.\nAdditionally, following our previous ap-\nproach, we assume that the system does not change state randomly, but does\nso in time intervals of length T I.\nFinally, the digital twin updates the system in\nregular time steps of size T S.\nA Q-Learning agent has been used.\nThe action space is a choice between the two\nconsensus protocols and the reward function is simply the average transaction\nlatency of the optimised T S as measured in the physical system.\n4: Results of the experimental evaluation with (a) showing the training per-\nformance of the agent on WL1 (b) the performance of the agent and the agent\n+ simulation (denoted as agent+) for WL2\nwhere the state has evolved over time.\nIn WL2 the agent\u00e2\u20ac\u2122s\nperformance is shown to decrease in comparison to that of the agent augmented\nwith the simulation (agent+) (Fig.\n5 Conclusions\nLeveraging on our previous work on utilising Digital Twins for dynamically man-\naging the trilemma trade-off in blockchain systems, in this paper we have focused\non the DDDAS feedback loop that links the Digital twin with the blockchain sys-\ntem.\nThe optimiser combines\nRe-enforcement Learning and Simulation to take advantage of the efficiency of\nthe agent with the accuracy of the simulation.\nOur experimental results confirm\nthat the proposed approach not only can successfully increase the performance\nof the agent but do so more efficiently, requiring less computational overhead.\n"}, {"role": "assistant", "content": "Abstract. In recent years, we have seen an increase in the adoption of\nblockchain-based systems in non-financial applications, looking to ben-\nefit from what the technology has to offer. Although many fields have\nmanaged to include blockchain in their core functionalities, the adoption\nof blockchain, in general, is constrained by the so-called trilemma trade-\noff between decentralization, scalability, and security. In our previous\nwork, we have shown that using a digital twin for dynamically manag-\ning blockchain systems during runtime can be effective in managing the\ntrilemma trade-off. Our Digital Twin leverages DDDAS feedback loop,\nwhich is responsible for getting the data from the system to the digital\ntwin, conducting optimisation, and updating the physical system. This\npaper examines how leveraging DDDAS feedback loop can support the\noptimisation component of the trilemma benefiting from Reinforcement\nLearning agent and a simulation component to augment the quality of\nthe learned model while reducing the computational overhead required\nfor decision making.\n\n1\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nVLPMs continue to grow not only in terms of accuracy but\nsize as well, as the newer models have parameters in bil-\nlions and can perform several tasks with human-like accu-\nracy.\nAs shown in Figure 2, compared to 2018, there has\nbeen a big surge in articles about \u00e2\u20ac\u0153vision-language trans-\nformer in 2022, nearly 9.5 times more, and an even larger\nincrease, nearly 12.5 times more, in 2021.\nCross-modal contrastive Learning is another pre-\ntraining task quite similar to ITM but in a contrastive manner\nin the way that matched image-text pairs are pushed together\nand non-matched pairs are pushed apart using contrastive\nloss.\nThe large datasets used for pre-training have been con-\nsidered to be a cause of bias (Park and Choi 2022; Radford\net al.\nThis amalgamation raises concerns about the po-\ntential incorporation of biases present in the content from\nthe open web into the models themselves (Mittal et al.\nBiases like gender or racial bias\nhave proven harmful, especially when they affect humans\nin real life (Singh et al.\nLiterature has shown that VLPMs are heavily in-\nfluenced by language modality and can sometimes be harm-\nful.\nThe authors not only depict gender and racial bias\nbut also analyze recent captioning models to see the differ-\nences in the performance from a lens of bias.\nSome stud-\nies have looked at task-specific datasets as well, as (Hirota,\nNakashima, and Garcia 2022a) analyze five Visual Question\nAnswering (VQA) datasets for gender and racial bias.\nThe authors tested zero-\nshot and linear probe instances of the model to mark the\npotential sources of biases and harmful markers.\n(Zhang,\nWang, and Sang 2022) proposes the CounterBias method\nand FairVLP framework to quantify social bias in VLPMs\nin a counterfactual manner while proposing a new dataset\nto measure gender bias.\nFurthermore, VLStereoSet (Zhou, Lai, and Jiang 2022)\nmeasured stereotypical biases in VLPMs using probing tasks\nby testing their tendency to recognize stereotypical state-\nments for anti-stereotypical images.\nThey\nalso proposed two metrics called vision-language relevance\nscore and vision-language bias score, using which they con-\ncluded that state-of-the-art VLPMs under consideration not\nonly encode stereotypical bias but are more complex than\nlanguage bias and need to be studied.\nEven when look-\ning for societal biases \u00e2\u20ac\u201c gender and racial, there is a lack\nof commonality, yet none of the observations and results\ncan be denied as less crucial.\n2022) proposed\na Robust Vision Transformer (RVT) after studying the com-\nponents affecting the robustness of the model, proposing a\nnew patch-wise augmentation and a position-aware atten-\ntion scaling (PAAS) to boost the RVT other than modify-\ning damaging elements in the architecture for better robust-\nness.\nThe noisy\ndata is created using poison attacks like label flipping and\nhas been compared under adversarial filtering augmentation.\nThey introduced a novel robustness metric called Mean Rate\nof change of Accuracy with change in Poisoning (MRAP),\nusing which they observed that the models are not robust un-\nder adversarial filtering.\nIn most of these studies, the com-\nparison between CNNs and transformers is drawn from ex-\nisting attacks proposed originally for CNNs, but it is impor-\ntant to devise attacks that exploit vulnerabilities of the latter,\nkeeping in mind the critical architecture difference between\nVLPMs and their Robustness\nVLPMs are studied under the robustness lens but not as ex-\ntensively as unimodal transformers.\nSince the attack framework is sensi-\ntive to the transformer architecture, the attacks consider both\npatches by perturbing only a subset of them at each iteration\nand attention module by skipping some attention gradients.\n2022) investigated how\nVLPMs perform under data with missing or incomplete\nmodalities (examining only one modality at a time) in terms\n\fof accuracy and were improved using different fusion strate-\ngies.\nThey concluded that transformers are not only sensitive\nto missing modalities but also that there is no optimal fusion\nstrategy as multimodal fusion affects the robustness of these\nmodels and is dependent on datasets.\n2022) an-\nalyzes VLPMs to get a better insight into the multimodal re-\nlationship using probing tasks, concluding that concepts like\nposition and size are difficult for the models under consid-\neration to understand.\n(Schlar-\nmann and Hein 2023) on the other hand, studied adversar-\nial robustness for imperceivable attacks on VQA and Im-\nage captioning tasks for well-known multimodal foundation\nmodels and (Mao et al.\nThe authors proposed a text-guided con-\ntrastive adversarial training (TeCoA) to be used along with\nfinetuning to improve the zero-shot adversarial robustness.\nA few methods originally proposed\nfor CNNs have been extended for transformers as well,\nlike GradCAM (Selvaraju et al.\nWhile visualization-based methods usually use inter- and\nintra-modality interactions to visually explain the decisions,\nprobing tasks are specifically designed to explain a particu-\nlar aspect or component of the models and can be restrictive.\nModifications of these meth-\nods have been proposed in the literature, like the Attention\nRollout (Abnar and Zuidema 2020), which combined lay-\ners to get averaged attention.\nFurther, Relevancy Map or Hila-\nCAM (Chefer, Gur, and Wolf 2021) uses the self-attention\nand co-attention modules considering classification tokens\nappended during downstream tasks and associated values\nto generate a relevancy map tracking interactions between\ndifferent modalities and backpropagating relevancies.\n2022)\nis more like a tool that gives an interactive interface looking\nat interactions between modalities from a bottom-up per-\nspective.\nLower relevance is assigned to the background pixels, so\nthe foreground is considered with more confidence.\n2022) proposes the AttCAT explanation method that\nuses attentive class activation tokens built on encoded fea-\ntures, gradients, and attention weights to provide the expla-\nnation.\n2023) proposes an-\nother interpretation method called Vision DiffMask, which\nidentifies the relevant input part for final prediction using a\ngating mechanism.\nThe authors made several important observations: (i)\nthe pre-trained models attend to language more than vision,\nsomething that has been corroborated throughout the litera-\nture; (ii) there is a set of attention heads that capture cross-\nmodal interactions; and (iii) plotting attention can depict in-\nterpretable visual relations as was corroborated in the pre-\nvious section as well, among others.\n(Hendricks and Nematzadeh 2021)\nfurthermore proposes probing tasks for verb understanding\nby collecting image-sentence pairs with 421 verbs com-\nmonly found in the Conceptual Captions dataset (Sharma\net al.\nFurther, datasets are de-\nsigned carefully for multimodal probing, trying to reduce de-\npendency on bias while making predictions.\nHowever, recently,\nattention has been pointed out not to be a reliable parame-\nter for explaining a model\u00e2\u20ac\u2122s decision in some studies.\nFor\nVLPMs, in particular, fusing the modalities can make it dif-\nficult to interpret how the attention is distributed and how\nit should be explained.\nFurther, (Jain and Wallace\n2019) studied the relationship between attention weights and\nthe final decision for several NLP tasks and concluded that\nattention weights often do not relate to gradient-based meth-\nods for computing feature importance; hence, they do not\nprovide helpful or meaningful explanations.\nWhile these methods concluded that attention is not re-\nliable as a justification tool, the studies have been limited\nto language-based tasks and need a proper in-depth analysis\ngiven how heavily current methods rely on the mechanism\nto interpret the models.\n2022) studies attention under convex\nduality that can help provide interpretability for the archi-\ntecture.\nOne basic need is to make these models just as trust-\nworthy to ensure that their decisions can be trusted and re-\nlied upon while staying away from harmful biases like using\nfaithfulness tests for quantifying the model\u00e2\u20ac\u2122s explainability.\nTherefore, we should examine attention from three differ-\nent angles: its impact on model performance, its role in ex-\nplaining decisions, and its role in understanding the model\u00e2\u20ac\u2122s\nProbing the Vision Modality: The literature has time and\nagain iterated that for VLPMs, decisions have a stronger in-\nfluence from the language modality than the visual modality.\nWhile tasks like VQA\nhave recognized language bias, VLPM as a generalized ar-\nchitecture has not been explored for this bias as extensively.\nBetter pre-trained tasks aligning the vision modality along\nwith cross-modality interactions can be a way forward for\nimproving the generalization as well as the effect of the vi-\nsion modality on the entire model.\n2023)\nhave used one modality to guide the other while training or\n\fused one modality to train the multimodal models, which\ncan allow correcting for bias or adversarial vulnerabilities.\nA universal model that can represent both modalities\nsimilarly can help with performance as well as robustness,\nhowever, there is still a gap in getting universal pre-trained\nweights that can adapt to different modalities and require\nfurther research.\nTherefore, it is important to understand the crucial\nrole of Responsible Machine Learning Datasets (Mittal et al.\nIn addition,\nmachine unlearning concepts should be explored to ensure\nthese systems can adapt and comply with evolving regula-\ntory norms.\nDespite the remarkable human-like performance demon-\nstrated by Vision-Language Pre-trained Models (VLPMs)\nand Vision Transformers, it is of paramount importance not\nto underestimate the crucial dimension of trustworthiness.\nFirstly, we scrutinize biases\nwithin VLPMs, recognizing that while datasets often serve\nas the primary source of bias, biases can also seep into the\nmodels and algorithms themselves.\n"}, {"role": "assistant", "content": "Abstract\n\nRecently, transformers have become incredibly popular in\ncomputer vision and vision-language tasks. This notable rise\nin their usage can be primarily attributed to the capabilities\noffered by attention mechanisms and the outstanding ability\nof transformers to adapt and apply themselves to a variety of\ntasks and domains. Their versatility and state-of-the-art per-\nformance have established them as indispensable tools for a\nwide array of applications. However, in the constantly chang-\ning landscape of machine learning, the assurance of the trust-\nworthiness of transformers holds utmost importance. This\npaper conducts a thorough examination of vision-language\ntransformers, employing three fundamental principles of re-\nsponsible AI: Bias, Robustness, and Interpretability. The pri-\nmary objective of this paper is to delve into the intricacies\nand complexities associated with the practical use of trans-\nformers, with the overarching goal of advancing our compre-\nhension of how to enhance their reliability and accountability.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nIntroduction\nStable Diffusion (Rombach et al., 2022) has become increas-\ningly prominent in generating human images.\nHowever, a recurrent issue with this model is\nits tendency to produce inaccurate hand images, a problem\nwe define as the non-standard hand.\nThe concept of the uncanny valley (Mori, 1970) describes\na sense of unease or discomfort when humanoid figures\nclosely resemble humans but are not quite lifelike.\nIn fields like augmented reality (AR), virtual reality (VR),\nand gaming, where a seamless and realistic experience is\ncrucial, such anomalies in hand images can be particularly\nunsettling for users.\nAccurate hand representation is,\ntherefore, essential not just for visual appeal, but also for\nthe functionality and overall user satisfaction in interactive\nvirtual settings.\nBut the easier training of the\nDiffusion model is a big plus over GANs, which need adver-\nsarial training, and VAEs, which need a variational posterior.\nThe Stable Diffusion uses UNet architecture (Ronneberger\net al., 2015) from image segmentation and shows stable loss\nduring training and very good performance.\nOur contribution is a structured method that\ninitially identifies \u00e2\u20ac\u2122non-standard\u00e2\u20ac\u2122 hand depictions and sub-\nsequently corrects them to match the appearance of actual\nhuman hands, referred to here as \u00e2\u20ac\u0153standard hands.\nUtilizing a specialized dataset and the advanced capabilities\nof the YOLOv8 algorithm (LLC, 2023), our approach attains\nnotable accuracy in detection, with Figure 3 demonstrating\nan instance of the detection phase.\nUpon identifying anomalies, our system progresses to the\nrestoration phase.\nThe process involves a set of defined\n\fDetecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images\nFigure 1.\nBased on this skeleton, a template is accurately positioned over the non-standard hand to create the\n\u00e2\u20ac\u0153control image.\nUsing this\nunion mask, the control image, and a descriptive template prompt, we repair the area covered by the mask.\noperations: Body Pose Estimation, utilizing Google\u00e2\u20ac\u2122s Medi-\naPipe (Lugaresi et al., 2019) to determine the hand\u00e2\u20ac\u2122s position\nand movement; Control Image Generation, which provides\nthe Stable Diffusion model with directive images for better\nrestoration outcomes; ControlNet Inpainting, offering an ini-\ntial refinement based on the Control Image; and ultimately,\n\fDetecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images\n(a) Image with both standard and\nnon-standard-hand.\nAn illustration of this pipeline\nis depicted in Figure 4.\nIn this report, we present a solution that improves the image\nquality produced by Stable Diffusion, particularly the de-\npiction of hands.\nWe create a pipeline to rectify anatomical inaccuracies\nin hand images Our results demonstrate the effective-\nness in producing anatomically accurate and realistic\nhand images in outputs from Stable Diffusion.\nWe finetune a detection model to locate and classify\nstandard and nonstandard hands, and fine-tuned an In-\nstructPix2Pix model to make high-fidelity adjustments\nto the images.\nWe create a dataset featuring a diverse collection of\nhand images, both standard and nonstandard, to facili-\ntate comprehensive model training.\nStable Diffusion occasionally generates images with atypi-\ncal hands, defined as non-standard hands.\nOur method is to\nidentify these variations and then adjust them to resemble\nreal-world hands, defined as standard hands.\nThe following sections provide detailed\ninsights into each part of the process.\nThis\ndataset should encompass a wide range of images featuring\nhands.\nCrucially, every hand in these images requires an-\nnotation.\nWe begin with HaGRID (HAnd Gesture Recognition Image\nDataset) (Kapitanov et al., 2022) as our foundational dataset.\nThe pho-\ntos were taken indoors under different lighting conditions,\nproviding a rich assortment of visual details.\nThis diversity\ngrants our dataset extensive coverage and high levels of gen-\neralizability, enhancing our model\u00e2\u20ac\u2122s capability to process\nhands in a wide range of appearances.\nWe employed the Stable Diffusion\nmodel (Rombach et al., 2022) to recreate the hand areas,\nas outlined by their bounding boxes in HaGRID images.\nDue to certain limitations of the Stable Diffusion model,\nsome hands in the redrawn images were classified as non-\nstandard hands.\nFrom these redrawn images, we manually\nselected samples featuring non-standard hands, pairing each\nwith its corresponding original image, which depicts a stan-\ndard hand.\nTo enable a comprehensive evaluation of\nour model, we divided this data into training and testing\nsets.\nThis dataset is specifically prepared for detecting non-\nstandard hands.\nTo identify and classify hands as ei-\nther non-standard or standard, we finetuned the YOLOv8\nmodel using the training dataset described above.\nAfter\nimplementing the trained YOLO model, we annotate the\nbounding boxes around the hands and classify them.\nBody Pose Estimation\nEstimating body pose is crucial for determining the size po-\nsition, and chirality of hands in our images.\nNotably, its pose landmark detection feature is ca-\npable of detecting the human body skeleton.\nThis includes\na machine learning model skilled at identifying body land-\nmarks such as hands, elbows, shoulders, hips, and more in\nimages or videos, and their structural interconnections, as\nshown in Figure 8.\nControl Image Generation\nControlNet (Zhang et al., 2023) enhances the Stable Dif-\nfusion model by introducing additional input modalities,\nincluding key landmarks, depth maps, and edge maps.\nThis\nintegration directs Stable Diffusion to produce images with\nmore stable and realistic structures.\nThis\nimage, tailored to enhance Stable Diffusion\u00e2\u20ac\u2122s restoration of\nnon-standard hands, pushes the model to its limits and im-\nproves the output quality.\nWe select an appropriate Template based on the gesture\nand context of the image undergoing restoration.\nA background image, identical in size to the redrawn\nimage but entirely black, is prepared (see Figure 10a).\nThe Template is initially positioned at the top-left cor-\nner of this background (see Figure 10b).\nThe angle is calculated as:\n\fDetecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images\nFigure 7.\nFor a comprehensive view of the process, we compile\nsamples of the redrawn images, body skeleton, control\nimage, and union mask into a single visual representa-\ntion (see Figure 11c).\nThese prompts, familiar within the community for gener-\nating high-quality images, reflect prompt engineering do-\nmain knowledge.\nFollowing the initial\nrestoration with ControlNet, IP2P enhances the textures,\nfocusing on giving the hands a more realistic and authentic\nappearance to blend seamlessly with the rest of the image.\nInitially, the IP2P model is fine-tuned using our training set,\nwhich comprises 9623 pairs of images.\nEach pair includes a\nreal image with a standard hand from the HaGRID dataset\n\fDetecting and Restoring Non-Standard Hands in Stable Diffusion Generated Images\nFigure 12.\nFor restoration, the fine-tuned model is then applied.\n"}, {"role": "assistant", "content": "Abstract\n\nWe introduce a pipeline to address anatomical\ninaccuracies in Stable Diffusion generated hand\nimages. The initial step involves constructing\na specialized dataset, focusing on hand anoma-\nlies, to train our models effectively. A finetuned\ndetection model is pivotal for precise identifica-\ntion of these anomalies, ensuring targeted correc-\ntion. Body pose estimation aids in understanding\nhand orientation and positioning, crucial for ac-\ncurate anomaly correction. The integration of\nControlNet and InstructPix2Pix facilitates sophis-\nticated inpainting and pixel-level transformation,\nrespectively. This dual approach allows for high-\nfidelity image adjustments. This comprehensive\napproach ensures the generation of images with\nanatomically accurate hands, closely resembling\nreal-world appearances. Our experimental results\ndemonstrate the pipeline\u00e2\u20ac\u2122s efficacy in enhancing\nhand image realism in Stable Diffusion outputs.\nWe provide an online demo at fixhand.yiqun.io.\n\n1. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nWhile\ngenerative AI is currently ubiquitous in the scien-\ntific literature and public debate, it has not (yet)\nreplaced discriminative AI for information extrac-\ntion tasks like NER.\nPopular open source frameworks, like\nthe ones provided by HuggingFace (Wolf et al.,\n2020; Lhoest et al., 2021; Von Werra et al., 2022),\ngreatly facilitate the use of such models.\nPrivate datasets\nmay be stored on local filesystems or be created\nusing annotation tools.\n(ii) Data for NER is processed on three differ-\nent levels: tokens, words and entities.\nAt\ntraining time, labels for tokens that are not the first\ntoken of a word may be ignored (word level) or\nincluded (token level) in the computation of the\nloss.\nFinally, while model predictions are often\nmade on the entity level, some use cases may re-\nquire predictions on the word level, for instance if\nthe associated probabilities are to be used for ac-\ntive learning.\n(iii) There exists a multitude of NER-specific an-\nnotation schemes and variants and it is important\nto be aware of the differences.\nFor instance, during\ndata preprocessing, existing word or entity labels\nneed to be mapped to token labels, which is an an-\nnotation scheme dependent process.\nAt evaluation\ntime, there are different ways to cope with predic-\ntions that do not obey the rules of the given annota-\ntion scheme (we will get back to this in Sec.\nFor instance, while a\nsmall dataset often requires more training epochs,\nlarger datasets can usually be trained for fewer\nThe aim of nerblackbox is to provide a high-level\nframework which makes the usage of SOTA NER\nmodels as simple as possible.\n3, it offers easy access to datasets from\nvarious sources, automated training and evaluation\nas well as simple but versatile model inference.\nIt\ndoes so by hiding all technical complications from\nthe user2 and is targeted at developers as well as\npeople who are not necessarily experts in machine\nlearning or NLP.\nThis might\nmake the library appealing also for researchers and\n2 Related Work\ncommonly used framework for\ntransformer-based NLP is arguably the Hugging-\nFace ecosystem, in particular the open source\nlibraries transformers (Wolf et al., 2020), datasets\n(Lhoest et al., 2021) and evaluate (Von Werra\net al., 2022).\nHigh-level libraries that are build on top of trans-\nformers exist in the form of Simple Transformers\n(Rajapakse, 2019) and T-NER (Ushio and Camacho-\nCollados, 2021).\nT-NER is specific to\n2This is where the name nerblackbox stems from: The\nframework does not require any knowledge about internal\nprocesses and can be used as a black box by only specify-\ning inputs (pretrained model, dataset) and using the outputs\n(fine-tuned model).\nNote that there is no direct relation to\nexplainability.\nHowever, as will be discussed in the following\nsections, nerblackbox offers many unique and pow-\nerful features that\u00e2\u20ac\u201dto the best of our knowledge\u00e2\u20ac\u201d\nmake it distinct from any existing frameworks.\nIt does so\nin terms of the following classes:\n1 >> from nerblackbox import Dataset ,\nTraining , Model\nA high-level overview of the involved compo-\nnents is shown in Fig.\nSome datasets are pretok-\nenized and split into training/validation/test subsets,\nwhile others are not.\nThe set_up() method auto-\nmatically deals with these challenges and makes\nsure that every dataset, irrespective of the source,\nis transformed into a standard format4.\n3Currently, the two commonly used (open source) annota-\ntion tools LabelStudio (Tkachenko et al., 2020) and Doccano\n(Nakayama et al., 2018) are supported.\n3.2 Training\nIn order to train a model, one only needs to choose\na name for the training run (for later reference) and\nspecify the model and dataset names, like so:\nSimilar to evaluation, both NER models trained\nusing nerblackbox and models taken directly from\nHuggingFace (HF) can be used for inference.\nrun ()\nIn order to ensure stable results irrespective of\nthe dataset, the training employs well-established\nhyperparameters by default (Mosbach et al., 2021).\nIn particular, a specific learning rate schedule (Stol-\nlenwerk, 2022) based on early stopping and warm\nrestarts (Loshchilov and Hutter, 2017) is used to\naccommodate different dataset sizes.\n3.3 Evaluation\nAny NER model, whether it was trained using\nnerblackbox or is taken directly from Hugging-\nFace (HF), can be evaluated on any dataset that\nis accessible via nerblackbox (see Sec.\nAll metrics are determined\nboth on the entity and word level.\nIn ad-\ndition, a model can be applied directly to a file\ncontaining raw data, which may be useful for infer-\nence at large scale (e.g.\n4 Advanced Usage\nThe nerblackbox workflow and the API are de-\nsigned to be as simple as possible and to con-\nceal technical complications from the user.\nHow-\never, they are also highly customizable in terms\nof optional function arguments, which may be par-\nticularly interesting for machine learning experts\nand researchers.\nIn this section, we are going to\ncover a non-exhaustive selection of nerblackbox\u00e2\u20ac\u2122s\nadvanced features, with a slight emphasis on the\ntraining part.\nIn par-\n\fticular, all aspects of the learning rate schedule\n(e.g.\nAmong them\nare the learning rate schedules from (Devlin et al.,\n2019) and (Mosbach et al., 2021), which may work\nwell for larger and smaller datasets, respectively.\n4.2 Dataset Pruning\nnerblackbox provides the option to only use a sub-\nset of the training, validation or test data by spec-\nifying parameters like train_fraction .\nThis may\nbe useful to accelerate the training (for instance\nin the development phase of a product) or if one\nwants to investigate the effect of the dataset size\n(for instance to see if the model has saturated, or\nfor research).\n4.3 Annotation Schemes\nWhile every dataset is associated with a certain an-\nnotation scheme, nerblackbox provides the option\nto translate between schemes at training time.\nThis\nmay be interesting for users who aim to optimize\ntheir model\u00e2\u20ac\u2122s performance as well as researchers\nwho systematically want to investigate the impact\nof the annotation scheme.\n4.4 Multiple Runs\nSince the training of a neural network includes\nstochastic processes, the performance of the result-\ning model depends on the employed random seed.\nIn order to gain control over the associated statis-\ntical uncertainties, one may train multiple models\nusing different random seeds.\nWith nerblackbox,\nthis can trivially be done by setting the training\nparameter multiple_runs to an integer greater than\n1.\nIn that case, the evaluation metrics will be given\nin terms of the mean and its associated uncertainty.\n4.6 Careful Evaluation\nA model may predict labels for a sequence of to-\nkens that are inconsistent with the employed anno-\ntation scheme.\nWhen translated to entity predictions,\nnerblackbox ignores incorrect labels by default,\nboth at evaluation and inference time.\nHowever,\nthe popular evaluate (Von Werra et al., 2022) and\nseqeval (Nakayama, 2018) libraries do take incon-\nsistent predictions into account during evaluation.\n4.7 Compatibility with transformers\nnerblackbox is heavily based on transformers (Wolf\net al., 2020) such that compatibility is guaranteed.\nIn particular, the Model class has the attributes\ntokenizer and model , which are ordinary trans-\nformers classes and can be used as such.\nIt includes a pedagogical introduction\nto the library, an in-depth discussion of its fea-\ntures as well as docs for the python API.\nConsis-\ntent code syntax and typing are ensured by usage\nof black9 and mypy10, respectively.\nAs an additional cross-\ncheck, numerical results from the literature are re-\nproduced using nerblackbox (details can be found\nin the documentation).\nIn Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 53\u00e2\u20ac\u201c62, Online.\nIn Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 128\u00e2\u20ac\u201c136, Abu Dhabi,\nUAE.\n"}, {"role": "assistant", "content": "Abstract\n\nWe present nerblackbox, a python library to\nfacilitate the use of state-of-the-art transformer-\nbased models for named entity recognition. It\nprovides simple-to-use yet powerful methods\nto access data and models from a wide range of\nsources, for fully automated model training and\nevaluation as well as versatile model inference.\nWhile many technical challenges are solved\nand hidden from the user by default, nerblack-\nbox also offers fine-grained control and a rich\nset of customizable features. It is thus targeted\nboth at application-oriented developers as well\nas machine learning experts and researchers.\n\n1\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nImportantly, these interactions are largely initiated and con-\ntrolled by the developing child.\nFor example, an infant controls\nits visual, proprioceptive, and haptic inputs through its eye\nand body movements.\nThis ability to control and \u00e2\u20ac\u0153experiment\u00ef\u00bf\u00bd\nwith different movements to observe their effects on sensory\ninputs may be crucial for cognitive development, giving the\ndeveloping mind a means to probe the causal structure of\nthe world, rather than merely passively observing correlations\namong sensed variables.\nImportantly, such an active, self-controlled form of learning\nis in stark contrast\nto a large body of work in Artificial\nIntelligence (AI), including large language or vision models,\nwhich learn certain aspects of the statistical structure of large\ntraining datasets without having any control over their inputs.\nDespite the impressive successes of such models, they may\nultimately be severely limited in their ability to understand\nthe causal mechanisms underlying their training data, which\nwill limit their ability to generalize in novel situations.\nTherefore, scientists routinely exploit the ability to\nmanipulate a system under study using clever experimental\ndesigns where they interfere with some variables and observe\nthe effects on others to distill the causal mechanisms at work.\nTherefore, recreating the physical interaction with the envi-\nronment must be a central aspect of rebuilding cognitive devel-\nopment and it may be equally essential for achieving human-\n\flike intelligence and consciousness in AIs.\nThe idea that an\nAI could learn like a developing child can be traced back\nall the way to Turing [2].\nHowever, serious \u00e2\u20ac\u0153Developmental\nRobotics\u00ef\u00bf\u00bd or \u00e2\u20ac\u0153Developmental AI\u00ef\u00bf\u00bd efforts have become more\ncommon and practical only in the last 20 years, for reviews\nsee [3]\u00e2\u20ac\u201c[8].\nThere are two options for modeling the interaction of a\ndeveloping mind with its physical and social environment.\nThe main\nadvantage is the inherent realism: the system operates in the\nreal world governed by the actual laws of physics.\nFurthermore, the sensing abilities of today\u00e2\u20ac\u2122s robots\nare usually not comparable to those of actual humans.\nThe second option for modeling the interaction of the\ndeveloping mind with its physical and social environment is\nto do it completely in silico.\nMany physics simulators or game\nengines are available today that can approximate the physics\nof such interactions [9]\u00e2\u20ac\u201c[11], for review see [12].\nAmong\nthe disadvantages of such an approach are 1) inaccuracies\nof such simulations due to inevitable approximations and\n2) the high computational costs, especially when non-rigid\nbody parts and objects are considered.\nNevertheless, the in\nsilico approach avoids all\nthe problems of working with\nhumanoid robot hardware mentioned above.\nLastly, if simulations can be\nrun (much) faster than real-time, this greatly facilitates the\nsimulation of developmental processes unfolding over long\nperiods of time (weeks, months, or even years).\nTo support such in silico research, we here present the\nopen source software platform MIMo, the Multi-Modal Infant\nModel (Fig.\nMIMo is intended to support\nof research: 1) developing computational models of human\ncognitive development and 2) building developmental AIs that\ndevelop more human-like intelligence and consciousness by\nsimilarly exploiting their ability to probe the causal structure\nof the world through their actions.\nMIMo also features different facial expressions that can be\nused for studies of social development.\nTo simulate MIMo\u00e2\u20ac\u2122s\ninteraction with the physical environment we use the MuJoCo\nphysics engine [13], because of its strength at simulating\ncontact physics with friction.\nGenerally, we have aimed for\na balance between realism and computational efficiency in the\ndesign of MIMo.\nTo accelerate the simulation of physics and\ntouch sensation, MIMo\u00e2\u20ac\u2122s body is composed of simple rigid\nshape primitives such as a sphere for the head and capsules\nfor most other body parts.\nThe remainder of this article describes the detailed design\nof MIMo and illustrates how to use it.\nIn particular, we present\nfour scenarios where MIMo learns to 1) reach for an object,\n2) stand up, 3) touch different locations on his body, and 4)\ncatch a falling ball with his five-fingered hand.\nThese examples\nare used to illustrate and benchmark potential uses of MIMo.\nThey are not intended as faithful models of how infants acquire\nthese behaviors.\nThis paper is an extended version of preliminary work pre-\nviously published at the ICDL 2022 conference [1].\nCompared\nto the preliminary MIMo version we have added 1) five-\nfingered hands, 2) a new actuation model that more accurately\nmodels the force-generating behaviour and compliance of real\nmuscles, 3) a detailed play room (see Fig.\nA fifth contribution is that we have\nimproved computational efficiency and benchmarks have been\nupdated accordingly.\nRELATED WORK\nWe focus on two major classes of software platforms for\nsimulating cognitive development during embodied interac-\ntions with the environment.\nThe first kind is designed to\nsimulate a particular physical robot used in Developmental\nRobotics research and intended to complement the work with\nthat physical robot.\nThe second kind of platform emulates human body and\nsensing abilities directly and thus is not restricted by limi-\ntations of current robotics technology in general or that of\nspecific robots in particular.\nEXPERIMENTS\nA. Illustrations of learning\nWhile RL is a powerful framework to generate controllers,\nits chaotic nature and requirements for large amounts of data\nrequire a stable and efficient model.\nNote that badly designed mod-\nels can negatively affect RL performance, even if they are\nphysically accurate in some regimes.\nIn these tasks,\nstate-of-the-art, widely used deep RL algorithms, Proximal\nPolicy Optimization (PPO) [54] and Soft Actor-Critic (SAC)\n[23] are trained to optimize task-dependent reward functions\nby controlling MiMo\u00e2\u20ac\u2122s actuators.\nWe use the default parameters of the Stable-Baselines3\nlibrary [53], consisting of linear networks with two hidden\nlayers of size 64 for PPO and size 256 for SAC, as well\nas common improvements [57], [58] that are critical for\nperformance.\nWe merely use these examples to showcase MIMo learning\nfrom multimodal input.\n(b) Visualization of touch sensors in the hand, with\nthose reporting a force colored red.\nThe size of the circle corresponds to the\namount of force sensed.\nTouch Perception\nHuman touch sensation is produced by a variety of receptors\nresponding towards specific aspects of touch, such as the\nSlowly Adapting type 1 (SA1), which responds primarily to\ndirect pressure and coarse texture or the Rapidly Adapting\n(RA) type for slip and fine texture [50].\nAs with the other\nmodalities, we model these types very simply, ignoring signal\ntravel times and condensing the various types of receptors into\na single generic \u00e2\u20ac\u0153touch\u00ef\u00bf\u00bd sensor type that measures normal and\nfrictional forces at its location.\nSensor points are spread evenly\non each individual body part, with the sensor density varying\nbetween body parts based on the two-point discrimination\ndistances by [51].\nArea contacts between\nflat surfaces produce multiple contact points, for example at\nthe corners of the contact area.\nAs soft-body physics is very\ncomputationally expensive, we do not adjust these physics but\nweakly simulate the deformation of the skin to these point\ncontacts, by distributing the point contact forces over nearby\nsensor points according to a surface response function.\nOur\nresponse function decreases sensed force linearly with distance\nfrom the contact point and then normalizes over all sensor\npoints, such that the total sensed force remains identical to\nthe initial point contact force from MuJoCo.\nTo avoid bleed-through\nto the opposite side of thin bodies (such as the palm), we only\nselect nearby sensor points with a breadth-first-search on the\nmesh of sensor points.\nWhile also expensive, the results of this\nsearch can be cached and reused, leading to a only a small run\ntime penalty compared to using just the euclidean distance.\nAll of these aspects, from the sensor point density through\nthe response function can be easily adjusted or expanded.\nIn addition to adjusting the configuration of\nthe modalities or the scene, the performance of the simulation\ncan be improved by reducing the frequency of control steps,\nreducing the frequency of observation collection and thus the\ntime spent in the sensory modules.\n"}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dHuman intelligence and human consciousness\nemerge gradually during the process of cognitive development.\nUnderstanding this development is an essential aspect of un-\nderstanding the human mind and may facilitate the construc-\ntion of artificial minds with similar properties. Importantly,\nhuman cognitive development relies on embodied interactions\nwith the physical and social environment, which is perceived via\ncomplementary sensory modalities. These interactions allow the\ndeveloping mind to probe the causal structure of the world. This\nis in stark contrast to common machine learning approaches, e.g.,\nfor large language models, which are merely passively \u00e2\u20ac\u0153digesting\u00e2\u20ac\ufffd\nlarge amounts of training data, but are not in control of their\nsensory inputs. However, computational modeling of the kind\nof self-determined embodied interactions that lead to human\nintelligence and consciousness is a formidable challenge. Here\nwe present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simula-\ntions. MIMo\u00e2\u20ac\u2122s body is modeled after an 18-month-old child with\ndetailed five-fingered hands. MIMo perceives its surroundings via\nbinocular vision, a vestibular system, proprioception, and touch\nperception through a full-body virtual skin, while two different\nactuation models allow control of his body. We describe the design\nand interfaces of MIMo and provide examples illustrating its use.\nAll code is available at https://github.com/trieschlab/MIMo.\n\nIndex Terms\u00e2\u20ac\u201dcognitive development, developmental AI, infant\n\nmodel, multimodal perception, physics simulation\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nLong-term forecasting is carried out one or several seasons\nin advance to enable advance planning, which is essential in\nthe industrial sector (Harsem et al.\nIn predicting the\ntiming of opening and freezing of the water area, statistical\nmodeling is carried out based on retrospective observational\ndata (Wu et al.\nPhysical modeling became the classical approach to the\nsimulation of natural environments because of advantages\non the spatial and temporal scales and the ability to predict\nand analyze different scenarios (Chen et al.\nIt utilizes\ncomplex numerical methods to solve the equations of mo-\ntion, thermodynamics, and radiation (Stroeve et al.\nYears of active development of earth remote sens-\ning have contributed to the accumulation of spatiotemporal\ndatasets for environmental systems, which can be used to\ntrain various data-driven models, including deep neural net-\nworks.\n2023), the computa-\ntional cost for models\u00e2\u20ac\u2122 training to satisfy the task-specific\nrequirements (water area, time and spatial resolution, fore-\ncast length, objective function) can be excessive.\nFor this\nreason, we aimed to provide a more lightweight surrogate\nalternative to existing solutions for sea ice forecasting.\nThis paper presents a deep learning approach LANE-SI\nthat uses an ensemble of convolutional neural networks\n(CNN) for long-term predictive modeling of sea ice concen-\ntration.\nThe de-\nveloped model is lightweight due to its simple architecture\nand is also undemanding in input data - only the pre-history\nof the target parameter is required.\nThe compar-\nison results demonstrate that the surrogate model reproduces\nthe absolute values of ice concentration and their spatial dis-\ntribution with a quality comparable to the SEAS5 forecast.\nThe surrogate model reproduces the position of the ice edge\nbetter than the SEAS5 forecast, both on average contour\npoints position and the position of each point separately.\nThe paper is structured as follows: Section 2 reviews the\nkey related work, Section 3 describes the proposed method\nfor the design of surrogate sea ice forecasting models named\nLANE-SI, Section 4 contains the experimental results, and\nSection 5 presents concluding remarks for the proposed\nRelated Work\nThe classic method for modeling sea ice parameters is nu-\nmerical modeling based on systems of differential equa-\ntions.\nOver the years of technological development, a large\n\fnumber of both specific solutions that reproduce local ice\ndynamics (Girard et al.\n2011) and general methods of nu-\nmerical hydrometeorological forecasting (Wang, Chen, and\nKumar 2013; Zhang and Rothrock 2003) have been devel-\noped.\nConsidering the universality\nof numerical models, it should be noted that their adapta-\ntion to a specific territory requires significant effort from a\nsubject specialist and considerable computing resources, as\nwell as setting boundary and initial conditions, which raises\nthe need for high-quality initial data, which is not always\navailable for the water area interest.\nRegression models are limited in their applica-\nbility due to the enormously increasing complexity when\nmoving to grid calculations.\n2020) and more com-\nplex U-Net (Ali and Wang 2022) architectures provide high-\nresolution output images and allow a more detailed fore-\ncast.\nIt is known that combining a set of simple models into\nensembles makes it possible to achieve higher quality fore-\ncasting, including in the subject area of hydrometeorology\n(Ali et al.\nTo achieve a better quality of high-resolution long-term sea\nice forecasting in a computationally cheap way, we devel-\noped the LANE-SI approach that allows designing the pre-\ndictive model for sea ice in specific water areas.\nIn contrast to LSTM architectures,\nwhich allow the extraction of features only from time series\n(Choi, De Silva, and Yamaguchi 2019), convolution empha-\nsizes the spatial distribution of the parameter and reduces\nthe risk of stagnation over time.\nThe surrogate model was\nimplemented to carry out long-term forecasting with a fore-\ncast horizon of one year.\nWe aimed to make the model as\nflexible as possible to adapt to specific water areas to reduce\nthe computational cost against neural surrogate models that\ncover the entire Arctic region (Andersson et al.\nIn LANE-SI, the dataset for training a surrogate data-\ndriven predictive model for ice concentration is prepared\nsimilarly to the lagged transformation applied to time series.\nThe surrogate model is represented by an ensemble that\naggregates two CNNs trained with different loss functions\nand an inertial forecast - concentration values averaged for\neach day of the year over the previous five years.\nCNN for sea ice forecasting\nThe basic structure of the deep neural networks used in\nLANE-SI include five convolution layers with ReLU acti-\nvation functions (Agarap 2018).\nThe structural scheme of CNN is presented in Figure 1.\nTherefore, CNNs were trained on\ntwo loss functions:\n\u00e2\u20ac\u00a2 Mean Absolute Error (MAE, L1Loss) - represents the\ncloseness of the absolute values of each pixel of each pre-\ndiction element with the target image;\n\u00e2\u20ac\u00a2 Structural Similarity Index (SSIM) - represents the\nsimilarity local patterns of pixel intensities (Wang et al.\n2004) and shows similarity in the spatial distribution of\nthe parameter at predicted and target images.\nThe CNN accepts multi-channel\nimages with several\nyears of ice concentration pre-history.At the output, the\n54\u00d1\u20265427\u00d1\u20262754\u00d1\u202654Input: multi-channel image, prehistory of parameter \u00e2\u20ac\u201cn time stepsOutput: multi-channel image,forecast of parameter \u00e2\u20ac\u201ck time steps ahead\fCNN provides images of the same spatial resolution as the\ninput ones, but the number of channels is 52.\nIt corresponds\nto a one-year forecast with a time sampling of seven days.\nThe first CNN reflects\nthe spatial distribution of the parameter (SSIM), and the\nsecond reproduces the parameter\u00e2\u20ac\u2122s absolute values (MAE).\nAlso, a naive forecast in the form of repeating the average\nvalues for five years for each day of the year was added to re-\nproduce long-term dynamics not represented in pre-history.\nPredicted values for each\npixel of three separated models are summed with coeffi-\ncients that provide less error on the delayed part of train\n\u00e2\u20ac\u00a2 CNN trained with MAE loss function uniting the ele-\nments of the ensemble in the form of multi-channel im-\nages into one prediction;\n\u00e2\u20ac\u00a2 CNN trained with SSIM loss function unites the ensem-\nble\u00e2\u20ac\u2122s elements in the form of multi-channel images into\none prediction.\nThe general scheme for an approach that combines sin-\ngle models into using neural ensembling is presented in Fig-\nExperiments and Results\nWe use OSI SAF Global Sea Ice Concentration (SSMIS)\nproduct as training data for sea ice concentration forecasting\nin specific water areas.\nThe time resolution of the product\nis one day; however, for the problem of long-term forecast-\ning, such discretization is redundant, and therefore the time\nresolution was sparsed to weekly.\nAn area with the Kara Sea and part of\nthe Barents Sea was chosen as a test domain (as presented in\nWe conduct the experimental evaluation of LANE-SI for\nthe Kara Sea using SSMIS for 1996 to 2022 years.\nThe ensembling model was trained on predic-\ntions of single models from 2010 to 2015.\nThey were ad-\nditionally re-trained on the entire training set from 1996 to\n2015 to obtain better forecasts using single models.\nLANE-SI against separate data-driven models\nThe quality of the surrogate model (design with LANE-SI)\nfor the Kara Sea was analyzed compared to the naive fore-\ncast for both the final ensemble model and individual en-\nsemble components.\nAs the table shows,\nthe surrogate ensemble model\nachieves a higher forecast quality than single models accord-\ning to the SSIM metric for all years and almost all years ac-\ncording to the MAE metric.\nFigure 4 presents an example of a spatial comparison of\ndifferent data-driven forecasts (LANE-SI-designed ensemble\nand separate neural models) with actual data for a single\nforecast step.\nThe CNN with the L1 loss function reproduces the ice field\nwith large granularity despite its good fit into the absolute\nvalues on average over the image.\nCNN with SSIM loss\nfunction and naive forecast tends to over-smooth the im-\nage and underestimate the absolute values of the parame-\nter.\nThe maximum forecast horizon of the SEAS5 model is\nnine months (against 12 months for the design surrogate\nmodel).\nThe error was calculated on average\nfor each season (three months) out of three fully predicted.\nAs the table shows, forecasting using a surrogate model\ndesigned by LANE-SI is comparable in quality to the\nphysics-based forecast and, in some cases, can even surpass\nit due to the better adaptation to specific water areas.\nAn expert can choose the threshold for switching to binary\ndata (it depends on the scenario of the forecast usage).\nThe distance metric is\ncalculated as a mean of the signed distance between each\npoint of one contour and the nearest contour edge of the sec-\nond contour (Brahmbhatt 2013).\nFigure 5 shows an example of the result of edge identifi-\ncation as 100 points with the distance metric calculation.\nFigure 6 provides a visualization for\nthe 2021 ice melt period.\n"}, {"role": "assistant", "content": "Abstract\n\nThe modeling and forecasting of sea ice conditions in the\nArctic region are important tasks for ship routing, offshore\noil production, and environmental monitoring. We propose\nthe adaptive surrogate modeling approach named LANE-SI\n(Lightweight Automated Neural Ensembling for Sea Ice) that\nuses ensemble of relatively simple deep learning models with\ndifferent loss functions for forecasting of spatial distribution\nfor sea ice concentration in the specified water area.\nExperimental studies confirm the quality of a long-term fore-\ncast based on a deep learning model fitted to the specific water\narea is comparable to resource-intensive physical modeling,\nand for some periods of the year, it is superior. We achieved a\n20% improvement against the state-of-the-art physics-based\nforecast system SEAS5 for the Kara Sea.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nRaising temperatures,\nintensifying ultraviolet radiation, and reducing relative humidity, are expected to increase pest\noutbreaks and undermine the efficacy of pest control methods like host-plant resistance, bio-pesticides,\nand synthetic pesticides [37, 41].\nPervasive pesticide use in agriculture, despite climate experts\u00e2\u20ac\u2122\nwarnings, adversely affects public health [9] and contributes to the climate crisis.\nThis impact includes:\ni) greenhouse gas (GHG) emissions from pesticide production, packaging, and transportation [6], ii)\ncompromised soil carbon sequestration [48], iii) elevated GHG emissions from soil [27, 18, 42], and\niv) contamination of adjacent soil and water ecosystems, resulting in biodiversity loss [35].\nIn response,\nthe European Commission (EC) has taken action for the reduce of all chemical and high-risk pesticides\nby 50% by 2030.\nAchieving such reductions requires adopting integrated pest management (IPM),\nwhich promotes sustainable agriculture and agroecology.\nThe authors in [8] condense these principles\n\u00e2\u02c6\u2014Equal contribution.\nTackling Climate Change with Machine Learning: workshop at NeurIPS 2023.\n\finto prevention and suppression, monitoring, decision-making, non-chemical methods, pesticide\nselection, reduced pesticide use, anti-resistance strategies, and evaluation.\nData-driven methods have played a crucial role in optimizing pest management decisions.\nSome\nemploy supervised machine learning (e.g., Random Forests, Neural Networks) with satellite Earth\nobservations (EO) and in-situ data for pest presence prediction [49, 4], some incorporating weather\ndata [40].\nRecurrent Neural Networks (RNNs) are used to capture temporal features from weather\ndata, effectively handling unobservable counterfactual outcomes [47].\nextract fine-scale\nIPM information from meteorological data, insect scouting, remote sensing and machine learning\n[21].\npropose an interpretable machine learning solution for Helicoverpa armigera\npresence in cotton fields [30], enhancing IPM decision-making beyond traditional thresholds.\nThere\nis a need for a new class of digital tools that take into account the entire set of IPM principles in\norder to truly enhance decision-making.\nIn this direction, we propose a data analysis framework for\nIPM based on causality and explainability.\nIt consists of short-term actionable advice for in-season\ninterventions and long-term advice for supporting strategic farm planning (Figure 1).\nThis way, we will upgrade the monitoring and decision-making IPM principles leading to actionable\nadvice for direct pest control interventions and assist the selection of practices relevant to other IPM\nprinciples, such as use non-chemical methods and reduce pesticide dosage.\nAdditionally, the proposed\nframework will better inform farmers with respect to the potential impact of practices that, in turn,\nwill enhance the IPM principle of prevention and suppression, e.g., crop rotation, day of sowing,\nand no-tillage.\nFurthermore, our framework employs observational causal inference to continuously\nassess the aforementioned recommendations and thereby satisfy the IPM principle of evaluation.\n3 Approach & Methods\nCausal Graph for representing domain knowledge.\nWe constructed a causal graph (Figure 2),\ndenoted as G, that represents the underlying causal relationships within the pest-farm ecosystem\nfor the H. armigera case.\nThe graph G comprises vertices V , which represent the variables in the\nsystem, and directed edges E, which symbolize the cause-and-effect relationships between these\nvariables (details about graph building in A.2 of Appendix).\nBesides helping us articulate domain\nknowledge, the causal graph G will benefit the downstream technical analyses in various ways.\nG\nP1\u00e2\u20ac\u201dprevention and suppressionP2\u00e2\u20ac\u201dmonitoringP3\u00e2\u20ac\u201ddecision-makingP4-7 \u00e2\u20ac\u201d interventions for pest controlP8\u00e2\u20ac\u201devaluationimpact assessment of recommendationsATE estimation using observational causal inferencetime within seasonPrinciples of Integrated Pest Management (IPM)DATAlong-term adviceheterogeneouse\u00ef\u00ac\u20acect estimation using CATE short-term adviceexplainable, invariant  pest population forecastcounterfactuals \fserves as an amalgamation of domain knowledge and a foundational schema that can be leveraged\nvariably depending on the methodological requirements of the analytical techniques in play.\nOur goal is to predict near-future\npest populations (Yt+1) using EO and environmental data (Xt) and weather forecasts (Wt+1) by\nlearning the function yt+1 = f (xt, wt+1).\nWe turn to causal learning\n[34], grounded in independent causal mechanisms that remain stable despite environmental changes.\nTo achieve this, we integrate invariant learning with causality, categorizing data into environments\nE (e.g., agroclimatic zones).\nInvariant Causal Prediction (ICP) [19], DAGs, and Invariant Risk Minimization (IRM)\n[5] help select causal features, identify potential relationships, and capture latent causal structures.\nExplainability & Counterfactual Reasoning for Short-term Advice.\nWe define the problem as a\nbinary classification of pest presence or absence, at the next time step, using EO data (Xt) and weather\nforecasts (Wt+1).\nEBM\u00e2\u20ac\u2122s additive model allows visualization of feature\ncontributions, enhancing trust.\nTo bolster trust, we propose generating counterfactual examples as\nrecommended interventions.\nWe follow the setup of [29], searching for minimal feature perturbations\nin (xt, wt+1) that alter predictions using the same model f .\nThese counterfactual examples represent\nproposed actions for real farm systems, ensuring practicality and feasibility [45, 29].\nWe provide long-term advice for pest\nprevention and suppression by assessing how practices (e.g., crop rotation, balanced fertilization,\nsowing dates) affect pest harmfulness and yield indices.\nDifferent agro-environments may yield\nvarying responses to the same practice.\nWe estimate the conditional average treatment effect (CATE)\n[17] using the potential outcomes framework [33].\nCATE quantifies the difference in potential\noutcomes (E[Y (T = 1) \u00e2\u02c6\u2019 Y (T = 0)|X]), controlling for field characteristics that drive heterogeneity.\nWe employ causal inference techniques\nto assess the effectiveness of our pest control recommendations, building on a recent approach\nintroduced in the context of cotton farming [44].\nAdapting this method to pest control interventions,\nwe turn to difference-in-differences [1].\nOur aim is to quantify the average treatment effect of\nadhering to our framework\u00e2\u20ac\u2122s recommendations (treated units) compared to those who did not (control\nunits).\nHistorical intervention data, annotated as recommended or not, will be used for the evaluation.\nCausal inference will be conducted on a per-environment basis, ensuring similarity between treatment\nand control groups, following the parallel trends assumption [23].\nDepending on data volume and\ntime series length, other methods like synthetic control or panel data may also be considered.\n4 Conclusions\nBreaking the harmful cycle between pesticides and climate change is essential.\nIn this direction,\nIPM aims to successfully control pests while minimizing the adverse effects of conventional pest\nmanagement on human health and the environment.\nWe propose an AI-driven framework for IPM that\nprovides short- and long-term advice, promoting sustainable practices and timely control methods.\nFinally, our approach ensures effective pest control and enhances trust and transparency.\nAcknowledgments and Disclosure of Funding\nWe express our gratitude to Corteva Agriscience Hellas, particularly to Dr. George Zanakis, the\nMarketing & Development Manager, for their invaluable support, trust, and provision of data.\nThis\nresearch was primarily funded by the \"Financing of Charalambos Kontoe\u00e2\u20ac\u2122s Research Activities_code\n8003\" under Special Reseacrh Account of National Observatory of Athens.\nVasileios Sitokonstantinou and Gustau Camps-Valls\nwork has been supported by the GVA PROMETEO project \"Artificial Intelligence for complex\nsystems: Brain, Earth, Climate, Society\" agreement CIPROM/2021/56.\n"}, {"role": "assistant", "content": "Abstract\n\nPesticides serve as a common tool in agricultural pest control but significantly\ncontribute to the climate crisis. To combat this, Integrated Pest Management\n(IPM) stands as a climate-smart alternative. Despite its potential, IPM faces low\nadoption rates due to farmers\u00e2\u20ac\u2122 skepticism about its effectiveness. To address this\nchallenge, we introduce an advanced data analysis framework tailored to enhance\nIPM adoption. Our framework provides i) robust pest population predictions\nacross diverse environments with invariant and causal learning, ii) interpretable\npest presence predictions using transparent models, iii) actionable advice through\ncounterfactual explanations for in-season IPM interventions, iv) field-specific\ntreatment effect estimations, and v) assessments of the effectiveness of our advice\nusing causal inference. By incorporating these features, our framework aims\nto alleviate skepticism and encourage wider adoption of IPM practices among\nfarmers.\n\n1\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nHere, we present PCoQA, an innovative and exten-\nsive Persian Conversational Question Answering\ndataset tailored explicitly for Question Answering\n1https://github.com/HamedHematian/PCoQA\nin Context, drawing inspiration from two influen-\ntial predecessors, CoQA (Reddy et al., 2019) and\nQuAC (Choi et al., 2018).\nTo this end,\nlike CoQA, both questioner and responder have ac-\ncess to the document in order to control the rate of\nunanswerable questions.\nSince questioner\u00e2\u20ac\u2122s acces-\nsibility to documents increases the odds of string\nmatching and paraphrasing questions (Choi et al.,\n2018), two further measures are taken to diminish\nthe phenomena.\nFirst, the questioner is informed to\nask questions that do not contain lexical matching,\nand second, in the post-processing stage, questions\nthat contain a high level of lexical overlap with the\nsentence containing the answer, are paraphrased to\nensure the quality of the dataset.\nThis characteristic is fur-\nther compounded by the inclusion of abstract\ntopics (since we don\u00e2\u20ac\u2122t value pages containing\nhigh number of entities over other pages) in our\ndataset, where documents often lack entities or\nnoun phrases, and answers tend to be explanatory\nand lengthy.\nFinally, we provide various bench-\nmarks for the dataset, including baseline meth-\nods.\nGiven that our dataset is approximately \u00c3\u201410\nsmaller than larger datasets like CoQA and QuAC,\nand data scarcity poses a challenge, we also explore\nthe potential of enhancing model performance by\npre-training it on other question-answering datasets.\n(2019a) pro-\nposes marking history answers in the embedding\nlayer, and Qu et al.\nAnother line\nof research utilizes question rewriting (Vakulenko\net al., 2021) to address the problem.\n(2021) employs consistency training to mitigate the\nerror propagation problem in rewritten questions,\nwhile Chen et al.\nDespite these signif-\nicant efforts, a notable issue in most of the men-\ntioned research is the use of ground truth answers as\npart of the modeling process.\nA dialog sample of the dataset\nis depicted in Figure A1 whose title is Pride &\n3.1 Document Collection\nThe documents in our dataset are based on\nWikipedia articles.\nIn line with previous question-\nanswering datasets in Persian (Darvishi et al., 2023;\nAyoubi, 2021), we have chosen Wikipedia as the\nprimary source for obtaining documents.\nTo build\nour documents, we have taken a different approach\ncompared to CoQA, which selects the initial por-\ntion of each article as the final document (Reddy\net al., 2019).\nWikipedia articles typically begin\nwith an abstract that provides general informa-\ntion about the article\u00e2\u20ac\u2122s topic, and subsequent sec-\ntions delve into specific details.\nWhile the abstract\nis essential for constructing final documents, the\nfiner-grained information in the subsequent sec-\ntions should not be overlooked.\nTo address this\nconcern and ensure diversity among documents\nwith consistent contexts, we have devised a unique\nprocess for building our final documents.\nInitially,\nwe select two bounds for the minimum and maxi-\nmum document lengths, denoted as Dm and DM\nrespectively.\nDm is set to ensure that all documents\ncontain a minimum context necessary for a mean-\ningful dialog, while DM prevents excessively long\ndocuments that can challenge current network mod-\neling capabilities, as transformers consist our main\nmodels and they receive a limited length as input.\nIn\nour approach, we differentiate between the abstract\nand other sections, which we represent as A and\nSi, respectively, with i being the section number.\nThe role of the\ndocument provider is crucial in curating documents\nthat align with the dataset\u00e2\u20ac\u2122s requirements and main-\ntain contextual coherence.\n(2019),\nwe don\u00e2\u20ac\u2122t consider any pre-condition, like a good\nnumber of entities, for the selected pages.\nNext, It is\ndecided to whether select the document from the A\nor a random Si-s:\n\u00e2\u20ac\u00a2 If A is selected as the beginning of the doc-\nument: If LA + (cid:80)\ni LSi \u00e2\u2030\u00a4 Dm, meaning\nthat the length of page is below the minimum\n\fconstraint, the document is discarded and if\nA \u00e2\u2030\u00a5 DM , the A is tailored to ensure the max-\nimum length constraint.\nTo encourage diversity, the document\nprovider is allowed to append Si-s to the A\nin order to elongate the document such that\nthe maximum length constraint is preserved;\nthese Si-s are selected in a way so that they\nare semantically consistent with each other\n\u00e2\u20ac\u00a2 If Sj is selected as the beginning of the docu-\nment: the process follows a similar pattern\nas previously described.\nHowever, in this\ncase, the document begins with Sj and is sub-\nsequently extended with potentially semanti-\ncally consistent sections, as determined by the\ndocument provider.\nTo illustrate the process, an example involving\nthe Wikipedia page for \"Canada\" is shown in Fig-\nure A2.\nThe final document is then com-\nposed by concatenating these three selected sec-\nIt is important to emphasize the pivotal role of\nthe document provider in this process.\nFor instance, if\nSj is selected as the beginning of the document\nand it contains a co-reference to a previous sec-\ntion or some of its content is vague due to lack of\nprevious context, Sj should be omitted from the\nselection, and the process should proceed with the\nnext suitable section.\n3.2 Dataset Annotation\nTo establish dialogs, each document is assigned to\nto a questioner and a responder, both of whom hav-\ning access to the title and text of the document.\nThe questioners\nare informed that they should start the conversation\nwith general information and continue it to specific\nsubjects, to match the same process of human infor-\nmation seeking in real world.\nAddi-\ntionally, questioners are informed to change their\nquestions if their questions exhibit a substantial\noverlap with the potential answer.\n3.3 Post-Processing\nWhile our dataset is designed to provide question-\ners with access to documents, it is possible that\nstring-matching questions may arise (Choi et al.,\n2018), despite our efforts to guide questioners to\navoid such issues.\nPrevious studies have indicated\nthat questions exhibiting high similarity to the sen-\ntence containing the answer have a greater likeli-\nhood of being answered correctly (Sugawara et al.,\n2018).\nTo ensure the dataset\u00e2\u20ac\u2122s quality, we have\nidentified these questions and had them rewritten to\nreduce lexical overlap between the rewritten ques-\ntion and the sentence that contains the correspond-\ning answer.\nEach question that shares at least one\nsimilar word with the answer-containing sentence\nis subjected to this rewriting process.\nA question is\nrewritten in one of three ways:\n\u00e2\u20ac\u00a2 Words were removed due to ellipsis\n\u00e2\u20ac\u00a2 Words were replaced by their synonyms\n\u00e2\u20ac\u00a2 Words were replaced by their co-references\n| overlap |\nWe quantified the similarity using the formula\n| question words | where overlap is the\nsimilarity =\nset of shared words between the question and the\nsentence containing the answer.\nThis is\ndue to the fact that each question can have multiple\nanswers; Therefore, it is indispensable to obtain\naccurate and unbiased scores for evaluation.\nIn line with previous research (Choi et al.,\n2018; Rajpurkar et al., 2016), multiple annotations\nare assigned to each question in the Dev/Test set.\nThis practice is essential because a single ques-\ntion may have multiple valid answers.\nAdditionally, our docu-\nments are longer than those in CoQA and QuAC,\nnecessitating the use of transformers with larger\ninput sizes, as standard transformers have limited\ninput capacities.\nF1 indicates the degree of overlap between the\npredicted answer and the gold answer, and HEQ-Q\nand HEQ-D are the ratio of questions and dialogs\nfor which the model outperforms the human re-\nspectively (Choi et al., 2018).\nWhile HEQ-D is a\nstringent metric that requires the model to outper-\nform humans for every question within a dialog to\nearn a point, it may be overly strict in some cases.\nWhile HEQ-D is a stringent metric that requires\nthe model to outperform humans for every question\nwithin a dialog to earn a point, it may be overly\nstrict in some cases.\nTo address this, we introduce\nanother metric, called HEQ-M. HEQ-M quantifies\nthe number of dialogs for which the model achieves\na better overall performance compared to human\nperformance on average.\nAdditionally, we analyze\nthe F1 score for each dialog turn to gain insights\ninto the model\u00e2\u20ac\u2122s performance at different turns of\nthe conversation.\nFigure 1 illustrates the per-\nformance variation of the model concerning the\ninclusion of a different number of history ques-\ntions.\nNotably, excluding the history questions\nresults in a sharp drop in the model\u00e2\u20ac\u2122s performance.\nHowever, including more than\n2 history questions gradually leads to a decline in\nperformance.\nThis suggests that histories with dis-\ntances over 2 are irrelevant and don\u00e2\u20ac\u2122t introduce new\ninformation on average, and their inclusion induces\nsome noise in the model.\nIn our implementation, each model\ntakes the concatenated question and previous his-\ntory questions as the first input and the document\nas the second input, which is then fed into the trans-\nBaseline Methods ParsBERT and XLM-Roberta\nare fine-tuned on PCoQA, constituting our baseline\nPre-Trained Methods ParSQuAD + Pars-\nBERT denotes\npre-training ParsBERT on\nParSQuAD (Abadani et al., 2021), a translated\ndataset of SQUAD (Rajpurkar et al., 2018) to\nFarsi, and then fine-tuning it on PCoQA using\nhistory concatenation.\n"}, {"role": "assistant", "content": "Abstract\n\nHumans seek information regarding a specific\ntopic through performing a conversation con-\ntaining a series of questions and answers. In\nthe pursuit of conversational question answer-\ning research, we introduce the PCoQA, the first\nPersian Conversational Question Answering\ndataset, a resource comprising information-\nseeking dialogs encompassing a total of 9,026\ncontextually-driven questions. Each dialog in-\nvolves a questioner, a responder, and a docu-\nment from the Wikipedia; The questioner asks\nseveral inter-connected questions from the text\nand the responder provides a span of the docu-\nment as the answer for each question. PCoQA\nis designed to present novel challenges com-\npared to previous question answering datasets\nincluding having more open-ended non-factual\nanswers, longer answers, and fewer lexical\noverlaps. This paper not only presents the\ncomprehensive PCoQA dataset but also reports\nthe performance of various benchmark models.\nOur models include baseline models and pre-\ntrained models, which are leveraged to boost\nthe performance of the model. The dataset and\nbenchmarks are available at our Github page.1\n\n1\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nAs the field of autonomous vehicles has grown, the ma-\njority of research has been directed towards common driving\nsituations.\nThe control software for autonomous racing must\nnavigate racecars at extreme speeds and dynamic limits, a feat\neasily handled by skilled human racers but challenging for\ncurrent low-speed optimized autonomous driving systems.\nAn essential aspect of autonomous racing is the precise\nmodeling of a vehicle\u00e2\u20ac\u2122s dynamic behavior.\nAdditionally, calculating a racecar\u00e2\u20ac\u2122s\nmoment of inertia is a laborious task necessitating vehicle\ndisassembly, precise component weighing and positioning, and\nthe use of CAD software or multi-body physics simulations.\nFrequent recalculations are necessary to accommodate vary-\ning vehicle setups, track conditions, and tire wear, adding\ncomplexity to vehicle dynamics modeling, which is further\ncomplicated by the common omission of drivetrain dynamics,\nleading to incomplete models and suboptimal performance.\nDeep neural networks (DNNs) have proven adept at captur-\ning complex nonlinear dynamical effects [7], offering a simpler\nalternative to physics-based models as they obviate the need\nfor extra testing equipment.\nWe present Deep Dynamics, a PINN-\nbased approach that to our best knowledge is the first model of\nits kind suited for vehicle dynamics modeling in autonomous\nracing - a domain distinctly marked by high accelerations\nand high speeds.\nThis paper makes the following research\ncontributions:\n1) We introduce Deep Dynamics - A PINN that can estimate\nthe Pajecka tire coefficients, drivetrain coefficients, and\nmoment of inertia that capture the complex and varying\ndynamics of an autonomous racecar.\n2) We introduce the Physics Guard layer designed to ensure\nthat the model coefficients consistently remain within\ntheir physically meaningful ranges.\nSuch methods of-\nten make a simplifying assumption that the model coefficients\nare time-invariant, which does not hold true in autonomous\nracing where coefficients may vary due to changes in racecar\nsetups or tire wear throughout a race.\nAdditionally, while purely\ndata-driven models are great at learning complex non-linear\npatterns in the data, they are not designed to be consistent\nwith the laws of physics and can produce state predictions\nthat the vehicle cannot physically realize.\nRecently, Physics-Informed Neural Networks (PINNs) for\nvehicle dynamics have been gaining traction as they adeptly\nmerge the strengths of both physics-based and data-driven\nmodels.\nThe closest related work to ours is the Deep\nPacejka Model (DPM) introduced in [8], which estimates tire\nmodel coefficients for autonomous vehicles based on historical\nstates and control inputs.\nHowever, as detailed in Section IV-A,\nthe DPM bears inherent limitations that make it unsuitable\nfor use in autonomous racing applications.\nOur research aims\nto address these limitations, introducing a new PINN model\nsuitable for implementation in a real autonomous racecar.\nDynamic Single-Track Vehicle Model\nWe model the autonomous racecar using a dynamic single-\ntrack vehicle model [10], with the state variables, discrete-\ntime state equations, and notations listed in Table I.\nA drivetrain model [14] estimates the longitudinal force at\nthe rear wheels, Frx, using drivetrain coefficients, Cm1 and\nCm2, the rolling resistance Cr, and the drag coefficient Cd.\nThe Pacejka Magic Formula tire model [5] estimates the\ntire forces on the front and rear wheels, Ff y and\nFry, inflicted by the vehicle\u00e2\u20ac\u2122s movement and steering.\nThe\nPacejka model first calculates the sideslip angles for the front\nand rear wheels, \u00ce\u00b1f and \u00ce\u00b1r, using state St and the distance\nbetween the vehicle\u00e2\u20ac\u2122s center of gravity to the front and rear\naxles, lf and lr.\nWe make the following modeling\nassumptions for the dynamic single-track model used in this\nwork: 1) The model assumes that the racecar drives in 2D\nspace despite the banking on real-world racetracks.\nActuation\ndelays can be accounted for in the model, therefore this is\njust a simplifying assumption.\nThe Deep Dynamics Model uses a DNN to indirectly learn a\nvehicle\u00e2\u20ac\u2122s Pacejka coefficients, drivetrain coefficients, and moment of inertia\nusing historical state and control inputs.\nThese include\nthe mass, m, and geometric properties, lr and lf , which can\nbe measured in a typical garage with corner weight scales.\nLimitations of the Deep Pacejka Model\nThe DPM uses a neural network to only estimate the Pacejka\ncoefficients {Bf /r, Cf /r, Df /r, Ef /r} \u00e2\u0160\u201a \u00ce\u00a6u, and rather than\nusing a physics-based drivetrain model, directly estimates the\nlongitudinal force, Frx, for an autonomous vehicle given a\nhistory of the vehicle\u00e2\u20ac\u2122s state and control inputs.\nBackpropagating results in a network that learns Pacejka\ncoefficients and a purely data-driven drivetrain model that\ncapture the vehicle\u00e2\u20ac\u2122s dynamics.\nFirst, the DPM\nsolves an unconstrained version of the problem in Equation\n2, i.e.\nIn addition, the DPM does\nnot have a drivetrain model as there is no learned relation\nbetween the vehicle throttle, T , and longitudinal wheel force,\nFrx, which limits the DPM to using PID throttle control to\ntrack Vdes set by sampling-based MPC rather than directly\ncontrolling T .\nMore importantly, the Deep Pacejka Model was\ndesigned for low-speed autonomous driving and not for a high-\nspeed racing environment.\nThe estimated coefficients and single-track\nmodel state equations are then used to predict \u00cb\u2020Xt+1, and the\nbackpropagated loss function is computed using\nt+1 \u00e2\u02c6\u2019 \u00cb\u2020X (i)\nacross the state variables [vx, vy, \u00cf\u2030].\nIn\ncontrast to the DPM, the architecture of the DDM allows for\nlongitudinal control using an MPC solver due to the inclusion\nof a physics-based drivetrain model.\nAn optimal raceline, W \u00e2\u02c6\u02c6 Rn\u00c3\u20142, contain-\ning a sequence of n reference points wi = [xref,i, yref,i] \u00e2\u02c6\u02c6 R2 is\ntracked using MPC by solving for the optimal control inputs,\nU0, ..., UH\u00e2\u02c6\u20191, that minimize the cost function\nU0,...,UH\u00e2\u02c6\u20191\nxh \u00e2\u02c6\u2019 xref,h\nyh \u00e2\u02c6\u2019 yref,h\nacross the forward prediction horizon H. The first term, the\ntracking cost, penalizes deviations from the optimal raceline\nusing the cost matrix Q \u00e2\u02c6\u02c6 R2\u00c3\u20142.\nThe second term,\nactuation cost, penalizes rapid steering and throttle changes\nusing the cost matrix R \u00e2\u02c6\u02c6 R2\u00c3\u20142.\nOnce the optimal series of\ncontrol inputs U0, ..., UH\u00e2\u02c6\u20191 have been derived, the controller\ncan enact U0 and repeat this procedure.\nAt time t, Deep Dynamics is used to predict the vehicle\u00e2\u20ac\u2122s\ntrajectory over the forward horizon, H, by first estimating\nmodel coefficients \u00cb\u2020\u00ce\u00a6ut using the observed states and control\ninputs from times t \u00e2\u02c6\u2019 \u00cf\u201e to t. These coefficients are held\nconstant and used by MPC to propagate the predicted state\nof the vehicle from time t to t + H. Holding \u00cb\u2020\u00ce\u00a6ut constant\nthroughout H allows an MPC solver to compute steering and\nthrottle commands that minimize Equation 5 without having\nto differentiate through the DDM\u00e2\u20ac\u2122s hidden layers, making it\nfeasible to run in real-time.\nThe\ntraining data, Dtrain\nreal consisting of 13,418 samples was collected\nat the Putnam Park road course in Indianapolis, while the\ntest set, Dtest\nreal, was curated from laps at the Las Vegas Motor\nSpeedway and consists of 10,606 samples.\nSimulation dataset,\nDsim, contains ground-truth vehicle state information from laps\ndriven in a 1:43 scale racecar simulator [14], where the ground\ntruth coefficient values \u00ce\u00a6 are known.\nB. Nominal Model Coefficient Ranges\nThe Physics Guard layer requires bounds \u00ce\u00a6\n\u00c2\u00af u and \u00c2\u00af\u00ce\u00a6u for\neach of the model coefficients \u00ce\u00a6u estimated by Deep Dynam-\nics.\nThe the drag coefficient, Cd, and rolling\nresistance, Cr0, were given ranges from 0.1 \u00e2\u02c6\u2019 1.0 kg m\u00e2\u02c6\u20191\nand 0.1 \u00e2\u02c6\u2019 1.4 N, respectively, based on values estimated in\n[25].\nThe range for drivetrain coefficient Cm1 was set to be\n500 \u00e2\u02c6\u2019 2000 N based on the engine specification for Indy cars\nand the coefficient range for Cm2 was set to 0.0 \u00e2\u02c6\u2019 1.0 kg s\u00e2\u02c6\u20191\nto ensure force lost to engine friction would remain relatively\nsmall to the force generated.\nIn addition, we also\ngenerate horizon predictions using the estimated coefficients\nof each model and compute the average displacement error\n(ADE) and final displacement error (FDE) [26].\nDisplacement (m)\nOPEN-LOOP MODEL PERFORMANCE\nD. Open-Loop Testing\nTo highlight the inadequacy of presuming precise knowl-\nedge of the ground truth moment of inertia, Iz, we trained\nthree distinct iterations of the DPM: one given the ground\ntruth value, termed DPM (GT); another with a value inflated by\n20%, labeled DPM (+20%); and a third with a value reduced\nby 20%, referred to as DPM (-20%).\nDDM\u00e2\u20ac\u2122s\nability to predict velocities with high accuracy translates into\nmore precise future position predictions, as evidenced by its\nlowest ADE, and FDE across both simulated and real data.\nEven when given the ground-truth value of Iz, the DPM still\nexhibited ADE and FDE nearly 2 orders of magnitude worse\nfor both metrics.\nThe DPM\u00e2\u20ac\u2122s esti-\nmated Pacejka coefficients for both simulation and real data\ndiverged significantly from anticipated values, being either an\norder of magnitude different or contrary in sign.\nFurthermore,\nthe Physics Guard layer ensured that the coefficients derived\nreal generalize to different track conditions in Dtest\nfrom the Dtrain\nE. Closed-Loop Testing\nClosed-loop testing and comparison with DPM was per-\nformed using the MPC implementation described in Sec-\ntion IV-C. For DPM, a PID controller was used for throttle\ncontrol (as opposed to a sampling-based MPC controller in the\nDPM paper).\nFigure 4 plots the time annotated traces for each\nmodel.\nRacing-relevant metrics such as lap time (s), average\nspeed, and track boundary violations are reported in Table IV\nfor a single lap with an initial vx of 0.1m s\u00e2\u02c6\u20191.\nDDM results\nin the fastest lap time of 5.38s while incurring 0 boundary\nLap Time (s) Average Speed (m/s) Track Violations\nCLOSED-LOOP RACING ANALYSIS\n\fREFERENCES\n[1] Johannes Betz et al.\nBoth models used the Mish activation function for the hidden\nlayers and were optimized using Adam.\nA validation set was\ncreated using 20% of the training set chosen randomly, and\nthe DDM and DPM with the lowest validation loss were used\nto generate our results.\n"}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dAutonomous racing is a critical research area for\nautonomous driving, presenting significant challenges in vehicle\ndynamics modeling, such as balancing model precision and\ncomputational efficiency at high speeds (> 280km h\u00e2\u02c6\u20191), where\nminor errors in modeling have severe consequences. Existing\nphysics-based models for vehicle dynamics require elaborate\ntesting setups and tuning, which are hard to implement, time-\nintensive, and cost-prohibitive. Conversely, purely data-driven\napproaches do not generalize well and cannot adequately en-\nsure physical constraints on predictions. This paper introduces\nDeep Dynamics, a physics-informed neural network (PINN)\nfor vehicle dynamics modeling of an autonomous racecar. It\ncombines physics coefficient estimation and dynamical equations\nto accurately predict vehicle states at high speeds and includes\na unique Physics Guard layer to ensure internal coefficient\nestimates remain within their nominal physical ranges. Open-\nloop and closed-loop performance assessments, using a physics-\nbased simulator and full-scale autonomous Indy racecar data,\nhighlight Deep Dynamics as a promising approach for modeling\nracecar vehicle dynamics.\n\nI. "}]}
