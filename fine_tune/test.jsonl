{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nOur\ncontributions include a highly customizable and scalable HVAC and\nbuilding simulator, a rapid configuration method to customize the\nsimulator to a particular building, a calibration method to improve\nthis fidelity using real world data, and an evaluation method to\nmeasure the simulator fidelity.\nOverview of the HVAC problem Most office buildings are\nequipped with advanced HVAC devices, like Variable Air Volume\n(VAV) devices, Hot Water Systems (HWS), Air Conditioner (AC)\nand Air Handlers that are configured and tuned by the engineers,\nmanufacturers, installers, and operators to run efficiently with the\ndevice\u00e2\u20ac\u2122s local control loops [15]. However, integrating multiple\nHVAC devices from diverse vendors into a building \u00e2\u20ac\u0153system\u00ef\u00bf\u00bd re-\nquires technicians to program fixed operating conditions for these\nunits, which may not be optimal for every building and every poten-\ntial weather condition. If existing setpoint control policies are not\noptimal under all conditions, the possibility exists that an ML model\nmay be trained to continuously tune a small number of setpoints\nto achieve greater energy efficiency and reduced carbon emission\n[22].\nRL for HVAC systems We define the state of the office building\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd at time \u00ef\u00bf\u00bd as a fixed length vector of measurements from sensors on\nthe building\u00e2\u20ac\u2122s devices, such as a specific VAV\u00e2\u20ac\u2122s zone air temperature,\ngas meter\u00e2\u20ac\u2122s flow rate, etc. The action on the building \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd is a fixed-\nlength vector of device setpoints selected by the agent at time \u00ef\u00bf\u00bd,\nsuch as the boiler\u00e2\u20ac\u2122s supply water temperature setpoint, etc. We\nalso define a custom feedback signal, or reward, that indicates the\nquality of taking an action in the current state, as a weighted sum of\nnegative cost functions for carbon emission, energy consumption,\nand zone-level setpoint deviation. The goal of RL is to maximize the\nexpected long-term cumulative reward [18]. In order to converge\nto the optimal policy, the agent requires many training iterations,\nmaking training directly on the building from scratch inefficient\nand impracticable. Therefore, it is necessary to train in an offline\nsandbox environment that adequately emulates the dynamics of\nthe building before being deployed on the actual building.\n2 RELATED WORK\nConsiderable attention has been paid to HVAC control in recent\nyears, and a growing portion of that has considered how RL and its\nvarious associated algorithms can be leveraged [8, 14, 20, 23, 24].\nA central requirement in RL is the offline environment that trains\nthe RL agent. Several methods for this have been proposed, largely\nfalling under three broad categories.\nData-driven Emulators: Some works attempt to learn a dy-\nnamics model as a multivariate regression model from real world\ndata [25, 29], often using recurrent neural network architectures,\nsuch as LSTMs [16, 19, 28? ]. The difficulty here is that data-driven\nmodels often do not generalize well to circumstances outside the\ntraining distribution, especially since they are not physics based.\nPhysics-based Simulation: EnergyPlus [7], a high-fidelity sim-\nulator developed by the Department of Energy, is commonly used\n[2, 21, 22, 27], but suffers from the scalability issues outlined above.\nOffline RL: The second approach is to train the agent directly\nfrom the historical real world data, without ever producing an in-\nteractive environment [4\u00e2\u20ac\u201c6]. While the real world data is obviously\nof high accuracy and quality, this presents a major challenge, since\nthe agent cannot take actions in the real world and interact with\nany form of an environment, severely limiting its ability to improve\nover the baseline policy that produced the real world data [10].\nTo overcome the limitations of each of the above three methods,\nsome work has proposed a hybrid approach [3, 26], and this is the\ncategory our work falls under. Our approach uses a physics-based\nsimulator that achieves an ideal balance between speed and fidelity,\nwhich is sufficient to train an effective control agent off-line.\n3 A SIMPLE CALIBRATED SIMULATION\nDesign Considerations A fundamental tradeoff when designing a\nsimulator is speed versus fidelity. Fidelity is the simulator\u00e2\u20ac\u2122s ability to\nreproduce the building\u00e2\u20ac\u2122s true dynamics that affect the optimization\nprocess. Speed refers to both simulator configuration time, and\nagent training time, i.e., the time necessary for the agent to optimize\nits policy using the simulator. Every building is unique, due to\nits physical layout, equipment, and location. Fully customizing\na high fidelity simulation to a specific target building requires\nnearly exhaustive knowledge of the building structure, materials,\nlocation, etc., some of which is unknowable. Also, the configuration\ntime required for high-fidelity simulations limits their utility for\ndeploying RL-based optimization to many buildings. Our goal is to\ndevelop a method for applying RL at scale to commercial buildings.\nTo this end, we must have a simulated environment to train the\nagent, as real world training is not possible. In order to scale to many\nbuildings it must be easy for our simulator to be configured to a\nnew building, with enough fidelity to the real world to be useful. We\nnote that while RL is the control method proposed in this work, our\ncalibrated simulator is useful as well for other controller methods\nsuch as Model Predictive Control [9]. To meet these requirements\nwe designed a lightweight simulator based on Finite Differences\napproximation of heat exchange, with a novel automated procedure\nto go from building floor plans to a custom simulator with little\nmanual effort. For improved fidelity, we designed a calibration and\nevaluation pipeline based on real telemetry.\nWe selected water supply temperature \u00cb\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd and the air handler\nsupply temperature \u00cb\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd as agent actions because they affect the\nbalance of electricity and natural gas consumption, they affect\nmultiple device interactions, and they affect occupant comfort.\nFinite Differences Approximation The diffusion of thermal\nenergy in time and space of the building can be approximated using\nthe method of Finite Differences (FD)[12, 17], and applying an\nenergy balance. This method divides each floor of the building into\na grid of three-dimensional control volumes and applies thermal\ndiffusion equations to estimate the temperature of each control\nvolume. By assuming each floor is thermally isolated, (i.e., no heat\nis transferred between floors), we can simplify the three-spatial\ndimensions into a spatial two-dimensional heat transfer problem.\nFinally, the thermal energy associated with the VAV (c) is equally\ndistributed to all associated control volumes that have a diffuser.\nThermal diffusion within the building is mainly accomplished\nvia forced or natural convection currents, which can be notoriously\ndifficult to estimate accurately. We note that heat transfer using air\ncirculation is effectively the exchange of air mass between control\nvolumes, which we approximate by a randomized shuffling of air\nwithin zones, parameterized by a shuffle probability.\nHVAC Model The HVAC system is modeled as an energy bal-\nance with a lossy thermal exchange between the hot water and air\ncircuits, evaluating temperature differences, and air and water flow\nrates with efficiency parameters. Aggregate heating and cooling\ndemands at the AC and HWS are governed by thermostats that\ncreate demand when its zone temperature is near zone air tempera-\nture setpoints. Special control volumes associated with air diffusers\nallow thermal energy exchange between the HVAC model and the\nFD grid, (cid:164)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd .\nSimulator Configuration For RL to scale to many buildings, it\nis critical to be able to easily and rapidly configure the simulator\nto any arbitrary building. To configure the simulator, we require\nfloorplans, and HVAC metadata. We preprocess the detailed floor-\nplan blueprints of the building, and extract a grid that gives us an\napproximate placement of walls. We also employ a user interface\nto label the location of each HVAC device on the floorplan grid.\nWe tested this pipeline on our pilot building, which consisted of\ntwo floors with combined surface area of 68,000 ft2, and 127 HVAC\ndevices. Given floorplans and HVAC layout information, a single\ntechnician was able to generate a fully specified simulation in under\n3 hours that matched the real building in every device and room.\nSimulator Calibration and Evaluation In order to calibrate\nthe simulator to the real world using data, we must have a metric\nwith which to evaluate our simulator, and an optimization method\nto improve our simulator on this metric. We proposed a novel\nevaluation procedure, based on \u00ef\u00bf\u00bd -step prediction. Each iteration\nstep of our simulator was designed to represent a five minute in-\nterval, and our real world data is also sampled in five minute in-\ntervals. To evaluate the simulator, we take a chunk of real data\nof \u00ef\u00bf\u00bd observations. We then initialize the simulator so that its ini-\ntial state matches that of the starting observation, and run the\nsimulator for \u00ef\u00bf\u00bd steps, replaying the same HVAC policy as was\nused in the real world observations. Next, we calculate our sim-\nulation fidelity metric, which is the mean absolute error of the\nmeasured zone air temperatures, at the \u00ef\u00bf\u00bd th timestep.\nSimulator Calibration Once we defined our simulation fidelity\nmetric, we can minimize the error by searching over the physical\nparameters that affect the simulation response. The variables tuned\nduring the parameter search included the following: (a) the forced\nconvection coefficient quantifying outside wind and air currents\nagainst the building exterior surfaces; (b) the thermal conductivity,\nheat capacity, and density of exterior and interior walls; and (c)\nthe shuffle probability that controls how likely an air volume will\nbe exchanged with another within the zone to approximate the\ninternal air circulation and interior forced convection.\n4 EXPERIMENT RESULTS\nWe now demonstrate the results of how our simulator, when tuned\nand calibrated, is able to make real-world predictions.\nExperiment Setup To test out our simulator, we obtained teleme-\ntry recordings from our pilot building, a commercial office building\nlocated in Mountain View, California. The building has two stories\nwith a combined surface area of 68,000 square feet, and has 127\nHVAC devices. We obtained floor plan blueprints and used them to\nconfigure a customized simulator for the building, a process that\ntook a single human less than three hours to complete.\nCalibration Data To tune and evaluate our simulator, we took\nthree, non-overlapping intervals of telemetry: (a) Thursday, July\n6 2023, from 1:40 AM to 7:40 AM (\u00ef\u00bf\u00bd = 72), (b) Friday, July 7 2023,\nfrom 1:40 AM to 7:40 AM (\u00ef\u00bf\u00bd = 72), and (c) from Friday, July 7 2023,\nfrom 11:40 AM to 5:40 PM (\u00ef\u00bf\u00bd = 72). The first telemetry interval,\n(a), was used to tune the simulator for calibration. The second and\nthird telemetry intervals, (b) and (c), were used for validation.\nCalibration Procedure We tuned for 100 iterations on our\nsimulator. The parameters varied, the sweep range limits, and the\nvalues found that minimized the calibration metric are shown in\nTable 1. Because of our focus on speed, we can similate a five\nminute interval in about 15 seconds on one CPU, allowing for very\nrapid tuning. We reviewed the parameters that yielded the lowest\nsimulation error from calibration. Densities, heat capacities, and\nconductivities plausibly matched common interior and exterior\nbuilding materials. However, the external convection coefficient\nwas higher than under normal weather conditions, and likely is\ncompensating for radiative losses and gains, which were not directly\nsimulated.\nCalibration Results Table 2 shows the \u00ef\u00bf\u00bd -step prediction fi-\ndelity, as MAE, over a six-hour prediction window (\u00ef\u00bf\u00bd = 72). We\ncalculated the spatial mean absolute temperature error, as defined\nabove. We also present a second metric, the median spatial temper-\nature error. This was not used in the tuning process, but gives us\nsome insight into how well the calibration process is performing.\nAs indicated in Table 2, our tuning procedure drifts only 0.6\u00e2\u2014\u00a6 on\naverage over a 6 hour period on the tuning set, and we get good\ngeneralization: almost identical error for the same time period of a\ndifferent day. It should be noted that an uncalibrated model had a\nmuch larger error of 2.1\u00e2\u2014\u00a6. Furthermore, the median temperature in\nthese two cases is near zero, indicating that even despite drift, our\nprediction remains a mostly unbiased estimator. For the different\ntime of day scenario, we observe a larger drift of 1.2\u00e2\u2014\u00a6, and the me-\ndian is shifted as well, indicated that tuning on one time of day is\nnot as useful when making predictions on another.\nVisualizing Temperature Drift Over Time Figure 2 illustrates\ntemperature drift over time. At each time step, we plot the real world\ntemperatures in a blue boxplot and the simulator ones in orange.\nVisualizing Spatial Errors Figure 3 is a heatmap of the spatial\ntemperature difference across both floors after six hours. Red re-\ngions indicate the simulator was warmer than the validation data,\nblue regions indicate cooler, and white regions had no temperature\ndifference. The ring of blue around the building indicates that our\nsimulator is too cold on the perimeter, which implies that the heat\nexchange with the outside is happening more rapidly than it would\nin the real world. The inside of the building remains red, which\nmeans that despite the simulator perimeter being cooler than the\nreal world, the inside is warmer. The white band region running\nalong the perimeter is where the temperature achieved a perfect\nfit with the validation data. The interior red regions are due to\nslight errors in modeling interior heat exchange, and the blue re-\ngions indicate errors due to high thermal losses with the outside.\nWe suspect these errors are largely due to assuming thermal isola-\ntion between floors and assuming no radiative exchange with the\noutside. Consequently, the calibration process compensates with\na high convection coefficient. Internal thermal exchange seems\nslower than the real world, and the calibration compensates with\nmore rapid outside thermal exchange."}, {"role": "assistant", "content": "ABSTRACT\nModern commercial Heating, Ventilation, and Air Conditioning\n(HVAC) devices form a complex and interconnected thermody-\nnamic system with the building and outside weather conditions,\nand current setpoint control policies are not fully optimized for\nminimizing energy use and carbon emission. Given a suitable train-\ning environment, a Reinforcement Learning (RL) model is able to\nimprove upon these policies, but training such a model, especially\nin a way that scales to thousands of buildings, presents many real\nworld challenges. We propose a novel simulation-based approach,\nwhere a customized simulator is used to train the agent for each\nbuilding. Our open-source simulator1 is lightweight and calibrated\nvia telemetry from the building to reach a higher level of fidelity. On\na two-story, 68,000 square foot building, with 127 devices, we were\nable to calibrate our simulator to have just over half a degree of drift\nfrom the real world over a six-hour interval. This approach is an\nimportant step toward having a real-world RL control system that\ncan be scaled to many buildings, allowing for greater efficiency and\nresulting in reduced energy consumption and carbon emissions.\n\nKEYWORDS\nHVAC Optimization, Simulation, Reinforcement Learning\n\nACM Reference Format:\nJudah Goldfeder and John Sipple. 2023. A Lightweight Calibrated Simulation\nEnabling Efficient Offline Learning for Optimal Control of Real Buildings.\nIn Proceedings of RLEM \u00e2\u20ac\u212223. ACM, New York, NY, USA, 5 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n\n1 "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nAcknowledg-\ning the inherent unpredictability of precise stock prices [2],\nour research leverages these blue-chip stocks to forecast future\ntrends in stock price movement and volatility [3], [4].\nResearch within the realm of the stock market generally\nfalls into two branches: technical analysis and fundamental\nanalysis. Moreover, the existing\nmodels are primarily focused on predicting whether trends will\nchange [10], but overlook the significance of the magnitude of\nsuch changes. In reality, the extent of these changes constitutes\nan important part of stock behavior.\nIn this paper,\nto address\nthe aforementioned limita-\n3GICS is a method for assigning companies to a specific economic sector\nand industry group that best defines its business operations.\n1The ratio is a measure of the total value of all publicly traded stocks in a\n4Investment-worthy refers to high-quality companies rated Baa or higher\nmarket divided by that economy\u00e2\u20ac\u2122s gross domestic product (GDP).\naccording to evaluations by Moody\u00e2\u20ac\u2122s and Standard & Poor\u00e2\u20ac\u2122s (S&P).\n2A blue chip stock is stock issued by a large, well-established, financially-\n5Although Twitter has recently been re-branded to \u00e2\u20ac\u0153X\u00ef\u00bf\u00bd, this article contin-\nsound company with an excellent reputation.\nues to use the original name,\nIn this study, we aim to overcome the limitations of previous\nmethodologies. We adopt an efficient tweet filter to eliminate\nredundant content and reduce noise. Additionally, we embrace\na multi-dimensional approach to the stock market, integrating\ncorrelations of stock market information in a multi-task man-\nner. By skillfully combining the strengths of both technical and\nmodel, we achieve state-of-the-art performance in predicting\nstock movements.\nIII. PROBLEM SETUP\nWe offer a formal definition for the problem of predicting\nstock movement and volatility. Let S denote a set of target\nstocks for which predictions are sought. We define a set xst\nof feature vectors, with s \u00e2\u02c6\u02c6 S and t \u00e2\u02c6\u02c6 T , which encapsulate\nhistorical prices. Let T represents the set of available training\ndays. In addition, we identify a set \u00ce\u00b5 consisting of tweets,\neach referencing at least one stock in S. These historical\nprices incorporate the opening, highest, lowest, closing prices\nand adjusted closing price7 for each stock. Using P to\nsignify the adjusted closing price of stock s on day t as\nP = {Ps,1, . . . , Pst}, we then express the actual labels for\nmovement and volatility as follows:\nWe usually can generate multiple training examples by\nshifting the time lag within extensive historical stock data in\npractical situations. However, in order to simplify the expla-\nnation of our proposed method, we focus on a specific time\nlag for both movement and volatility prediction. We\u00e2\u20ac\u2122ve also\nintegrated the use of adjusted stock prices into our predictive\nmodels. [23]. This allows our models to learn from the genuine\nchanges in a stock\u00e2\u20ac\u2122s value that occur due to market conditions,\nrather than being swayed by artificial fluctuations resulting\nfrom corporate actions.\nIV. ECON STRUCTURE\nWe present an overview of data ingestion, text preprocess-\ning, and model architecture, as illustrated in Figure 1. Initially,\nwe process the historical stock prices and macroeconomic\nfactors to distill pertinent features. Following this, we sift\nthrough Twitter data to identify tweets that most aptly reflect\npublic sentiment. During text preprocessing, we mask stock\ntickers associated with companies found in the curated tweets.\nThese masked tweets are then transformed into embeddings.\nLeveraging a self-aware mechanism, we generate embeddings\nfor both sectors and tweets. In the concluding step, we amal-\ngamate information from stocks, sectors, and macroeconomic\nindicators in a multi-trends approach to predict stock price\nmovement and volatility.\n In our research, we consistently selected the\ntop six tweets, those garnering the highest daily impressions,\nas our primary input for text preprocessing.\nSentiment Analysis. In our research, we undertook a sen-\ntiment analysis on a dataset comprising 7.7 million tweets.\nTo achieve this, we leveraged a language model based on\nRoberta8, and the model has been fine-tuned on Twitter data.\nThis optimized model can produce one of three sentiment\noutcomes for each tweet: positive, neutral, or negative. For\nevery tweet e associated with a blue-chip stock s, we compared\nthe predicted sentiment pe with the same day\u00e2\u20ac\u2122s binary adjusted\nprice movement ys\nm gauges the\nprice fluctuation between days t \u00e2\u02c6\u2019 1 and t. Same as problem\nsetup, if the stock price ascends in this interval, ys\nm is labeled\n\u00e2\u20ac\u0153Up\u00ef\u00bf\u00bd, and conversely, if it declines, it\u00e2\u20ac\u2122s labeled \u00e2\u20ac\u0153Down\u00ef\u00bf\u00bd. It\u00e2\u20ac\u2122s\npertinent to note that our dataset only incorporated tweets from\nthe trading hours of day t. To aggregate daily sentiment, if a\nday witnesses a predominance of negative sentiments in tweets\nconcerning a specific stock, we designate that day as \u00e2\u20ac\u0153nega-\ntive\u00ef\u00bf\u00bd. In contrast, if the tweets are primarily characterized by\npositive sentiments, the day is classified as \u00e2\u20ac\u0153positive\u00ef\u00bf\u00bd.\nm of stock s. The measure ys\nTweets extracting. In addition to textual information, we\ngathered metrics on likes, retweets, and impressions for each\ntweet.\nV 0.0394 0.0643 0.0747 0.0816 0.0833 0.0877 0.0872 0.0853 ... 0.0917\nC. Text preprocessing\nIn natural language processing (NLP), text preprocessing\nhas long been recognized as a key step. By streamlining\ntext into a format more amenable to training, it not only\nsimplifies the content but also boosts the efficiency of machine\nlearning algorithms. Shifting focus to the concept of self-\naware mechanism, our main objective is pinpointing sector\nidentification. Within this context, the embeddings of both\nsectors and tweets are crucial. When the stock symbol s is\nmasked, we aim to identify the sector c a tweet e refers to\nbased solely on its content.\nTokenization. Tokenization is the process by which text is\ndivided into smaller units, such as sentences, words, charac-\nters, or subwords. Word tokenization, specifically, refers to the\nsegmentation of text into individual words. In our research, we\nutilized spaCy9\u00e2\u20ac\u201dan open-source natural language processing\ntool\u00e2\u20ac\u201dfor word tokenization of every tweet e. Moreover, a\n9https://spacy.io/\n\fstock ticker like \u00e2\u20ac\u0153AAPL\u00ef\u00bf\u00bd for Apple might inadvertently be\ntokenized into \u00e2\u20ac\u0153AA\u00ef\u00bf\u00bd and \u00e2\u20ac\u0153PL\u00ef\u00bf\u00bd. To circumvent such issues,\nwe established specific rules to ensure that stock tickers are\nconsistently recognized as single, intact tokens. After tokeniza-\ntion, words are converted to lowercase because stock tickers\nwon\u00e2\u20ac\u2122t overlap with regular vocabulary. Such normalization not\nonly ensures vocabulary consistency across the dataset but also\naugments computational efficiency.\nCompany Masking. For the subsequent self-awareness\nportion of the model architecture, we replace all tokens in\n\u00ce\u00b5s that correspond to the name of stock s with a special token\n[mask]. Based on the sector classification rules of GISC, we\nmaintained a dictionary mapping the masked companies to\ntheir respective sectors. For example, after data filtering, we\nobtain a tweet with a high impression: \u00e2\u20ac\u0153With Vision pro,\n$AAPL is about to soar!\u00ef\u00bf\u00bd Next, we mask the stock ticker,\nconverting it to: \u00e2\u20ac\u0153With Vision pro, [mask] is about to soar!\u00ef\u00bf\u00bd\nSentence encoding. Words from the sentence are first\ntokenized and then mapped to their corresponding word em-\nbeddings. These embeddings are high-dimensional vectors that\ncapture the intrinsic semantics of each word. We record the\noriginal length of each tweet, storing this information in the\nvariable l. This data proves crucial when padding comes\ninto play, a technique used to append or prepend sequences\nwith filler values, ensuring uniformity. Given the importance\nof maintaining consistent sequence lengths, especially when\nworking in batches, padding ensures our dataset\u00e2\u20ac\u2122s sequences\nadhere to a standardized length, adding consistency and struc-\nture to our data processing pipeline.\nIn our approach, we adopted a\nbi-directional LSTM (BiLSTM) for the predictor network to\nproduce the tweet and sector embeddings.\nThe first attention is to find tweets relevant to each target sector\nc. We find relevant tweets by using the embedding hc of sector\nc as the query of attention as follows:\nlet \u00ce\u00b5t represent\nConsidering a day t,\nthe collection of\ntweets. For each tweet e within this set, its corresponding\nembedding is denoted by he. Additionally, \u00ce\u00b1e signifies the\nattention weight attributed to the tweet. The vector wct \u00e2\u02c6\u02c6 R2k,\nwhich can be envisioned as a weighted mean of all the tweet\nembeddings for day t. This averaging is influenced by the\npertinence of each tweet to the specific target sector c. Then,\nwe employ a linear layer to project wct onto all possible\ncompanies S:\ngst = W1(wct) + b1,\n(10)\nwhere W1 \u00e2\u02c6\u02c6 Rn\u00c3\u20142k, b1 \u00e2\u02c6\u02c6 Rn. As a result of the attention\nfunction of Equation (10), gst \u00e2\u02c6\u02c6 Rn represent that summarizes\nall tweets at day t considering the relevance to sector c. Our\nsecond attention uses the resulting tweet vector to find the\nmost relevant stocks at the moment. This is done by using gst\nas the query vector of attention to aggregate the stock price\nfeatures at day t based on the stock embeddings:\nThe micro trend vector ist \u00e2\u02c6\u02c6 Rn encapsulates all tweets for\nday t, integrating the information from companies associated\nwith the sector. This vector is then combined with the stock\nprice vector xt \u00e2\u02c6\u02c6 Rn\u00c3\u2014p which generates from Yahoo\u00e2\u20ac\u2122s stock\ndata and the macro trend vector at, serving as the input to the\nAttention GRU for forecasting stock movement and volatility.\nPredictions layer. The model is primarily designed to ex-\ntract nuanced information from stock market, macroeconomic\nfactors, and tweet data. Once the macro trend at and micro\ntrend ist are generated, they are concatenated with the stock\nprice feature xst \u00e2\u02c6\u02c6 Rp using a linear layer, producing a multi-\nlevel feature.\nV. EXPERIMENT\nWe conduct experiments to answer the following questions\nabout the performance of framework:\nQ1. Baseline methods. How do other baseline models perform\non our task?\nQ2. Tweets quality. Does using our model to extract data\nyield better results than performing sentiment analysis using\nthe entire 7.7 million tweets as input?\nQ3. Ablation study. How do the macro and micro trend affect\nits performance?\nA. Baseline Methods\nWe evaluate our model\u00e2\u20ac\u2122s effectiveness by comparing it with\ntechnical and fundamental analysis models. Technical analysis\nmodels focus on capturing patterns of historical prices and\nTABLE III : Classification performance of ECON and baseline\nmethods, measured with the accuracy (ACC) , the Matthews\ncorrelation coefficient (MCC) and Receiver Operating Char-\nacteristic (ROC). The best is in bold, Our ECON shows the\nbest performance in all evaluation metrics.\nfundamental analysis models combine historical prices with\nother government and financial market information for stock\nprice movement and volatility prediction.\n\u00e2\u20ac\u00a2 Extreme Gradient Boosting (XGBoost) [32] is a high-\nperformance gradient boosting framework renowned for its\nefficiency in handling large datasets, regularization features,\nand capability in both classification and regression tasks.\n\u00e2\u20ac\u00a2 Autoregressive Integrated Moving (ARIMA) [33] is a fore-\ncasting model for time series data, consisting of autoregressive\n(AR), differencing (I), and moving average (MA) components.\nIt assumes data linearity and stationarity.\n\u00e2\u20ac\u00a2 Long Short-Term Memory (LSTM) [34] is a type of\nrecurrent neural network (RNN) architecture specifically de-\nsigned to tackle the vanishing and exploding gradient problems\nencountered in traditional RNNs. LSTM units use gate mech-\nanisms to regulate the flow of information, making them well-\nsuited for learning from long-term dependencies in sequences.\n\u00e2\u20ac\u00a2 Attention LSTM (ALSTM) [12] is a LSTM network with\nan attention mechanism, enhancing sequence modeling by\nemphasizing relevant elements.\n\u00e2\u20ac\u00a2 DTML [35] is based on Transformer, it learns temporal\ncorrelations and combines the contexts of all target stocks.\n\u00e2\u20ac\u00a2 SLOT [9] is based on ALSTM, it captures correlations\nbetween tweets and stocks through self-supervised learning.\nTo test whether our Twitter filter can still accurately capture\nmarket sentiment even with a significantly reduced Twitter\ninput:\n\u00e2\u20ac\u00a2 AGRUD-A concatenated the sentiment score of all 7.7\nmillion tweets with stock prices and macroeconomic data,\nkeeping the structure of main predictor AGRUD unchanged.\n\u00e2\u20ac\u00a2 AGRUD-F concatenated the sentiment score of data refined\nwith stock prices and macroeconomic data, keeping the struc-\nture of main predictor AGRUD unchanged.\nEvaluation metrics. Following previous work for stock\nmovement prediction, we adopt the standard measure of Accu-\nracy (ACC) and Matthews Correlation Coefficient (MCC) as\nevaluation metrics. MCC avoids bias due to data skew. Given\nthe confusion matrix containing the number of samples clas-\nsified as true positive (tp), false positive (f p), true negative\n(tn) and false negative (f n), Acc. and MCC are calculated as\n\fTABLE IV : This table illustrates the effectiveness of the tweet\nfilter. By retaining only the tweets with the highest impres-\nsions, the results for predicting stock price movement and\nvolatility are consistent with those obtained using sentiment\nanalysis on all tweets.\neven though the number of tweets significantly decreased,\nAGRUD-A and AGRUD-F results remained almost the same.\nThis demonstrates that the filtered tweets effectively represent\npeople\u00e2\u20ac\u2122s genuine intentions. Consequently, this allows ECON\nto achieve more accurate forecasting results at a reduced cost\nand with greater efficiency.\nFor stock volatility prediction, given that data points in-\nvolving stock price changes greater than 5% only constitute\na minor portion of our dataset, using accuracy to evaluate\nECON can be misleading due to model overfitting. When\nevaluating the imbalanced data, we use Area Under the Curve\n- Receiver Operating Characteristic (AUC-ROC) where AUC\nis the area under the ROC curve, which falls in the range\n[0,1]. AUC is insensitive to the absolute numbers of positive\nand negative samples. It primarily focuses on how the model\ndistinguishes between positive and negative samples. AUC is\nthe area under the ROC curve, which falls in the range [0,1].\nAUC is calculated as follows:\nAU C =\n(cid:90) 1\n0\nT P R(F P R) dF P R,\n(20)\nwhere T P R(F P R) represents the True Positive Rate as a\nfunction of the False Positive Rate. The T P R gives the\nproportion of actual positives that are correctly identified.\nF P R is the False Positive Rate, indicating the proportion of\nnegative samples that are mistakenly identified as positive.\nB. Results\nThe performances of our ECON and the established base-\nlines are detailed in Table III. ECON is observed to be the\nsuperior baseline model in terms of accuracy and MCC for\nmovement prediction. While DTML shows an outstanding\nperformance in the AUC and MCC for volatility prediction.\nECON surpasses both of these models by significant margins.\nIn terms of accuracy, ECON achieves a score of 53.36 and\n62.84, outperforming SLOT and DTML by 1.7% and 2.7%\nrespectively, and outshines in AUC by a margin of 4.0%\nand 4.0%. We argue this improvement of price movement\nprediction is attributed to use of macro and micro trends\nto investigate the influence of multiple sectors on stocks.\nOn the other hand, leveraging macro insights and placing a\nstronger emphasis on recent information have also significantly\ncontributed to improved volatility forecasting.\nAs shown in Table V, our\nablation study revealed that incorporating multi-dimensional\nperception of stock market significantly enhances the pre-\ndictive capabilities of the model for stock movement and\nvolatility. A noteworthy point is that the result of ECON(A)\ndemonstrates a significant role of the macro trend in enhancing\nthe accuracy of volatility prediction."}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dPredicting stock market is vital for investors and\npolicymakers, acting as a barometer of the economic health. We\nleverage social media data, a potent source of public sentiment, in\ntandem with macroeconomic indicators as government-compiled\nstatistics, to refine stock market predictions. However, prior\nresearch using tweet data for stock market prediction faces\nthree challenges. First, the quality of tweets varies widely. While\nmany are filled with noise and irrelevant details, only a few\ngenuinely mirror the actual market scenario. Second, solely\nfocusing on the historical data of a particular stock without\nconsidering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly,\nsimply forecasting the direction of price movement without\nassessing its magnitude is of limited value, as the extent of the rise\nor fall truly determines profitability. In this paper, diverging from\nthe conventional methods, we pioneer an ECON (A Framework\nLeveraging Tweets, Macroeconomic Indicators, and Historical\nPrices to Predict Stock Movement and Volatility). The framework\nhas following advantages: First, ECON has an adept tweets filter\nthat efficiently extracts and decodes the vast array of tweet\ndata. Second, ECON discerns multi-level relationships among\nstocks, sectors, and macroeconomic factors through a self-aware\nmechanism in semantic space. Third, ECON offers enhanced\naccuracy in predicting substantial stock price fluctuations by\ncapitalizing on stock price movement. We showcase the state-\nof-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements\nand volatility.\n\nIndex Terms\u00e2\u20ac\u201dstock market prediction, tweets extraction, time\n\nseries forecasting, sentiment analysis, macroeconomic factors\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nThis version of MIMo extends a previous version published at ICDL [1].\n(a) Full body view\n(b) Facial expressions\nFig. 1. MIMo, the multimodal infant model. (a) MIMo sitting in a room with\ntoys. (b) Six facial expressions of MIMo.\nImportantly, these interactions are largely initiated and con-\ntrolled by the developing child. For example, an infant controls\nits visual, proprioceptive, and haptic inputs through its eye\nand body movements. This ability to control and \u00e2\u20ac\u0153experiment\u00ef\u00bf\u00bd\nwith different movements to observe their effects on sensory\ninputs may be crucial for cognitive development, giving the\ndeveloping mind a means to probe the causal structure of\nthe world, rather than merely passively observing correlations\namong sensed variables.\nImportantly, such an active, self-controlled form of learning\nis in stark contrast\nto a large body of work in Artificial\nIntelligence (AI), including large language or vision models,\nwhich learn certain aspects of the statistical structure of large\ntraining datasets without having any control over their inputs.\nDespite the impressive successes of such models, they may\nultimately be severely limited in their ability to understand\nthe causal mechanisms underlying their training data, which\nwill limit their ability to generalize in novel situations.\nThis should come as no surprise. Every scientist learns\nthat mere correlation is not sufficient evidence for inferring\ncausation. Therefore, scientists routinely exploit the ability to\nmanipulate a system under study using clever experimental\ndesigns where they interfere with some variables and observe\nthe effects on others to distill the causal mechanisms at work.\nTherefore, recreating the physical interaction with the envi-\nronment must be a central aspect of rebuilding cognitive devel-\nopment and it may be equally essential for achieving human-\n\n\flike intelligence and consciousness in AIs. The idea that an\nAI could learn like a developing child can be traced back\nall the way to Turing [2]. However, serious \u00e2\u20ac\u0153Developmental\nRobotics\u00ef\u00bf\u00bd or \u00e2\u20ac\u0153Developmental AI\u00ef\u00bf\u00bd efforts have become more\ncommon and practical only in the last 20 years, for reviews\nsee [3]\u00e2\u20ac\u201c[8].\nThere are two options for modeling the interaction of a\ndeveloping mind with its physical and social environment. The\nfirst option is using humanoid robots. This is the approach\ntaken by the Developmental Robotics community. The main\nadvantage is the inherent realism: the system operates in the\nreal world governed by the actual laws of physics. However,\nworking with humanoid robots is expensive, time-consuming,\nand suffers from the brittleness of today\u00e2\u20ac\u2122s humanoid hardware.\nAll these factors negatively impact the reproducibility of the\nresearch. Furthermore, the sensing abilities of today\u00e2\u20ac\u2122s robots\nare usually not comparable to those of actual humans. This\nis particularly problematic for the sense of touch. The human\nbody is covered by a flexible skin containing various types\nof mechanoreceptors, thermoreceptors, and nociceptors (pain\nreceptors). These allow us to sense touch, pressure, vibration,\ntemperature, and pain. Today, reproducing such a human-like\nskin in humanoid robots is still out of reach.\nThe second option for modeling the interaction of the\ndeveloping mind with its physical and social environment is\nto do it completely in silico. Many physics simulators or game\nengines are available today that can approximate the physics\nof such interactions [9]\u00e2\u20ac\u201c[11], for review see [12]. Among\nthe disadvantages of such an approach are 1) inaccuracies\nof such simulations due to inevitable approximations and\n2) the high computational costs, especially when non-rigid\nbody parts and objects are considered. Nevertheless, the in\nsilico approach avoids all\nthe problems of working with\nhumanoid robot hardware mentioned above. Furthermore, it\nensures perfect reproducibility. Lastly, if simulations can be\nrun (much) faster than real-time, this greatly facilitates the\nsimulation of developmental processes unfolding over long\nperiods of time (weeks, months, or even years).\nTo support such in silico research, we here present the\nopen source software platform MIMo, the Multi-Modal Infant\nModel (Fig. 1). MIMo is intended to support\ntwo kinds\nof research: 1) developing computational models of human\ncognitive development and 2) building developmental AIs that\ndevelop more human-like intelligence and consciousness by\nsimilarly exploiting their ability to probe the causal structure\nof the world through their actions.\nWe have decided to model the body of MIMo after an\naverage 18-month-old child. In total, MIMo has 82 degrees\nof freedom of the body and 6 degrees of freedom of the eyes.\nMIMo also features different facial expressions that can be\nused for studies of social development. To simulate MIMo\u00e2\u20ac\u2122s\ninteraction with the physical environment we use the MuJoCo\nphysics engine [13], because of its strength at simulating\ncontact physics with friction. Generally, we have aimed for\na balance between realism and computational efficiency in the\ndesign of MIMo. To accelerate the simulation of physics and\ntouch sensation, MIMo\u00e2\u20ac\u2122s body is composed of simple rigid\nshape primitives such as a sphere for the head and capsules\n2\nfor most other body parts. Presently, MIMo features four\nsensory modalities: binocular vision, proprioception, full-body\ntouch sensation, and a vestibular system. We also plan to add\naudition in the future.\nThe remainder of this article describes the detailed design\nof MIMo and illustrates how to use it. In particular, we present\nfour scenarios where MIMo learns to 1) reach for an object,\n2) stand up, 3) touch different locations on his body, and 4)\ncatch a falling ball with his five-fingered hand. These examples\nare used to illustrate and benchmark potential uses of MIMo.\nThey are not intended as faithful models of how infants acquire\nthese behaviors.\nThis paper is an extended version of preliminary work pre-\nviously published at the ICDL 2022 conference [1]. Compared\nto the preliminary MIMo version we have added 1) five-\nfingered hands, 2) a new actuation model that more accurately\nmodels the force-generating behaviour and compliance of real\nmuscles, 3) a detailed play room (see Fig. 2), in which MIMo\ncan interact, and 4) a new demo environment of learning\nto catch a falling ball. A fifth contribution is that we have\nimproved computational efficiency and benchmarks have been\nupdated accordingly.\nII. RELATED WORK\nWe focus on two major classes of software platforms for\nsimulating cognitive development during embodied interac-\ntions with the environment. The first kind is designed to\nsimulate a particular physical robot used in Developmental\nRobotics research and intended to complement the work with\nthat physical robot. The second kind of platform emulates human body and\nsensing abilities directly and thus is not restricted by limi-\ntations of current robotics technology in general or that of\nspecific robots in particular. \nVII. EXPERIMENTS\nA. Illustrations of learning\nWhile RL is a powerful framework to generate controllers,\nits chaotic nature and requirements for large amounts of data\nrequire a stable and efficient model. In the following, we\ndemonstrate that MIMo is well-suited for RL even while\nincluding multimodal inputs. Note that badly designed mod-\nels can negatively affect RL performance, even if they are\nphysically accurate in some regimes. We use four example\ntasks: reaching for objects, standing up, self-body knowledge\ntwo different\nand catching a falling ball. In these tasks,\nstate-of-the-art, widely used deep RL algorithms, Proximal\nPolicy Optimization (PPO) [54] and Soft Actor-Critic (SAC)\n[23] are trained to optimize task-dependent reward functions\nby controlling MiMo\u00e2\u20ac\u2122s actuators. While both algorithms are\nactor-critics, PPO is a widely-used on-policy approach closely\nrelated to trust-region algorithms [55] and SAC is an off-policy\nalgorithm with a maximum-entropy formulation [56].\nWe use the default parameters of the Stable-Baselines3\nlibrary [53], consisting of linear networks with two hidden\nlayers of size 64 for PPO and size 256 for SAC, as well\nas common improvements [57], [58] that are critical for\nperformance. Input and output\nlayer sizes vary depending\non the observation and action spaces of the environments.\nPerformance is compared for PPO and SAC with 10 differ-\nent seeds (Fig. 6). We do not claim that such extrinsically\nmotivated learning is how human infants learn these skills.\nWe merely use these examples to showcase MIMo learning\nfrom multimodal input.\n(a) Binocular vision\n(b) Touch perception\nFig. 5. MIMo\u00e2\u20ac\u2122s multimodal perception while holding a ball. (a) Anaglyph of\nleft and right eye views. (b) Visualization of touch sensors in the hand, with\nthose reporting a force colored red. The size of the circle corresponds to the\namount of force sensed.\nA. Touch Perception\nHuman touch sensation is produced by a variety of receptors\nresponding towards specific aspects of touch, such as the\nSlowly Adapting type 1 (SA1), which responds primarily to\ndirect pressure and coarse texture or the Rapidly Adapting\n(RA) type for slip and fine texture [50]. As with the other\nmodalities, we model these types very simply, ignoring signal\ntravel times and condensing the various types of receptors into\na single generic \u00e2\u20ac\u0153touch\u00ef\u00bf\u00bd sensor type that measures normal and\nfrictional forces at its location. Sensor points are spread evenly\non each individual body part, with the sensor density varying\nbetween body parts based on the two-point discrimination\ndistances by [51]. Thus, the front and the back of the palm\nhave the same sensor density, but not the fingertips.\nMuJoCo only performs rigid-body simulations in which all\ncontacts are treated as point contacts. Area contacts between\nflat surfaces produce multiple contact points, for example at\nthe corners of the contact area. As soft-body physics is very\ncomputationally expensive, we do not adjust these physics but\nweakly simulate the deformation of the skin to these point\ncontacts, by distributing the point contact forces over nearby\nsensor points according to a surface response function. Our\nresponse function decreases sensed force linearly with distance\nfrom the contact point and then normalizes over all sensor\npoints, such that the total sensed force remains identical to\nthe initial point contact force from MuJoCo. The distance\nmeasure is the euclidean distance as geodesic approximations\nwere too computationally expensive. To avoid bleed-through\nto the opposite side of thin bodies (such as the palm), we only\nselect nearby sensor points with a breadth-first-search on the\nmesh of sensor points. While also expensive, the results of this\nsearch can be cached and reused, leading to a only a small run\ntime penalty compared to using just the euclidean distance.\nAll of these aspects, from the sensor point density through\nthe response function can be easily adjusted or expanded.\nPerformance can be improved for specific experiments in\nmultiple ways. In addition to adjusting the configuration of\nthe modalities or the scene, the performance of the simulation\ncan be improved by reducing the frequency of control steps,\nreducing the frequency of observation collection and thus the\ntime spent in the sensory modules."}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dHuman intelligence and human consciousness\nemerge gradually during the process of cognitive development.\nUnderstanding this development is an essential aspect of un-\nderstanding the human mind and may facilitate the construc-\ntion of artificial minds with similar properties. Importantly,\nhuman cognitive development relies on embodied interactions\nwith the physical and social environment, which is perceived via\ncomplementary sensory modalities. These interactions allow the\ndeveloping mind to probe the causal structure of the world. This\nis in stark contrast to common machine learning approaches, e.g.,\nfor large language models, which are merely passively \u00e2\u20ac\u0153digesting\u00e2\u20ac\ufffd\nlarge amounts of training data, but are not in control of their\nsensory inputs. However, computational modeling of the kind\nof self-determined embodied interactions that lead to human\nintelligence and consciousness is a formidable challenge. Here\nwe present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simula-\ntions. MIMo\u00e2\u20ac\u2122s body is modeled after an 18-month-old child with\ndetailed five-fingered hands. MIMo perceives its surroundings via\nbinocular vision, a vestibular system, proprioception, and touch\nperception through a full-body virtual skin, while two different\nactuation models allow control of his body. We describe the design\nand interfaces of MIMo and provide examples illustrating its use.\nAll code is available at https://github.com/trieschlab/MIMo.\n\nIndex Terms\u00e2\u20ac\u201dcognitive development, developmental AI, infant\n\nmodel, multimodal perception, physics simulation\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nIntroduction\nIn our opinion, it is worth taking a step back and mea-\nsuring the goodness of XAI systems from the information\nthey can provide to users. Although measuring how much\ninformation a system can generate could be challenging, we\ncan focus on the amount of new knowledge that such flow\nof information creates in the users\u00e2\u20ac\u2122 mental models. Hence,\nif we let only non-expert users interact with XAI systems,\nwe can state that the knowledge they acquired arose from\nthe interaction with the system. Finally, if such knowledge\nis quantifiable, we can assess how much the system has been\ninformative.\nIn this paper, we aim to propose an assessment task to\nobjectively and quantitatively measure the goodness of XAI\nsystems during human-AI decision-making tasks intended\nas how much they are informative to non-expert users. More-\nover, we plan to use such a task to study the effectiveness\nof user-centred XAI in HRI scenarios. In particular, we are\ninterested in understanding whether a user-centred XAI ap-\nproach is more informative than a classical one.\nMethods\nWe are interested in introducing an objective and quantita-\ntive assessment task to measure the goodness of XAI sys-\ntems without relying only on indirect measures. Hence, we\nplan to measure the XAI systems\u00e2\u20ac\u2122 information power di-\nrectly and validate the task through a user study.\nThe task\nThe assessment consists of a decision-making task where\nusers can interact with a control panel to perform actions\nin a simulated environment (see Section ). During the task,\nusers can interact with an expert AI (in our case, with the\n2\n\f\nhumanoid robot iCub) by asking what it would do, and its\nXAI system by asking why it would do that. Apart from the\ninstructions about interacting with the control panel and the\nrobot, users start the task knowing nothing about it.\nIn a fixed amount of time, the users have to perform ac-\ntions and interact with the robot to discover (1) which is the\ntask at hand (e.g., what is the goal of the task), as well as\n(2) the rules that the AI model uses to make its actions and,\nsince the robot is an expert agent, (3) the rules that govern\nthe simulated environment.\nInteraction modalities The roles between the human-\nrobot team and their interaction modalities are simple. The\nrobot can not perform actions, but its role is limited to assist-\ning users during decision-making. However, the robot can\nnot take the initiative in giving suggestions either, but it al-\nways replies to users\u00e2\u20ac\u2122 questions. Thus, only the users can\ninteract with the control panel and act in the simulated envi-\nronment.\nWe expect the following HRI modalities. If the user al-\nready knows what to do, they act on the control panel. Oth-\nerwise, they ask the robot what it would do; then, the robot\nanswers this what question saying what action it would do in\nthe current scenario. If the user understands the robot\u00e2\u20ac\u2122s rea-\nsons, they act on the control panel (not necessarily the sug-\ngested one). Otherwise, the user asks the robot why it would\ndo that. At this point, the robot answers this why question by\nexplaining as justification for its suggestion. All that remains\nfor the user is to act on the control panel.\nCharacteristics of the task We need non-expert users to\nconsider the information passed through the interaction as\nnew information. Taking it to extremes, we consider only\nparticipants without knowledge about the task and its under-\nlying rules. For this reason, we implemented a simulation\nof a nuclear power plant management task (Figure 2). We\nchose this kind of task because it met all the requirements\nwe needed: it is challenging and captivating for non-expert\nusers, simple rules govern it, an AI model can learn those\nrules and, usually, people know nothing about the function-\ning of nuclear power plants.\nThe main objectives of the task (which we hide from\nusers) are to generate as much energy as possible and main-\ntain the system in an equilibrium state. The features of the\nenvironment are subject to rules and constraints, which we\ncan summarise as follow:\n\u00c2\u00b7 Each action corresponds to an effect on the environment:\nthus, a change of its features\u00e2\u20ac\u2122 value.\n\u00c2\u00b7 Several preconditions must be satisfied to start and con-\ntinue nuclear fission and produce energy.\n\u00c2\u00b7 Some conditions irremediably damage the plant.\n\u00c2\u00b7 The task is divided into steps of fixed time duration (e.g.,\n10 seconds), in which the users can interact with either\nthe robot or the control panel.\nThe simulated environment\nFeatures of the environment Our simulated power plant\nis composed of 4 continuous features:\n\u00c2\u00b7 The pressure in the reactor\u00e2\u20ac\u2122s core.\n\u00c2\u00b7 The temperature of the water in the reactor\n\u00c2\u00b7 The amount of water in the steam generator\n\u00c2\u00b7 The reactor\u00e2\u20ac\u2122s power.\nFurthermore, the power plant has four other discrete features\nthat regard the reactor\u00e2\u20ac\u2122s rods: security rods, fuel rods, sustain\nrods, and regulatory rods. The firsts two have two levels:\nup and down. Instead, the latter two have three levels: up,\nmedium, and down.\nThe reactor power linearly decreases over time for the ef-\nfect of the de-potentiation of the fuel rods. Hence, the reac-\ntor\u00e2\u20ac\u2122s power depends on the values of the environment\u00e2\u20ac\u2122s fea-\ntures and whether nuclear fission is taking place. Moreover,\n3\n\fthe energy produced at each step is computed by dividing the\nreactor\u00e2\u20ac\u2122s power by 360, which is the power that a 1000MW\nreactor without power dispersion produces in 10 seconds.\nActions to perform on the environment The actions that\nthe user can perform (12 in total) go from changing the po-\nsition of the rods to adding water to the steam generator or\nskipping to the next step. All those actions change the value\nof 3 parameters - T , P , and L - which correspond to the\nwater\u00e2\u20ac\u2122s temperature in the core, the core\u00e2\u20ac\u2122s pressure, and the\nwater level in the steam generator, respectively. The setting\nof the rods determines the entity of the feature updates; such\nupdates are performed at the end of each step, right after\nthe users\u00e2\u20ac\u2122 action. For example, if the safety rods are lowered\nin the reactor\u00e2\u20ac\u2122s core, the nuclear fission stops; thus, T and P\ndecrease until they reach their initial values, and the water in\nthe steam generator remains still. On the other hand, if nu-\nclear fission occurs and the user lowers the regulatory rods,\nthe fission accelerates. This acceleration consumes more wa-\nter in the steam generator, raising the core\u00e2\u20ac\u2122s temperature and\npressure more quickly, but also raising the reactor\u00e2\u20ac\u2122s power\nand the electricity produced. If the users do not act within\nthe time provided for each step, the application automati-\ncally chooses a skip action, which applies the features\u00e2\u20ac\u2122 up-\ndates based on the setting of the rods at hand.\nThe robot\u00e2\u20ac\u2122s AI\nRegarding the robot\u00e2\u20ac\u2122s AI model, we trained a deterministic\ndecision tree (DT) using the Conservative Q-Improvement\nlearning algorithm (Roth et al. 2019), which allowed us to\ntrain the DT using a reinforcement learning strategy. In-\nstead of extracting the DT from a more complex ML model\n(Vasilev, Mincheva, and Nikolov 2020)(Xiong, Zhang, and\nZhu 2017), we used this learning strategy to simplify the\ntranslation from the AI to the XAI without losing perfor-\nmance. The robot uses this expert DT to choose its action: it\ncan perform each of the 12 actions based on the eight envi-\nronment\u00e2\u20ac\u2122s features.\nStarting from its root node, the DT is queried on each of\nits internal nodes - which represent binary splits - to decide\nwhich of the two sub-trees continue the descent. Each inter-\nnal node regards a feature xi and a value for that feature vi:\nthe left sub-tree contains instances with values of xi \u00e2\u2030\u00a4 vi,\nwhile the right sub-tree contains instances with values of\nxi > vi (Buhrman and de Wolf 2002).\nThe DT\u00e2\u20ac\u2122s leaf nodes represent actions; in the implementa-\ntion of Roth et al. (Roth et al. 2019), they are defined with an\narray containing the actions\u00e2\u20ac\u2122 expected Q-values: the greater\nQ-value is associated with the most valuable action. This\nway, the DT can be queried by users with both what and why\nquestions. To answer a what question, we only need to nav-\nigate the DT using the current values of the environment\u00e2\u20ac\u2122s\nfeatures and present the resulting action to the user.\nThe robot\u00e2\u20ac\u2122s XAI\nAs we already saw in Section , answering why questions re-\ngards the robot XAI. Since the AI model to explain is already\ntransparent (Adadi and Berrada 2018), it also constitutes the\nXAI model. We can use DTs to provide explanations simply\nby using one (or more) of the feature values we encounter\nduring the descent.\nWhich of the features we encounter during descent to use\nis a problem called \u00e2\u20ac\u0153explanation selection\u00ef\u00bf\u00bd. In our case, the\nexplanation selection depends on the XAI strategy we want\nto test. For example, classical approaches use the most rel-\nevant features (in terms of the Gini index, information gain,\nor other well-established measures (Stoffel and Raileanu\n2001)). We plan to objectively and quantitatively compare\ntwo XAI strategies at first glance: a classical approach vs an\nuser-aware one.\nThe classical XAI explains using only the AI outcomes\nand environment\u00e2\u20ac\u2122s states. In particular, it justifies the robot\u00e2\u20ac\u2122s\nsuggestions by using the most relevant features, which are\nthe first ones in the DT\u00e2\u20ac\u2122s structure (see (Roth et al. 2019)).\nThe system tries to give always different explanations by\ntaking track of the DT\u00e2\u20ac\u2122s node already used and preferring\nto use the others in decreasing the level of relevance.\nOn the other hand, the user-aware XAI approach, through\nmonitoring and scaffolding (Figure 1), maintains a model of\nthe users to take track of their previous actions and predict\ntheir future behaviour (Rohlfing et al. 2020). Hence, the pre-\ndicted users\u00e2\u20ac\u2122 actions can be used to produce counterfactual\nexplanations: the fact (the outcome to explain) will be the\nrobot\u00e2\u20ac\u2122s suggestion; in contrast, the foil (the expected out-\n4\n\fcome) will be the predicted users\u00e2\u20ac\u2122 action. Figure 3 shows an\nexample of these two approaches.\nMeasuring the explanations\u00e2\u20ac\u2122 goodness\nThe assessment of the model\u00e2\u20ac\u2122s information power involves\nthe interaction between non-expert users and the system it-\nself. During this assessment, the users aim to learn as many\nsystem rules as possible. Thus, we need to collect measures\nfor each rule and combine them to obtain the model\u00e2\u20ac\u2122s infor-\nmation power. The general assessment steps are the follow-\ning:\n1. To quantify how many rules regard each feature and de-\nfine a method for measuring the number of learned rules\nrelative to each feature.\n2. To quantify how much informative weight each fea-\nture has. For the sake of simplicity, we can assume\nthat they have the same informative weight (equal to 1\nk ,\nwhere k is the number of features). The features\u00e2\u20ac\u2122 infor-\nmative weights must respect the following rule: \u00e2\u02c6\u20acj \u00e2\u02c6\u02c6\n{1, ..., k}, (cid:80)\nj \u00ce\u00b3j = 1. The features\u00e2\u20ac\u2122 informative weight\nis intended to describe how difficult it is to understand\nthe rules regarding such features.\n3. To measure the model\u00e2\u20ac\u2122s informative power for each user,\nobtaining a measure (and a set of secondary descriptive\nmeasures) for each of them.\n4. To average those measures obtaining the final results; the\nsecondary descriptive measures could also have valuable\nmeaning.\nHence, if k \u00e2\u02c6\u02c6 N is the number of the model\u00e2\u20ac\u2122s features,\nj \u00e2\u02c6\u02c6 N\n\u00ce\u00b3j \u00e2\u02c6\u02c6 [0, 1] is the informative weight of the feature j, nr\nis the number of rules regarding the feature j, nlr(i)\n\u00e2\u02c6\u02c6 N is\nthe number of rules regarding the feature j learned by the\nuser i, and am is the accuracy of the AI model m, then the\ninformative power of the model m for the user i is computed\nas follows:\nApart from the number of rules regarding each feature,\nthe most delicate aspect of the assessment regards the def-\ninition of the features\u00e2\u20ac\u2122 information weights. We suggest at\nleast two ways to set them: to make them all equal or to\ndefine the weights using experimental data. The former ap-\nproach depends on the task at hand. For our task (Section ),\nwe set the features\u00e2\u20ac\u2122 information weight by normalising the\nnumber of interactions with the system that users needed to\nunderstand those features. To state about which feature the\ninteraction regarded, we plan to proceed as follows: if the\ninteraction stopped at the request for suggestions, then the\ninteraction regards the feature on which the action suggested\neffects; otherwise, if the interaction continues with a request\nfor an explanation, then the interaction regards the feature\ncontained into the explanation.\nExperimental measures During our task, we plan to col-\nlect several quantitative measures to compute the model\u00e2\u20ac\u2122s\ninformation power as defined above:\n(M1) Measures of performance, such as the users\u00e2\u20ac\u2122 final score.\n(M2) Measures of rules understanding, such as the number of\ntask rules learned, the number of requests and interac-\ntions users needed to learn such rules, and the number of\ncorrect answers to the post-experiment test.\n(M3) Measures of generalisation, such as the number of correct\nanswers to what-if questions about the robot\u00e2\u20ac\u2122s decisions\nin particular states of the environment.\nMoreover, we collect some subjective measures:\n(M4) Measures of satisfaction, such as users\u00e2\u20ac\u2122 satisfaction level\nabout the explanations and the interaction.\n(M5) Measures of robot perception, such as users\u00e2\u20ac\u2122 feelings to-\nwards the robot and perception of it.\nDiscussion\nThe assessment task we propose satisfies properties unique\nto the XAI research field. Firstly, it focuses on the infor-\nmation power of the XAI system intended as the amount\nof information it can give to the user. Then, it defines the\ngoodness of the XAI system as a function of its information\npower. Secondly, it allows for an objective and quantitative\nanalysis of such goodness.\nThe most critical requirement of our assessment task is\nuser interaction. It allows for a two-way, possibly iterative,\ninteraction with the users. In particular, it allows the users\nto query the system by asking what it would do in a specific\nsituation and why. Consequently, the XAI system should be\nable to answer both what and why questions to exploit the\nfull potential of the assessment task.\nAnother essential factor of our task is that we can easily\ngeneralise it. To generalise our task, we need the following:\n\u00c2\u00b7 A decision-making task with characteristics similar to the\nones listed in Section .\n\u00c2\u00b7 An expert AI and several non-expert users.\n\u00c2\u00b7 The approaches to compare could be XAI algorithms or\nHRI dynamics.\n\u00c2\u00b7 At least one quantitative measure about users\u00e2\u20ac\u2122 under-\nstanding of the task: if two or more, a method to compact\nthem into a single measure is also needed.\n\u00c2\u00b7 At least one quantitative measure about users\u00e2\u20ac\u2122 ability to\ngeneralise to unseen scenarios: if two or more, a method\nto compact them into a single measure is also needed.\nAn assessment task with those characteristics is flexible\nenough to test different AI models and XAI techniques as\nlong as they allow interaction between the user and the sys-\ntem. Moreover, it can be used to objectively compare two\nor more HRI approaches to test whether HRI dynamics ease\nthe users in understanding the robot\u00e2\u20ac\u2122s suggestions and expla-\nnations. Indeed, in our future work, we plan to use this task\n5\n\fto test whether interacting with a social robot makes users\nmore receptive to learning than interacting with a non-social\none.\n"}, {"role": "assistant", "content": "Abstract\n\nThere is an increasing consensus about the effectiveness of\nuser-centred approaches in the explainable artificial intelli-\ngence (XAI) field. Indeed, the number and complexity of per-\nsonalised and user-centred approaches to XAI have rapidly\ngrown in recent years. Often, these works have a two-fold ob-\njective: (1) proposing novel XAI techniques able to consider\nthe users and (2) assessing the goodness of such techniques\nwith respect to others. From these new works, it emerged\nthat user-centred approaches to XAI positively affect the in-\nteraction between users and systems. However, so far, the\ngoodness of XAI systems has been measured through indi-\nrect measures, such as performance. In this paper, we propose\nan assessment task to objectively and quantitatively measure\nthe goodness of XAI systems in terms of their information\npower, which we intended as the amount of information the\nsystem provides to the users during the interaction. Moreover,\nwe plan to use our task to objectively compare two XAI tech-\nniques in a human-robot decision-making task to understand\ndeeper whether user-centred approaches are more informative\nthan classical ones.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nIn this work, we aim to model pedestrian behaviour in\nurban areas so as to create a risk map of the environment\nthat can then be used by other parts of the AV system to\nnavigate through such an area. We do this by modelling the\nsocial forces around the car in the environment, identifying\nthe different possible goals for each pedestrian in the scene\nand utilising the map information around each detected\npedestrian at each time step in their trajectory.\nIn summary, our contributions presented in this work are\nas follows: (1) we propose a novel model architecture that\ncaptures social and environmental interactions to accurately\nmodel spatio-temporal pedestrian trajectories over a short\nto medium term horizon (2) this architecture incorporates\na novel goal identification module that generates multiple\nlikely end points for each observed pedestrian partial trajec-\ntory. (3) A generative model that beats the state of the art\non two drone datasets proving that the model can generalise\nto similar environmental scenarios.\nIn our work, we take ideas from each of these systems such\nas using Transformers to capture temporal trajectories, Graph\nNetworks to capture varied and heterogeneous interactions\nand CVAEs to model the multi-modality of human behaviour.\nThe exact method in which these components are assembled\ntogether in our architecture will be described in the next\nsection.\nIII. THEORETICAL APPROACH\nA. Problem formulation\nOur problem of multimodally predicting pedestrian trajec-\ntories can be formally prescribed as follows. Starting with a\nbird\u00e2\u20ac\u2122s eye view (BEV) scene of an environment captured in\nRGB, semantically segmented into different classes, denoted\nby M contains a history of positions of each pedestrian in\nthis scene. For each scene, there exist a finite number of\ngoals that can be reached by a pedestrian denoted by G.\nEach pedestrian also interacts with every other agent in the\nscene, denoted by E.\nGiven this information, the model aims to predict the\ndistribution of pedestrian\u00e2\u20ac\u2122s trajectory for H timesteps in the\nfuture.\nB. Proposed model\nOur proposed model can be split into three main compo-\nnents, in two phases as depicted in Fig. 1. In the Encoding\nPhase, we encode the ego agent\u00e2\u20ac\u2122s interactions with the\nenvironment and other agents in the scene to create \u00e2\u20ac\u0153Social\nEmbeddings\u00ef\u00bf\u00bd. These Embeddings are used along with other\ndata to gererate multi-modal trajectories in the Generative\nPhase of the model. The three main components are the\nGraph Attention Encoder (GAE), the Temporal Transfmer\nEncoder-Decoder (TT) and the generative CVAE model for\nthe ultimate output of multi-modal trajectories. Each of these\ncomponents and their sub-components are described in detail\nbelow.\nFig. 1: GSGFormer model overview. Consider various VRUs in a\nstructured urban environment, in BEV as shown. Our model uses\nthe understanding of the interplay of social forces between different\nVRUs and the environment in which the motion is captured. A\nmap encoder captures the influence of the environment on the VRU\nunder consideration while the other encoders - for encoding the ego\nmotion of the VRU, identifying the different goals of this VRU and\nthose of its neighbours - are fed into a Graph Attention Network\nEncoder to capture a \u00e2\u20ac\u0153Social Embedding\u00ef\u00bf\u00bd within the environment.\nSocial Embeddings for each time step are then passed on to the\nGenerative Phase of the model, where different contexts from the\nTransformer Decoder and the CVAE Encoder are concatenated\nalong the last dimension of the tensor and passed to the Residual\nGMM generator. Finally, positions for a time interval h from current\ntime t are predicted by integrating these velocities over the time\ninterval dt.\n3) Goal Encoder: This subcomponent is one of our novel\ncontributions in this paper. We propose a 1D CNN to estimate\nthe goal position. Initially, The state of the ego agent for\neach history time step is embedded using the agent state\nencoder. Then the embedding is stacked over the time di-\nmension, resulting in a tensor of shape (n\u00c3\u2014dmodel). The 1D\nconvolutional neural network extracts the temporal feature\nbetween the n channels(time steps). The map embedding is\nconcatenated to the output of the goal encoder and passed\nto a linear layer. The output of the network is a vector of\nsize dmodel. The vector then serves as the input condition for\nthe CVAE-Residual-GMM module detailed further in III-B.6,\nfacilitating the generation of a range of potential goals.\n4) Social Graph Attention Neural Network: To model the\nsocial force acting on the ego agent, we introduce a hetero-\ngeneous graph representation. This representation comprises\nfour distinct nodes: the ego agent, neighboring agents, the\nsemantic map, and the goal populated by their respective\nembeddings. Physically, we only consider the effect of these\nembeddings unidirectionally on the ego agent at time step t.\nThen a heterogeneous graph attention network[35] performs\nboth node-level and semantic-level attention aggregating the\nsocial information to the ego agent node. The output is\nthe social force embedding in size of dmodel is embedded\nfrom the aggregated ego agent node. This embedding is then\nelement-wise combined with the state embedding of the ego\nagent, producing the social embedding at time step t.\n5) Temporal Transformer Model: Once the social infor-\nmation are encoded from the social graph of the ego agent\nat each time step, it is necessary to build the dependencies\nbetween graphs across different time steps. We employ a\nTransformer framework [12] to model the temporal relation-\nship. The history sequence of social embedding are fed into\nthe Transformer encoder as the source sequence with a fixed\nlength n. Since the primary operation in Transformer is the\nattention mechanism which is not sensitive to the relative or\nthe absolute position of the elements in the sequence, we add\nthe \u00ef\u00bf\u00bdpositional encodings\u00ef\u00bf\u00bd with the same dimension to each\nsocial embedding in the sequence. We compute positional\nencodings through sine and cosine functions, as described\nin [12], which allows the model to easily learn to attend by\nrelative positions. Meanwhile, it enables the model recognize\nany missing observations, as each comes with its distinct\npositional encoding.\n6) CVAE-Residual-GMM Module: We propose a gener-\native CVAE-Residual-GMM Module to realize the one-to-\nmany mapping for both goal and waypoints. The module is\ncomposed of two main parts: a Conditional Variational Auto-\nEncoder(CVAE) [36] and a Gaussian Mixture Model(GMM)\n[37]. Let\u00e2\u20ac\u2122s denote the input condition as X and its cor-\nresponding output (be it goal position or waypoint state)\nas Y. The high dimensional\nlatent variable Z is drawn\nfrom the prior distribution P\u00ce\u00b8(Z|X), facilitated by a prior\nthe\nnetwork. Throughout\nground truth is accessible, Z is drawn from the posterior\ndistribution Q\u00cf\u2022(Z|X, Y) by the recognition network. Y is\nfinally generated from the distribution P\u00cf\u02c6(Y|X, Z) through\nthe generation network.\nthe training phase, given that\nDifferent from the vanilla CVAE, the generated Y is the\nsum of two parts: the output \u00cb\u2020Y from CVAE decoder decoding\nthe concatenated (X, Z) and the residual \u00e2\u02c6\u2020Y, which adheres\nto a GMM distribution. This distribution is parameterized by\nan MLP, defining means, variances, and component weights\nbased on (X, Z). Notably, every waypoint within a single\ntrajectory utilizes a consistent Z for each sampling iteration.\n7) Training Strategy: During training, the most recent\nhistorical state, alongside the entire future state sequence, is\nconcurrently fed into the Transformer decoder. A subsequent\nmasking is applied to speed up the training and to ensure\npredictions for time step t rely solely on preceding time\n\fsteps. The latent variable Z is drawn both from P\u00ce\u00b8(Z|X)\nand Q\u00cf\u2022(Z|X, Y).\nIV. EXPERIMENTS AND RESULTS\nA. Datasets\nWe train and evaluate our model on three stationary BEV\ndatasets - the intersection Drone (inD) Dataset [33], the\nStanford Drone Dataset (SDD) [32] and the ETH/UCY\nDataset [38], [39].\ninD dataset contains 32 drone recordings of various VRU\ntrajectories of 5 different classes (pedestrian, bicycle, car,\ntruck and bus) in 4 different scenes at German intersections.\nThe raw data is samples in 25Hz. Training and test sets are\nsplit based on previous work done [40] such that scenes 1\nand 2 of the dataset are considered as training data, scene 3\nto be the validation set and scene 4 as the test set.\nSDD is a similar drone dataset, with 60 recordings at 8\nunique scenes in the Stanford university campus, each scene\ncontaining a mixture of 6 classes of VRUs (pedestrians,\nskateboarders, bicyclists, cars, cars and buses). The raw data\nis sampled at 30 Hz. The entire dataset is split in two specific\nways - one proposed in TrajNet [41] and another full split\nas in DESIRE [8].\nETH/UCY is a classic dataset widely used for the evalu-\nation for pedestrian trajectory prediction which contain only\ndata of pedestrians with rich human-human interactions. The\nraw data is extracted and annotated at 2.5Hz. We follow the\nleave-one-out strategy as presented in previous works [4], [7]\nfor evaluation.\nThe first two datasets are more representative of scenarios\nthat AVs can face in urban environments, especially in shared\nspaces than the classic ETH/UCY dataset. The background\nRGB frames in these datasets act as the maps for our model.\nIn each dataset, we only concentrate on predicting trajecto-\nries of the pedestrian VRU class while utilizing the effects of\nthe other VRU classes on these pedestrians. Another rationale\nfor utilizing similar datasets is to investigate our model\u00e2\u20ac\u2122s\nrobustness to different scenes and vantages.\nB. Experimental Setup\nFor each of the datasets chosen and all their corresponding\nscenes, we perform trajectory preprocessing as provided by\nOpenTraj tools [42]. All VRU trajectories are sampled at\n2.5Hz and velocities for these trajectories are calculated as\n\u00e2\u02c6\u2020X|t+1\n/\u00e2\u02c6\u2020t where X represents the sampled position at\nt\ntime t and \u00e2\u02c6\u2020t is the sampling interval. Each observed partial\ntrajectory lasts for 8 timesteps with a prediction horizon\nh of 12 timesteps thus implying a medium term trajectory\nprediction window of 4.8s after an observation window of\n3.2s, consistent with the baselines (IV-C) chosen to compare\nagainst our work.\nFor each trajectory, a distance of 15m (or equivalent) is\nchosen as an effective threshold for neighbour interactions\nwhile a crop patch of 10m (or equivalent) is chosen for\nmap interaction for social embeddings. Parentheses imply an\nestimation from pixel space to Cartesian space where direct\nmeasurements are unavailable in the dataset.\nThe model is set up to be trained on a single NVIDIA\nRTX A6000 GPU with 48 GB VRAM.\nC. Baselines and Metrics\na) Baselines: We compare our model\u00e2\u20ac\u2122s effectiveness\nwith different baselines for each of the datasets described\nin Section IV-A since each dataset has its own merits and\ndrawbacks. However, we explain some choices of baselines\nas below:\nSocial GAN [7]: One of the earliest generative architec-\ntures that focus on multi-modal trajectory prediction. They\napply a an LSTM Encoder-Decoder architecture connected\nby a social pooling module which used to aggregate the\nsocial interactions between actors in the scene.\nST-GAT [6]: ST-GAT is a classic work on human trajec-\ntory prediction which uses graph attention network to model\nhuman interactions. LSTM is applied to model the temporal\nfeature of the pedestrians.\nY-Net [11]: This multi-modal trajectory generation model\nuses goals, waypoints and segmented maps to predict pedes-\ntrian motion. However, it does not explicitly model any VRU\n\fMethod\nD. Implementation details\nAll input states - ego state, neighbour states, goal states -\nare embedded in a space of dmodel = 128. We use a map\npatch centered on the ego agent parsed through a pretrained\nUNet architecture to extract semantic segments from the\npatch and subsequently embed it in a space of 128. The\nGraph Attention Network is composed of two input layers\nof size 128 in mean aggregation mode with 4 attention heads\nfor each layer. The single layer Transformer encoder decoder\nmodule has a feed forward dimension set to 256 with 4 heads\nof multi-head attention. Each MLP in the CVAE encoder,\ndecoder and GMM consists of two hidden layers of size\n512 \u00e2\u2020\u2019 256 with ReLU activations on each. The dimension of\nthe latent variable is 16 for the goal and 32 for the waypoint.\nThe number of mixture components is set as 4 for goal and\n20 for waypoint.\nThe semantic segmentation network for the map encoder\nis a single model trained on both the drone datasets, with\nmanual annotations in 6 classes where unavailable as pre-\nsented in Sec. III-B.2.\nThe model is trained using the Adam optimizer with a\nlearning rate starting at 0.001 and decaying by a factor of\n0.5 every 10 epochs for a total of 50 epochs with a batch\nsize of 512.\nE. Quantitative Results\nWe organise our results by dataset, with the reported\nminADE and minFDE for 20 samples each. Clearly, for all\ndatasets, lower scores are better for both metrics with the\nbest result shown in bold. Table II compares metrics for the\nbaselines with ours. As observed, our model beats the state\nof the art in one metric while performing at the state of the\nart in the other.\nTable III shows the compared metrics for two different\nsplits for SDD - overlapping trajectories for the entire dataset\nat the top with the Trajnet split at the bottom. It can be\nseen that our model performs better than the state of the art\nwhen trained on the entire dataset compared to the Trajnet\nsplit. We conjecture that this is due to the Trajnet split not\ncontaining sufficient social interaction information among its\ntrajectories.\nTable I compares metrics of the baselines with our ap-\nproach, however as can be seen, our model does not perform\nas well on this dataset as the others. This is because of the\nrelative simplicity of ETH/UCY scenes and the inability of\nthe Map Encoder module of our approach to reach its full\npotential, proving our model performs best on large scale\ndatasets with environmental diversity.\nF. Qualitative Results\nFig. 3 presents the qualitative results of our model, de-\npicted here for the inD and SDD datasets. In all presented\ncases, we see that our model predicts trajectories accurately\nover time - even in complicated situations like fig. 3e where\na turn in the trajectory is captured and predicted as one of the\nsamples. In contrast, fig. 3f shows a case of another turning\ntrajectory with a less confident goal sampling, leading to a\nchaotic trajectory and thus, risk map, prediction.\n\f(a) Best of 20 result (inD)\n(e) Turning trajectory (inD)\n(b) Risk map (inD)\n(c) Best of 20 result (SDD)\n(d) Risk map (SDD)\n(f) Chaotic prediction (inD)\nA second conclusion we would like to draw from this work\nis how small the changes in the state of the art has become.\nWe would like to draw attention to Table II where the average\nprediction is smaller than the average stride of a human and\nwhile we have beaten the state of the art here, there is a\nplateauing of scores with changes only in the second decimal.\nIt is our conclusion that we as a community have reached\nthe limits of resolution for prediction in certain situations.\nACKNOWLEDGMENT\nThese\nresearches have been conducted within the\nAI4CCAM project, financed by the EU Project AI4CCAM\nwith grant number 101076911.\nG. Ablation Study\nWe performed an ablation study, module-by-module, of\nour proposed model and verified on the inD dataset for its\neffectiveness in prediction tasks with results shown in Table\nIV. Beginning by removing all component modules except\na single CVAE layer, trained by the l2 loss, produces an\nFDE and ADE as seen. Continuing, we add modules one\nafter another and compare against the minimum baseline\nproduced by the single CVAE layer. As seen,we notice a\nconsistent decline of ADE and FDE with the addition of\neach component module thereby showcasing the importance\nof each of the four enumerated components in our module\nas well as in pedestrian prediction tasks in general."}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201d Pedestrian trajectory prediction, vital for self-\ndriving cars and socially-aware robots, is complicated due to\nintricate interactions between pedestrians, their environment,\nand other Vulnerable Road Users. This paper presents GS-\nGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interac-\ntions and offering a plethora of potential modal behaviors. We\nincorporate a heterogeneous graph neural network to capture\ninteractions between pedestrians, semantic maps, and potential\ndestinations. The Transformer module extracts temporal fea-\ntures, while our novel CVAE-Residual-GMM module promotes\ndiverse behavioral modality generation. Through evaluations\non multiple public datasets, GSGFormer not only outperforms\nleading methods with ample data but also remains competitive\nwhen data is limited.\n\nIndex Terms\u00e2\u20ac\u201d Situational Awareness, Pedestrian Behaviour,\n\nAutonomous Vehicles.\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nABSTRACT\nModern commercial Heating, Ventilation, and Air Conditioning\n(HVAC) devices form a complex and interconnected thermody-\nnamic system with the building and outside weather conditions,\nand current setpoint control policies are not fully optimized for\nminimizing energy use and carbon emission. Given a suitable train-\ning environment, a Reinforcement Learning (RL) model is able to\nimprove upon these policies, but training such a model, especially\nin a way that scales to thousands of buildings, presents many real\nworld challenges. We propose a novel simulation-based approach,\nwhere a customized simulator is used to train the agent for each\nbuilding. Our open-source simulator1 is lightweight and calibrated\nvia telemetry from the building to reach a higher level of fidelity. On\na two-story, 68,000 square foot building, with 127 devices, we were\nable to calibrate our simulator to have just over half a degree of drift\nfrom the real world over a six-hour interval. This approach is an\nimportant step toward having a real-world RL control system that\ncan be scaled to many buildings, allowing for greater efficiency and\nresulting in reduced energy consumption and carbon emissions.\n\nKEYWORDS\nHVAC Optimization, Simulation, Reinforcement Learning\n\nACM Reference Format:\nJudah Goldfeder and John Sipple. 2023. A Lightweight Calibrated Simulation\nEnabling Efficient Offline Learning for Optimal Control of Real Buildings.\nIn Proceedings of RLEM \u00e2\u20ac\u212223. ACM, New York, NY, USA, 5 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n\n1 "}, {"role": "assistant", "content": "ABSTRACT\nModern commercial Heating, Ventilation, and Air Conditioning\n(HVAC) devices form a complex and interconnected thermody-\nnamic system with the building and outside weather conditions,\nand current setpoint control policies are not fully optimized for\nminimizing energy use and carbon emission. Given a suitable train-\ning environment, a Reinforcement Learning (RL) model is able to\nimprove upon these policies, but training such a model, especially\nin a way that scales to thousands of buildings, presents many real\nworld challenges. We propose a novel simulation-based approach,\nwhere a customized simulator is used to train the agent for each\nbuilding. Our open-source simulator1 is lightweight and calibrated\nvia telemetry from the building to reach a higher level of fidelity. On\na two-story, 68,000 square foot building, with 127 devices, we were\nable to calibrate our simulator to have just over half a degree of drift\nfrom the real world over a six-hour interval. This approach is an\nimportant step toward having a real-world RL control system that\ncan be scaled to many buildings, allowing for greater efficiency and\nresulting in reduced energy consumption and carbon emissions.\n\nKEYWORDS\nHVAC Optimization, Simulation, Reinforcement Learning\n\nACM Reference Format:\nJudah Goldfeder and John Sipple. 2023. A Lightweight Calibrated Simulation\nEnabling Efficient Offline Learning for Optimal Control of Real Buildings.\nIn Proceedings of RLEM \u00e2\u20ac\u212223. ACM, New York, NY, USA, 5 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n\n1 "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nAbstract\u00e2\u20ac\u201dPredicting stock market is vital for investors and\npolicymakers, acting as a barometer of the economic health. We\nleverage social media data, a potent source of public sentiment, in\ntandem with macroeconomic indicators as government-compiled\nstatistics, to refine stock market predictions. However, prior\nresearch using tweet data for stock market prediction faces\nthree challenges. First, the quality of tweets varies widely. While\nmany are filled with noise and irrelevant details, only a few\ngenuinely mirror the actual market scenario. Second, solely\nfocusing on the historical data of a particular stock without\nconsidering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly,\nsimply forecasting the direction of price movement without\nassessing its magnitude is of limited value, as the extent of the rise\nor fall truly determines profitability. In this paper, diverging from\nthe conventional methods, we pioneer an ECON (A Framework\nLeveraging Tweets, Macroeconomic Indicators, and Historical\nPrices to Predict Stock Movement and Volatility). The framework\nhas following advantages: First, ECON has an adept tweets filter\nthat efficiently extracts and decodes the vast array of tweet\ndata. Second, ECON discerns multi-level relationships among\nstocks, sectors, and macroeconomic factors through a self-aware\nmechanism in semantic space. Third, ECON offers enhanced\naccuracy in predicting substantial stock price fluctuations by\ncapitalizing on stock price movement. We showcase the state-\nof-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements\nand volatility.\n\nIndex Terms\u00e2\u20ac\u201dstock market prediction, tweets extraction, time\n\nseries forecasting, sentiment analysis, macroeconomic factors\n\nI. "}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dPredicting stock market is vital for investors and\npolicymakers, acting as a barometer of the economic health. We\nleverage social media data, a potent source of public sentiment, in\ntandem with macroeconomic indicators as government-compiled\nstatistics, to refine stock market predictions. However, prior\nresearch using tweet data for stock market prediction faces\nthree challenges. First, the quality of tweets varies widely. While\nmany are filled with noise and irrelevant details, only a few\ngenuinely mirror the actual market scenario. Second, solely\nfocusing on the historical data of a particular stock without\nconsidering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly,\nsimply forecasting the direction of price movement without\nassessing its magnitude is of limited value, as the extent of the rise\nor fall truly determines profitability. In this paper, diverging from\nthe conventional methods, we pioneer an ECON (A Framework\nLeveraging Tweets, Macroeconomic Indicators, and Historical\nPrices to Predict Stock Movement and Volatility). The framework\nhas following advantages: First, ECON has an adept tweets filter\nthat efficiently extracts and decodes the vast array of tweet\ndata. Second, ECON discerns multi-level relationships among\nstocks, sectors, and macroeconomic factors through a self-aware\nmechanism in semantic space. Third, ECON offers enhanced\naccuracy in predicting substantial stock price fluctuations by\ncapitalizing on stock price movement. We showcase the state-\nof-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements\nand volatility.\n\nIndex Terms\u00e2\u20ac\u201dstock market prediction, tweets extraction, time\n\nseries forecasting, sentiment analysis, macroeconomic factors\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nAbstract\u00e2\u20ac\u201dHuman intelligence and human consciousness\nemerge gradually during the process of cognitive development.\nUnderstanding this development is an essential aspect of un-\nderstanding the human mind and may facilitate the construc-\ntion of artificial minds with similar properties. Importantly,\nhuman cognitive development relies on embodied interactions\nwith the physical and social environment, which is perceived via\ncomplementary sensory modalities. These interactions allow the\ndeveloping mind to probe the causal structure of the world. This\nis in stark contrast to common machine learning approaches, e.g.,\nfor large language models, which are merely passively \u00e2\u20ac\u0153digesting\u00e2\u20ac\ufffd\nlarge amounts of training data, but are not in control of their\nsensory inputs. However, computational modeling of the kind\nof self-determined embodied interactions that lead to human\nintelligence and consciousness is a formidable challenge. Here\nwe present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simula-\ntions. MIMo\u00e2\u20ac\u2122s body is modeled after an 18-month-old child with\ndetailed five-fingered hands. MIMo perceives its surroundings via\nbinocular vision, a vestibular system, proprioception, and touch\nperception through a full-body virtual skin, while two different\nactuation models allow control of his body. We describe the design\nand interfaces of MIMo and provide examples illustrating its use.\nAll code is available at https://github.com/trieschlab/MIMo.\n\nIndex Terms\u00e2\u20ac\u201dcognitive development, developmental AI, infant\n\nmodel, multimodal perception, physics simulation\n\nI. "}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dHuman intelligence and human consciousness\nemerge gradually during the process of cognitive development.\nUnderstanding this development is an essential aspect of un-\nderstanding the human mind and may facilitate the construc-\ntion of artificial minds with similar properties. Importantly,\nhuman cognitive development relies on embodied interactions\nwith the physical and social environment, which is perceived via\ncomplementary sensory modalities. These interactions allow the\ndeveloping mind to probe the causal structure of the world. This\nis in stark contrast to common machine learning approaches, e.g.,\nfor large language models, which are merely passively \u00e2\u20ac\u0153digesting\u00e2\u20ac\ufffd\nlarge amounts of training data, but are not in control of their\nsensory inputs. However, computational modeling of the kind\nof self-determined embodied interactions that lead to human\nintelligence and consciousness is a formidable challenge. Here\nwe present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simula-\ntions. MIMo\u00e2\u20ac\u2122s body is modeled after an 18-month-old child with\ndetailed five-fingered hands. MIMo perceives its surroundings via\nbinocular vision, a vestibular system, proprioception, and touch\nperception through a full-body virtual skin, while two different\nactuation models allow control of his body. We describe the design\nand interfaces of MIMo and provide examples illustrating its use.\nAll code is available at https://github.com/trieschlab/MIMo.\n\nIndex Terms\u00e2\u20ac\u201dcognitive development, developmental AI, infant\n\nmodel, multimodal perception, physics simulation\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nAbstract\n\nThere is an increasing consensus about the effectiveness of\nuser-centred approaches in the explainable artificial intelli-\ngence (XAI) field. Indeed, the number and complexity of per-\nsonalised and user-centred approaches to XAI have rapidly\ngrown in recent years. Often, these works have a two-fold ob-\njective: (1) proposing novel XAI techniques able to consider\nthe users and (2) assessing the goodness of such techniques\nwith respect to others. From these new works, it emerged\nthat user-centred approaches to XAI positively affect the in-\nteraction between users and systems. However, so far, the\ngoodness of XAI systems has been measured through indi-\nrect measures, such as performance. In this paper, we propose\nan assessment task to objectively and quantitatively measure\nthe goodness of XAI systems in terms of their information\npower, which we intended as the amount of information the\nsystem provides to the users during the interaction. Moreover,\nwe plan to use our task to objectively compare two XAI tech-\nniques in a human-robot decision-making task to understand\ndeeper whether user-centred approaches are more informative\nthan classical ones.\n\n"}, {"role": "assistant", "content": "Abstract\n\nThere is an increasing consensus about the effectiveness of\nuser-centred approaches in the explainable artificial intelli-\ngence (XAI) field. Indeed, the number and complexity of per-\nsonalised and user-centred approaches to XAI have rapidly\ngrown in recent years. Often, these works have a two-fold ob-\njective: (1) proposing novel XAI techniques able to consider\nthe users and (2) assessing the goodness of such techniques\nwith respect to others. From these new works, it emerged\nthat user-centred approaches to XAI positively affect the in-\nteraction between users and systems. However, so far, the\ngoodness of XAI systems has been measured through indi-\nrect measures, such as performance. In this paper, we propose\nan assessment task to objectively and quantitatively measure\nthe goodness of XAI systems in terms of their information\npower, which we intended as the amount of information the\nsystem provides to the users during the interaction. Moreover,\nwe plan to use our task to objectively compare two XAI tech-\nniques in a human-robot decision-making task to understand\ndeeper whether user-centred approaches are more informative\nthan classical ones.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nAbstract\u00e2\u20ac\u201d Pedestrian trajectory prediction, vital for self-\ndriving cars and socially-aware robots, is complicated due to\nintricate interactions between pedestrians, their environment,\nand other Vulnerable Road Users. This paper presents GS-\nGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interac-\ntions and offering a plethora of potential modal behaviors. We\nincorporate a heterogeneous graph neural network to capture\ninteractions between pedestrians, semantic maps, and potential\ndestinations. The Transformer module extracts temporal fea-\ntures, while our novel CVAE-Residual-GMM module promotes\ndiverse behavioral modality generation. Through evaluations\non multiple public datasets, GSGFormer not only outperforms\nleading methods with ample data but also remains competitive\nwhen data is limited.\n\nIndex Terms\u00e2\u20ac\u201d Situational Awareness, Pedestrian Behaviour,\n\nAutonomous Vehicles.\n\nI. "}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201d Pedestrian trajectory prediction, vital for self-\ndriving cars and socially-aware robots, is complicated due to\nintricate interactions between pedestrians, their environment,\nand other Vulnerable Road Users. This paper presents GS-\nGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interac-\ntions and offering a plethora of potential modal behaviors. We\nincorporate a heterogeneous graph neural network to capture\ninteractions between pedestrians, semantic maps, and potential\ndestinations. The Transformer module extracts temporal fea-\ntures, while our novel CVAE-Residual-GMM module promotes\ndiverse behavioral modality generation. Through evaluations\non multiple public datasets, GSGFormer not only outperforms\nleading methods with ample data but also remains competitive\nwhen data is limited.\n\nIndex Terms\u00e2\u20ac\u201d Situational Awareness, Pedestrian Behaviour,\n\nAutonomous Vehicles.\n\nI. "}]}