{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nOur\ncontributions include a highly customizable and scalable HVAC and\nbuilding simulator, a rapid configuration method to customize the\nsimulator to a particular building, a calibration method to improve\nthis fidelity using real world data, and an evaluation method to\nmeasure the simulator fidelity.\nOverview of the HVAC problem Most office buildings are\nequipped with advanced HVAC devices, like Variable Air Volume\n(VAV) devices, Hot Water Systems (HWS), Air Conditioner (AC)\nand Air Handlers that are configured and tuned by the engineers,\nmanufacturers, installers, and operators to run efficiently with the\ndevice\u00e2\u20ac\u2122s local control loops [15]. However, integrating multiple\nHVAC devices from diverse vendors into a building \u00e2\u20ac\u0153system\u00ef\u00bf\u00bd re-\nquires technicians to program fixed operating conditions for these\nunits, which may not be optimal for every building and every poten-\ntial weather condition. If existing setpoint control policies are not\noptimal under all conditions, the possibility exists that an ML model\nmay be trained to continuously tune a small number of setpoints\nto achieve greater energy efficiency and reduced carbon emission\n[22].\nRL for HVAC systems We define the state of the office building\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd at time \u00ef\u00bf\u00bd as a fixed length vector of measurements from sensors on\nthe building\u00e2\u20ac\u2122s devices, such as a specific VAV\u00e2\u20ac\u2122s zone air temperature,\ngas meter\u00e2\u20ac\u2122s flow rate, etc. The action on the building \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd is a fixed-\nlength vector of device setpoints selected by the agent at time \u00ef\u00bf\u00bd,\nsuch as the boiler\u00e2\u20ac\u2122s supply water temperature setpoint, etc. We\nalso define a custom feedback signal, or reward, that indicates the\nquality of taking an action in the current state, as a weighted sum of\nnegative cost functions for carbon emission, energy consumption,\nand zone-level setpoint deviation. The goal of RL is to maximize the\nexpected long-term cumulative reward [18]. In order to converge\nto the optimal policy, the agent requires many training iterations,\nmaking training directly on the building from scratch inefficient\nand impracticable. Therefore, it is necessary to train in an offline\nsandbox environment that adequately emulates the dynamics of\nthe building before being deployed on the actual building.\n2 RELATED WORK\nConsiderable attention has been paid to HVAC control in recent\nyears, and a growing portion of that has considered how RL and its\nvarious associated algorithms can be leveraged [8, 14, 20, 23, 24].\nA central requirement in RL is the offline environment that trains\nthe RL agent. Several methods for this have been proposed, largely\nfalling under three broad categories.\nData-driven Emulators: Some works attempt to learn a dy-\nnamics model as a multivariate regression model from real world\ndata [25, 29], often using recurrent neural network architectures,\nsuch as LSTMs [16, 19, 28? ]. The difficulty here is that data-driven\nmodels often do not generalize well to circumstances outside the\ntraining distribution, especially since they are not physics based.\nPhysics-based Simulation: EnergyPlus [7], a high-fidelity sim-\nulator developed by the Department of Energy, is commonly used\n[2, 21, 22, 27], but suffers from the scalability issues outlined above.\nOffline RL: The second approach is to train the agent directly\nfrom the historical real world data, without ever producing an in-\nteractive environment [4\u00e2\u20ac\u201c6]. While the real world data is obviously\nof high accuracy and quality, this presents a major challenge, since\nthe agent cannot take actions in the real world and interact with\nany form of an environment, severely limiting its ability to improve\nover the baseline policy that produced the real world data [10].\nTo overcome the limitations of each of the above three methods,\nsome work has proposed a hybrid approach [3, 26], and this is the\ncategory our work falls under. Our approach uses a physics-based\nsimulator that achieves an ideal balance between speed and fidelity,\nwhich is sufficient to train an effective control agent off-line.\n3 A SIMPLE CALIBRATED SIMULATION\nDesign Considerations A fundamental tradeoff when designing a\nsimulator is speed versus fidelity. Fidelity is the simulator\u00e2\u20ac\u2122s ability to\nreproduce the building\u00e2\u20ac\u2122s true dynamics that affect the optimization\nprocess. Speed refers to both simulator configuration time, and\nagent training time, i.e., the time necessary for the agent to optimize\nits policy using the simulator. Every building is unique, due to\nits physical layout, equipment, and location. Fully customizing\na high fidelity simulation to a specific target building requires\nnearly exhaustive knowledge of the building structure, materials,\nlocation, etc., some of which is unknowable. Also, the configuration\ntime required for high-fidelity simulations limits their utility for\ndeploying RL-based optimization to many buildings. Our goal is to\ndevelop a method for applying RL at scale to commercial buildings.\nTo this end, we must have a simulated environment to train the\nagent, as real world training is not possible. In order to scale to many\nbuildings it must be easy for our simulator to be configured to a\nnew building, with enough fidelity to the real world to be useful. We\nnote that while RL is the control method proposed in this work, our\ncalibrated simulator is useful as well for other controller methods\nsuch as Model Predictive Control [9]. To meet these requirements\nwe designed a lightweight simulator based on Finite Differences\napproximation of heat exchange, with a novel automated procedure\nto go from building floor plans to a custom simulator with little\nmanual effort. For improved fidelity, we designed a calibration and\nevaluation pipeline based on real telemetry.\nWe selected water supply temperature \u00cb\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd and the air handler\nsupply temperature \u00cb\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd as agent actions because they affect the\nbalance of electricity and natural gas consumption, they affect\nmultiple device interactions, and they affect occupant comfort.\nFinite Differences Approximation The diffusion of thermal\nenergy in time and space of the building can be approximated using\nthe method of Finite Differences (FD)[12, 17], and applying an\nenergy balance. This method divides each floor of the building into\na grid of three-dimensional control volumes and applies thermal\ndiffusion equations to estimate the temperature of each control\nvolume. By assuming each floor is thermally isolated, (i.e., no heat\nis transferred between floors), we can simplify the three-spatial\ndimensions into a spatial two-dimensional heat transfer problem.\nFinally, the thermal energy associated with the VAV (c) is equally\ndistributed to all associated control volumes that have a diffuser.\nThermal diffusion within the building is mainly accomplished\nvia forced or natural convection currents, which can be notoriously\ndifficult to estimate accurately. We note that heat transfer using air\ncirculation is effectively the exchange of air mass between control\nvolumes, which we approximate by a randomized shuffling of air\nwithin zones, parameterized by a shuffle probability.\nHVAC Model The HVAC system is modeled as an energy bal-\nance with a lossy thermal exchange between the hot water and air\ncircuits, evaluating temperature differences, and air and water flow\nrates with efficiency parameters. Aggregate heating and cooling\ndemands at the AC and HWS are governed by thermostats that\ncreate demand when its zone temperature is near zone air tempera-\nture setpoints. Special control volumes associated with air diffusers\nallow thermal energy exchange between the HVAC model and the\nFD grid, (cid:164)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd .\nSimulator Configuration For RL to scale to many buildings, it\nis critical to be able to easily and rapidly configure the simulator\nto any arbitrary building. To configure the simulator, we require\nfloorplans, and HVAC metadata. We preprocess the detailed floor-\nplan blueprints of the building, and extract a grid that gives us an\napproximate placement of walls. We also employ a user interface\nto label the location of each HVAC device on the floorplan grid.\nWe tested this pipeline on our pilot building, which consisted of\ntwo floors with combined surface area of 68,000 ft2, and 127 HVAC\ndevices. Given floorplans and HVAC layout information, a single\ntechnician was able to generate a fully specified simulation in under\n3 hours that matched the real building in every device and room.\nSimulator Calibration and Evaluation In order to calibrate\nthe simulator to the real world using data, we must have a metric\nwith which to evaluate our simulator, and an optimization method\nto improve our simulator on this metric. We proposed a novel\nevaluation procedure, based on \u00ef\u00bf\u00bd -step prediction. Each iteration\nstep of our simulator was designed to represent a five minute in-\nterval, and our real world data is also sampled in five minute in-\ntervals. To evaluate the simulator, we take a chunk of real data\nof \u00ef\u00bf\u00bd observations. We then initialize the simulator so that its ini-\ntial state matches that of the starting observation, and run the\nsimulator for \u00ef\u00bf\u00bd steps, replaying the same HVAC policy as was\nused in the real world observations. Next, we calculate our sim-\nulation fidelity metric, which is the mean absolute error of the\nmeasured zone air temperatures, at the \u00ef\u00bf\u00bd th timestep.\nSimulator Calibration Once we defined our simulation fidelity\nmetric, we can minimize the error by searching over the physical\nparameters that affect the simulation response. The variables tuned\nduring the parameter search included the following: (a) the forced\nconvection coefficient quantifying outside wind and air currents\nagainst the building exterior surfaces; (b) the thermal conductivity,\nheat capacity, and density of exterior and interior walls; and (c)\nthe shuffle probability that controls how likely an air volume will\nbe exchanged with another within the zone to approximate the\ninternal air circulation and interior forced convection.\n4 EXPERIMENT RESULTS\nWe now demonstrate the results of how our simulator, when tuned\nand calibrated, is able to make real-world predictions.\nExperiment Setup To test out our simulator, we obtained teleme-\ntry recordings from our pilot building, a commercial office building\nlocated in Mountain View, California. The building has two stories\nwith a combined surface area of 68,000 square feet, and has 127\nHVAC devices. We obtained floor plan blueprints and used them to\nconfigure a customized simulator for the building, a process that\ntook a single human less than three hours to complete.\nCalibration Data To tune and evaluate our simulator, we took\nthree, non-overlapping intervals of telemetry: (a) Thursday, July\n6 2023, from 1:40 AM to 7:40 AM (\u00ef\u00bf\u00bd = 72), (b) Friday, July 7 2023,\nfrom 1:40 AM to 7:40 AM (\u00ef\u00bf\u00bd = 72), and (c) from Friday, July 7 2023,\nfrom 11:40 AM to 5:40 PM (\u00ef\u00bf\u00bd = 72). The first telemetry interval,\n(a), was used to tune the simulator for calibration. The second and\nthird telemetry intervals, (b) and (c), were used for validation.\nCalibration Procedure We tuned for 100 iterations on our\nsimulator. The parameters varied, the sweep range limits, and the\nvalues found that minimized the calibration metric are shown in\nTable 1. Because of our focus on speed, we can similate a five\nminute interval in about 15 seconds on one CPU, allowing for very\nrapid tuning. We reviewed the parameters that yielded the lowest\nsimulation error from calibration. Densities, heat capacities, and\nconductivities plausibly matched common interior and exterior\nbuilding materials. However, the external convection coefficient\nwas higher than under normal weather conditions, and likely is\ncompensating for radiative losses and gains, which were not directly\nsimulated.\nCalibration Results Table 2 shows the \u00ef\u00bf\u00bd -step prediction fi-\ndelity, as MAE, over a six-hour prediction window (\u00ef\u00bf\u00bd = 72). We\ncalculated the spatial mean absolute temperature error, as defined\nabove. We also present a second metric, the median spatial temper-\nature error. This was not used in the tuning process, but gives us\nsome insight into how well the calibration process is performing.\nAs indicated in Table 2, our tuning procedure drifts only 0.6\u00e2\u2014\u00a6 on\naverage over a 6 hour period on the tuning set, and we get good\ngeneralization: almost identical error for the same time period of a\ndifferent day. It should be noted that an uncalibrated model had a\nmuch larger error of 2.1\u00e2\u2014\u00a6. Furthermore, the median temperature in\nthese two cases is near zero, indicating that even despite drift, our\nprediction remains a mostly unbiased estimator. For the different\ntime of day scenario, we observe a larger drift of 1.2\u00e2\u2014\u00a6, and the me-\ndian is shifted as well, indicated that tuning on one time of day is\nnot as useful when making predictions on another.\nVisualizing Temperature Drift Over Time Figure 2 illustrates\ntemperature drift over time. At each time step, we plot the real world\ntemperatures in a blue boxplot and the simulator ones in orange.\nVisualizing Spatial Errors Figure 3 is a heatmap of the spatial\ntemperature difference across both floors after six hours. Red re-\ngions indicate the simulator was warmer than the validation data,\nblue regions indicate cooler, and white regions had no temperature\ndifference. The ring of blue around the building indicates that our\nsimulator is too cold on the perimeter, which implies that the heat\nexchange with the outside is happening more rapidly than it would\nin the real world. The inside of the building remains red, which\nmeans that despite the simulator perimeter being cooler than the\nreal world, the inside is warmer. The white band region running\nalong the perimeter is where the temperature achieved a perfect\nfit with the validation data. The interior red regions are due to\nslight errors in modeling interior heat exchange, and the blue re-\ngions indicate errors due to high thermal losses with the outside.\nWe suspect these errors are largely due to assuming thermal isola-\ntion between floors and assuming no radiative exchange with the\noutside. Consequently, the calibration process compensates with\na high convection coefficient. Internal thermal exchange seems\nslower than the real world, and the calibration compensates with\nmore rapid outside thermal exchange."}, {"role": "assistant", "content": "ABSTRACT\nModern commercial Heating, Ventilation, and Air Conditioning\n(HVAC) devices form a complex and interconnected thermody-\nnamic system with the building and outside weather conditions,\nand current setpoint control policies are not fully optimized for\nminimizing energy use and carbon emission. Given a suitable train-\ning environment, a Reinforcement Learning (RL) model is able to\nimprove upon these policies, but training such a model, especially\nin a way that scales to thousands of buildings, presents many real\nworld challenges. We propose a novel simulation-based approach,\nwhere a customized simulator is used to train the agent for each\nbuilding. Our open-source simulator1 is lightweight and calibrated\nvia telemetry from the building to reach a higher level of fidelity. On\na two-story, 68,000 square foot building, with 127 devices, we were\nable to calibrate our simulator to have just over half a degree of drift\nfrom the real world over a six-hour interval. This approach is an\nimportant step toward having a real-world RL control system that\ncan be scaled to many buildings, allowing for greater efficiency and\nresulting in reduced energy consumption and carbon emissions.\n\nKEYWORDS\nHVAC Optimization, Simulation, Reinforcement Learning\n\nACM Reference Format:\nJudah Goldfeder and John Sipple. 2023. A Lightweight Calibrated Simulation\nEnabling Efficient Offline Learning for Optimal Control of Real Buildings.\nIn Proceedings of RLEM \u00e2\u20ac\u212223. ACM, New York, NY, USA, 5 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n\n1 "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nAcknowledg-\ning the inherent unpredictability of precise stock prices [2],\nour research leverages these blue-chip stocks to forecast future\ntrends in stock price movement and volatility [3], [4].\nResearch within the realm of the stock market generally\nfalls into two branches: technical analysis and fundamental\nanalysis. Moreover, the existing\nmodels are primarily focused on predicting whether trends will\nchange [10], but overlook the significance of the magnitude of\nsuch changes. In reality, the extent of these changes constitutes\nan important part of stock behavior.\nIn this paper,\nto address\nthe aforementioned limita-\n3GICS is a method for assigning companies to a specific economic sector\nand industry group that best defines its business operations.\n1The ratio is a measure of the total value of all publicly traded stocks in a\n4Investment-worthy refers to high-quality companies rated Baa or higher\nmarket divided by that economy\u00e2\u20ac\u2122s gross domestic product (GDP).\naccording to evaluations by Moody\u00e2\u20ac\u2122s and Standard & Poor\u00e2\u20ac\u2122s (S&P).\n2A blue chip stock is stock issued by a large, well-established, financially-\n5Although Twitter has recently been re-branded to \u00e2\u20ac\u0153X\u00ef\u00bf\u00bd, this article contin-\nsound company with an excellent reputation.\nues to use the original name,\nIn this study, we aim to overcome the limitations of previous\nmethodologies. We adopt an efficient tweet filter to eliminate\nredundant content and reduce noise. Additionally, we embrace\na multi-dimensional approach to the stock market, integrating\ncorrelations of stock market information in a multi-task man-\nner. By skillfully combining the strengths of both technical and\nmodel, we achieve state-of-the-art performance in predicting\nstock movements.\nIII. PROBLEM SETUP\nWe offer a formal definition for the problem of predicting\nstock movement and volatility. Let S denote a set of target\nstocks for which predictions are sought. We define a set xst\nof feature vectors, with s \u00e2\u02c6\u02c6 S and t \u00e2\u02c6\u02c6 T , which encapsulate\nhistorical prices. Let T represents the set of available training\ndays. In addition, we identify a set \u00ce\u00b5 consisting of tweets,\neach referencing at least one stock in S. These historical\nprices incorporate the opening, highest, lowest, closing prices\nand adjusted closing price7 for each stock. Using P to\nsignify the adjusted closing price of stock s on day t as\nP = {Ps,1, . . . , Pst}, we then express the actual labels for\nmovement and volatility as follows:\nWe usually can generate multiple training examples by\nshifting the time lag within extensive historical stock data in\npractical situations. However, in order to simplify the expla-\nnation of our proposed method, we focus on a specific time\nlag for both movement and volatility prediction. We\u00e2\u20ac\u2122ve also\nintegrated the use of adjusted stock prices into our predictive\nmodels. [23]. This allows our models to learn from the genuine\nchanges in a stock\u00e2\u20ac\u2122s value that occur due to market conditions,\nrather than being swayed by artificial fluctuations resulting\nfrom corporate actions.\nIV. ECON STRUCTURE\nWe present an overview of data ingestion, text preprocess-\ning, and model architecture, as illustrated in Figure 1. Initially,\nwe process the historical stock prices and macroeconomic\nfactors to distill pertinent features. Following this, we sift\nthrough Twitter data to identify tweets that most aptly reflect\npublic sentiment. During text preprocessing, we mask stock\ntickers associated with companies found in the curated tweets.\nThese masked tweets are then transformed into embeddings.\nLeveraging a self-aware mechanism, we generate embeddings\nfor both sectors and tweets. In the concluding step, we amal-\ngamate information from stocks, sectors, and macroeconomic\nindicators in a multi-trends approach to predict stock price\nmovement and volatility.\n In our research, we consistently selected the\ntop six tweets, those garnering the highest daily impressions,\nas our primary input for text preprocessing.\nSentiment Analysis. In our research, we undertook a sen-\ntiment analysis on a dataset comprising 7.7 million tweets.\nTo achieve this, we leveraged a language model based on\nRoberta8, and the model has been fine-tuned on Twitter data.\nThis optimized model can produce one of three sentiment\noutcomes for each tweet: positive, neutral, or negative. For\nevery tweet e associated with a blue-chip stock s, we compared\nthe predicted sentiment pe with the same day\u00e2\u20ac\u2122s binary adjusted\nprice movement ys\nm gauges the\nprice fluctuation between days t \u00e2\u02c6\u2019 1 and t. Same as problem\nsetup, if the stock price ascends in this interval, ys\nm is labeled\n\u00e2\u20ac\u0153Up\u00ef\u00bf\u00bd, and conversely, if it declines, it\u00e2\u20ac\u2122s labeled \u00e2\u20ac\u0153Down\u00ef\u00bf\u00bd. It\u00e2\u20ac\u2122s\npertinent to note that our dataset only incorporated tweets from\nthe trading hours of day t. To aggregate daily sentiment, if a\nday witnesses a predominance of negative sentiments in tweets\nconcerning a specific stock, we designate that day as \u00e2\u20ac\u0153nega-\ntive\u00ef\u00bf\u00bd. In contrast, if the tweets are primarily characterized by\npositive sentiments, the day is classified as \u00e2\u20ac\u0153positive\u00ef\u00bf\u00bd.\nm of stock s. The measure ys\nTweets extracting. In addition to textual information, we\ngathered metrics on likes, retweets, and impressions for each\ntweet.\nV 0.0394 0.0643 0.0747 0.0816 0.0833 0.0877 0.0872 0.0853 ... 0.0917\nC. Text preprocessing\nIn natural language processing (NLP), text preprocessing\nhas long been recognized as a key step. By streamlining\ntext into a format more amenable to training, it not only\nsimplifies the content but also boosts the efficiency of machine\nlearning algorithms. Shifting focus to the concept of self-\naware mechanism, our main objective is pinpointing sector\nidentification. Within this context, the embeddings of both\nsectors and tweets are crucial. When the stock symbol s is\nmasked, we aim to identify the sector c a tweet e refers to\nbased solely on its content.\nTokenization. Tokenization is the process by which text is\ndivided into smaller units, such as sentences, words, charac-\nters, or subwords. Word tokenization, specifically, refers to the\nsegmentation of text into individual words. In our research, we\nutilized spaCy9\u00e2\u20ac\u201dan open-source natural language processing\ntool\u00e2\u20ac\u201dfor word tokenization of every tweet e. Moreover, a\n9https://spacy.io/\n\fstock ticker like \u00e2\u20ac\u0153AAPL\u00ef\u00bf\u00bd for Apple might inadvertently be\ntokenized into \u00e2\u20ac\u0153AA\u00ef\u00bf\u00bd and \u00e2\u20ac\u0153PL\u00ef\u00bf\u00bd. To circumvent such issues,\nwe established specific rules to ensure that stock tickers are\nconsistently recognized as single, intact tokens. After tokeniza-\ntion, words are converted to lowercase because stock tickers\nwon\u00e2\u20ac\u2122t overlap with regular vocabulary. Such normalization not\nonly ensures vocabulary consistency across the dataset but also\naugments computational efficiency.\nCompany Masking. For the subsequent self-awareness\nportion of the model architecture, we replace all tokens in\n\u00ce\u00b5s that correspond to the name of stock s with a special token\n[mask]. Based on the sector classification rules of GISC, we\nmaintained a dictionary mapping the masked companies to\ntheir respective sectors. For example, after data filtering, we\nobtain a tweet with a high impression: \u00e2\u20ac\u0153With Vision pro,\n$AAPL is about to soar!\u00ef\u00bf\u00bd Next, we mask the stock ticker,\nconverting it to: \u00e2\u20ac\u0153With Vision pro, [mask] is about to soar!\u00ef\u00bf\u00bd\nSentence encoding. Words from the sentence are first\ntokenized and then mapped to their corresponding word em-\nbeddings. These embeddings are high-dimensional vectors that\ncapture the intrinsic semantics of each word. We record the\noriginal length of each tweet, storing this information in the\nvariable l. This data proves crucial when padding comes\ninto play, a technique used to append or prepend sequences\nwith filler values, ensuring uniformity. Given the importance\nof maintaining consistent sequence lengths, especially when\nworking in batches, padding ensures our dataset\u00e2\u20ac\u2122s sequences\nadhere to a standardized length, adding consistency and struc-\nture to our data processing pipeline.\nIn our approach, we adopted a\nbi-directional LSTM (BiLSTM) for the predictor network to\nproduce the tweet and sector embeddings.\nThe first attention is to find tweets relevant to each target sector\nc. We find relevant tweets by using the embedding hc of sector\nc as the query of attention as follows:\nlet \u00ce\u00b5t represent\nConsidering a day t,\nthe collection of\ntweets. For each tweet e within this set, its corresponding\nembedding is denoted by he. Additionally, \u00ce\u00b1e signifies the\nattention weight attributed to the tweet. The vector wct \u00e2\u02c6\u02c6 R2k,\nwhich can be envisioned as a weighted mean of all the tweet\nembeddings for day t. This averaging is influenced by the\npertinence of each tweet to the specific target sector c. Then,\nwe employ a linear layer to project wct onto all possible\ncompanies S:\ngst = W1(wct) + b1,\n(10)\nwhere W1 \u00e2\u02c6\u02c6 Rn\u00c3\u20142k, b1 \u00e2\u02c6\u02c6 Rn. As a result of the attention\nfunction of Equation (10), gst \u00e2\u02c6\u02c6 Rn represent that summarizes\nall tweets at day t considering the relevance to sector c. Our\nsecond attention uses the resulting tweet vector to find the\nmost relevant stocks at the moment. This is done by using gst\nas the query vector of attention to aggregate the stock price\nfeatures at day t based on the stock embeddings:\nThe micro trend vector ist \u00e2\u02c6\u02c6 Rn encapsulates all tweets for\nday t, integrating the information from companies associated\nwith the sector. This vector is then combined with the stock\nprice vector xt \u00e2\u02c6\u02c6 Rn\u00c3\u2014p which generates from Yahoo\u00e2\u20ac\u2122s stock\ndata and the macro trend vector at, serving as the input to the\nAttention GRU for forecasting stock movement and volatility.\nPredictions layer. The model is primarily designed to ex-\ntract nuanced information from stock market, macroeconomic\nfactors, and tweet data. Once the macro trend at and micro\ntrend ist are generated, they are concatenated with the stock\nprice feature xst \u00e2\u02c6\u02c6 Rp using a linear layer, producing a multi-\nlevel feature.\nV. EXPERIMENT\nWe conduct experiments to answer the following questions\nabout the performance of framework:\nQ1. Baseline methods. How do other baseline models perform\non our task?\nQ2. Tweets quality. Does using our model to extract data\nyield better results than performing sentiment analysis using\nthe entire 7.7 million tweets as input?\nQ3. Ablation study. How do the macro and micro trend affect\nits performance?\nA. Baseline Methods\nWe evaluate our model\u00e2\u20ac\u2122s effectiveness by comparing it with\ntechnical and fundamental analysis models. Technical analysis\nmodels focus on capturing patterns of historical prices and\nTABLE III : Classification performance of ECON and baseline\nmethods, measured with the accuracy (ACC) , the Matthews\ncorrelation coefficient (MCC) and Receiver Operating Char-\nacteristic (ROC). The best is in bold, Our ECON shows the\nbest performance in all evaluation metrics.\nfundamental analysis models combine historical prices with\nother government and financial market information for stock\nprice movement and volatility prediction.\n\u00e2\u20ac\u00a2 Extreme Gradient Boosting (XGBoost) [32] is a high-\nperformance gradient boosting framework renowned for its\nefficiency in handling large datasets, regularization features,\nand capability in both classification and regression tasks.\n\u00e2\u20ac\u00a2 Autoregressive Integrated Moving (ARIMA) [33] is a fore-\ncasting model for time series data, consisting of autoregressive\n(AR), differencing (I), and moving average (MA) components.\nIt assumes data linearity and stationarity.\n\u00e2\u20ac\u00a2 Long Short-Term Memory (LSTM) [34] is a type of\nrecurrent neural network (RNN) architecture specifically de-\nsigned to tackle the vanishing and exploding gradient problems\nencountered in traditional RNNs. LSTM units use gate mech-\nanisms to regulate the flow of information, making them well-\nsuited for learning from long-term dependencies in sequences.\n\u00e2\u20ac\u00a2 Attention LSTM (ALSTM) [12] is a LSTM network with\nan attention mechanism, enhancing sequence modeling by\nemphasizing relevant elements.\n\u00e2\u20ac\u00a2 DTML [35] is based on Transformer, it learns temporal\ncorrelations and combines the contexts of all target stocks.\n\u00e2\u20ac\u00a2 SLOT [9] is based on ALSTM, it captures correlations\nbetween tweets and stocks through self-supervised learning.\nTo test whether our Twitter filter can still accurately capture\nmarket sentiment even with a significantly reduced Twitter\ninput:\n\u00e2\u20ac\u00a2 AGRUD-A concatenated the sentiment score of all 7.7\nmillion tweets with stock prices and macroeconomic data,\nkeeping the structure of main predictor AGRUD unchanged.\n\u00e2\u20ac\u00a2 AGRUD-F concatenated the sentiment score of data refined\nwith stock prices and macroeconomic data, keeping the struc-\nture of main predictor AGRUD unchanged.\nEvaluation metrics. Following previous work for stock\nmovement prediction, we adopt the standard measure of Accu-\nracy (ACC) and Matthews Correlation Coefficient (MCC) as\nevaluation metrics. MCC avoids bias due to data skew. Given\nthe confusion matrix containing the number of samples clas-\nsified as true positive (tp), false positive (f p), true negative\n(tn) and false negative (f n), Acc. and MCC are calculated as\n\fTABLE IV : This table illustrates the effectiveness of the tweet\nfilter. By retaining only the tweets with the highest impres-\nsions, the results for predicting stock price movement and\nvolatility are consistent with those obtained using sentiment\nanalysis on all tweets.\neven though the number of tweets significantly decreased,\nAGRUD-A and AGRUD-F results remained almost the same.\nThis demonstrates that the filtered tweets effectively represent\npeople\u00e2\u20ac\u2122s genuine intentions. Consequently, this allows ECON\nto achieve more accurate forecasting results at a reduced cost\nand with greater efficiency.\nFor stock volatility prediction, given that data points in-\nvolving stock price changes greater than 5% only constitute\na minor portion of our dataset, using accuracy to evaluate\nECON can be misleading due to model overfitting. When\nevaluating the imbalanced data, we use Area Under the Curve\n- Receiver Operating Characteristic (AUC-ROC) where AUC\nis the area under the ROC curve, which falls in the range\n[0,1]. AUC is insensitive to the absolute numbers of positive\nand negative samples. It primarily focuses on how the model\ndistinguishes between positive and negative samples. AUC is\nthe area under the ROC curve, which falls in the range [0,1].\nAUC is calculated as follows:\nAU C =\n(cid:90) 1\n0\nT P R(F P R) dF P R,\n(20)\nwhere T P R(F P R) represents the True Positive Rate as a\nfunction of the False Positive Rate. The T P R gives the\nproportion of actual positives that are correctly identified.\nF P R is the False Positive Rate, indicating the proportion of\nnegative samples that are mistakenly identified as positive.\nB. Results\nThe performances of our ECON and the established base-\nlines are detailed in Table III. ECON is observed to be the\nsuperior baseline model in terms of accuracy and MCC for\nmovement prediction. While DTML shows an outstanding\nperformance in the AUC and MCC for volatility prediction.\nECON surpasses both of these models by significant margins.\nIn terms of accuracy, ECON achieves a score of 53.36 and\n62.84, outperforming SLOT and DTML by 1.7% and 2.7%\nrespectively, and outshines in AUC by a margin of 4.0%\nand 4.0%. We argue this improvement of price movement\nprediction is attributed to use of macro and micro trends\nto investigate the influence of multiple sectors on stocks.\nOn the other hand, leveraging macro insights and placing a\nstronger emphasis on recent information have also significantly\ncontributed to improved volatility forecasting.\nAs shown in Table V, our\nablation study revealed that incorporating multi-dimensional\nperception of stock market significantly enhances the pre-\ndictive capabilities of the model for stock movement and\nvolatility. A noteworthy point is that the result of ECON(A)\ndemonstrates a significant role of the macro trend in enhancing\nthe accuracy of volatility prediction."}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dPredicting stock market is vital for investors and\npolicymakers, acting as a barometer of the economic health. We\nleverage social media data, a potent source of public sentiment, in\ntandem with macroeconomic indicators as government-compiled\nstatistics, to refine stock market predictions. However, prior\nresearch using tweet data for stock market prediction faces\nthree challenges. First, the quality of tweets varies widely. While\nmany are filled with noise and irrelevant details, only a few\ngenuinely mirror the actual market scenario. Second, solely\nfocusing on the historical data of a particular stock without\nconsidering its sector can lead to oversight. Stocks within the\nsame industry often exhibit correlated price behaviors. Lastly,\nsimply forecasting the direction of price movement without\nassessing its magnitude is of limited value, as the extent of the rise\nor fall truly determines profitability. In this paper, diverging from\nthe conventional methods, we pioneer an ECON (A Framework\nLeveraging Tweets, Macroeconomic Indicators, and Historical\nPrices to Predict Stock Movement and Volatility). The framework\nhas following advantages: First, ECON has an adept tweets filter\nthat efficiently extracts and decodes the vast array of tweet\ndata. Second, ECON discerns multi-level relationships among\nstocks, sectors, and macroeconomic factors through a self-aware\nmechanism in semantic space. Third, ECON offers enhanced\naccuracy in predicting substantial stock price fluctuations by\ncapitalizing on stock price movement. We showcase the state-\nof-the-art performance of our proposed model using a dataset,\nspecifically curated by us, for predicting stock market movements\nand volatility.\n\nIndex Terms\u00e2\u20ac\u201dstock market prediction, tweets extraction, time\n\nseries forecasting, sentiment analysis, macroeconomic factors\n\nI. "}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nThis version of MIMo extends a previous version published at ICDL [1].\n(a) Full body view\n(b) Facial expressions\nFig. 1. MIMo, the multimodal infant model. (a) MIMo sitting in a room with\ntoys. (b) Six facial expressions of MIMo.\nImportantly, these interactions are largely initiated and con-\ntrolled by the developing child. For example, an infant controls\nits visual, proprioceptive, and haptic inputs through its eye\nand body movements. This ability to control and \u00e2\u20ac\u0153experiment\u00ef\u00bf\u00bd\nwith different movements to observe their effects on sensory\ninputs may be crucial for cognitive development, giving the\ndeveloping mind a means to probe the causal structure of\nthe world, rather than merely passively observing correlations\namong sensed variables.\nImportantly, such an active, self-controlled form of learning\nis in stark contrast\nto a large body of work in Artificial\nIntelligence (AI), including large language or vision models,\nwhich learn certain aspects of the statistical structure of large\ntraining datasets without having any control over their inputs.\nDespite the impressive successes of such models, they may\nultimately be severely limited in their ability to understand\nthe causal mechanisms underlying their training data, which\nwill limit their ability to generalize in novel situations.\nThis should come as no surprise. Every scientist learns\nthat mere correlation is not sufficient evidence for inferring\ncausation. Therefore, scientists routinely exploit the ability to\nmanipulate a system under study using clever experimental\ndesigns where they interfere with some variables and observe\nthe effects on others to distill the causal mechanisms at work.\nTherefore, recreating the physical interaction with the envi-\nronment must be a central aspect of rebuilding cognitive devel-\nopment and it may be equally essential for achieving human-\n\n\flike intelligence and consciousness in AIs. The idea that an\nAI could learn like a developing child can be traced back\nall the way to Turing [2]. However, serious \u00e2\u20ac\u0153Developmental\nRobotics\u00ef\u00bf\u00bd or \u00e2\u20ac\u0153Developmental AI\u00ef\u00bf\u00bd efforts have become more\ncommon and practical only in the last 20 years, for reviews\nsee [3]\u00e2\u20ac\u201c[8].\nThere are two options for modeling the interaction of a\ndeveloping mind with its physical and social environment. The\nfirst option is using humanoid robots. This is the approach\ntaken by the Developmental Robotics community. The main\nadvantage is the inherent realism: the system operates in the\nreal world governed by the actual laws of physics. However,\nworking with humanoid robots is expensive, time-consuming,\nand suffers from the brittleness of today\u00e2\u20ac\u2122s humanoid hardware.\nAll these factors negatively impact the reproducibility of the\nresearch. Furthermore, the sensing abilities of today\u00e2\u20ac\u2122s robots\nare usually not comparable to those of actual humans. This\nis particularly problematic for the sense of touch. The human\nbody is covered by a flexible skin containing various types\nof mechanoreceptors, thermoreceptors, and nociceptors (pain\nreceptors). These allow us to sense touch, pressure, vibration,\ntemperature, and pain. Today, reproducing such a human-like\nskin in humanoid robots is still out of reach.\nThe second option for modeling the interaction of the\ndeveloping mind with its physical and social environment is\nto do it completely in silico. Many physics simulators or game\nengines are available today that can approximate the physics\nof such interactions [9]\u00e2\u20ac\u201c[11], for review see [12]. Among\nthe disadvantages of such an approach are 1) inaccuracies\nof such simulations due to inevitable approximations and\n2) the high computational costs, especially when non-rigid\nbody parts and objects are considered. Nevertheless, the in\nsilico approach avoids all\nthe problems of working with\nhumanoid robot hardware mentioned above. Furthermore, it\nensures perfect reproducibility. Lastly, if simulations can be\nrun (much) faster than real-time, this greatly facilitates the\nsimulation of developmental processes unfolding over long\nperiods of time (weeks, months, or even years).\nTo support such in silico research, we here present the\nopen source software platform MIMo, the Multi-Modal Infant\nModel (Fig. 1). MIMo is intended to support\ntwo kinds\nof research: 1) developing computational models of human\ncognitive development and 2) building developmental AIs that\ndevelop more human-like intelligence and consciousness by\nsimilarly exploiting their ability to probe the causal structure\nof the world through their actions.\nWe have decided to model the body of MIMo after an\naverage 18-month-old child. In total, MIMo has 82 degrees\nof freedom of the body and 6 degrees of freedom of the eyes.\nMIMo also features different facial expressions that can be\nused for studies of social development. To simulate MIMo\u00e2\u20ac\u2122s\ninteraction with the physical environment we use the MuJoCo\nphysics engine [13], because of its strength at simulating\ncontact physics with friction. Generally, we have aimed for\na balance between realism and computational efficiency in the\ndesign of MIMo. To accelerate the simulation of physics and\ntouch sensation, MIMo\u00e2\u20ac\u2122s body is composed of simple rigid\nshape primitives such as a sphere for the head and capsules\n2\nfor most other body parts. Presently, MIMo features four\nsensory modalities: binocular vision, proprioception, full-body\ntouch sensation, and a vestibular system. We also plan to add\naudition in the future.\nThe remainder of this article describes the detailed design\nof MIMo and illustrates how to use it. In particular, we present\nfour scenarios where MIMo learns to 1) reach for an object,\n2) stand up, 3) touch different locations on his body, and 4)\ncatch a falling ball with his five-fingered hand. These examples\nare used to illustrate and benchmark potential uses of MIMo.\nThey are not intended as faithful models of how infants acquire\nthese behaviors.\nThis paper is an extended version of preliminary work pre-\nviously published at the ICDL 2022 conference [1]. Compared\nto the preliminary MIMo version we have added 1) five-\nfingered hands, 2) a new actuation model that more accurately\nmodels the force-generating behaviour and compliance of real\nmuscles, 3) a detailed play room (see Fig. 2), in which MIMo\ncan interact, and 4) a new demo environment of learning\nto catch a falling ball. A fifth contribution is that we have\nimproved computational efficiency and benchmarks have been\nupdated accordingly.\nII. RELATED WORK\nWe focus on two major classes of software platforms for\nsimulating cognitive development during embodied interac-\ntions with the environment. The first kind is designed to\nsimulate a particular physical robot used in Developmental\nRobotics research and intended to complement the work with\nthat physical robot. The second kind of platform emulates human body and\nsensing abilities directly and thus is not restricted by limi-\ntations of current robotics technology in general or that of\nspecific robots in particular. \nVII. EXPERIMENTS\nA. Illustrations of learning\nWhile RL is a powerful framework to generate controllers,\nits chaotic nature and requirements for large amounts of data\nrequire a stable and efficient model. In the following, we\ndemonstrate that MIMo is well-suited for RL even while\nincluding multimodal inputs. Note that badly designed mod-\nels can negatively affect RL performance, even if they are\nphysically accurate in some regimes. We use four example\ntasks: reaching for objects, standing up, self-body knowledge\ntwo different\nand catching a falling ball. In these tasks,\nstate-of-the-art, widely used deep RL algorithms, Proximal\nPolicy Optimization (PPO) [54] and Soft Actor-Critic (SAC)\n[23] are trained to optimize task-dependent reward functions\nby controlling MiMo\u00e2\u20ac\u2122s actuators. While both algorithms are\nactor-critics, PPO is a widely-used on-policy approach closely\nrelated to trust-region algorithms [55] and SAC is an off-policy\nalgorithm with a maximum-entropy formulation [56].\nWe use the default parameters of the Stable-Baselines3\nlibrary [53], consisting of linear networks with two hidden\nlayers of size 64 for PPO and size 256 for SAC, as well\nas common improvements [57], [58] that are critical for\nperformance. Input and output\nlayer sizes vary depending\non the observation and action spaces of the environments.\nPerformance is compared for PPO and SAC with 10 differ-\nent seeds (Fig. 6). We do not claim that such extrinsically\nmotivated learning is how human infants learn these skills.\nWe merely use these examples to showcase MIMo learning\nfrom multimodal input.\n(a) Binocular vision\n(b) Touch perception\nFig. 5. MIMo\u00e2\u20ac\u2122s multimodal perception while holding a ball. (a) Anaglyph of\nleft and right eye views. (b) Visualization of touch sensors in the hand, with\nthose reporting a force colored red. The size of the circle corresponds to the\namount of force sensed.\nA. Touch Perception\nHuman touch sensation is produced by a variety of receptors\nresponding towards specific aspects of touch, such as the\nSlowly Adapting type 1 (SA1), which responds primarily to\ndirect pressure and coarse texture or the Rapidly Adapting\n(RA) type for slip and fine texture [50]. As with the other\nmodalities, we model these types very simply, ignoring signal\ntravel times and condensing the various types of receptors into\na single generic \u00e2\u20ac\u0153touch\u00ef\u00bf\u00bd sensor type that measures normal and\nfrictional forces at its location. Sensor points are spread evenly\non each individual body part, with the sensor density varying\nbetween body parts based on the two-point discrimination\ndistances by [51]. Thus, the front and the back of the palm\nhave the same sensor density, but not the fingertips.\nMuJoCo only performs rigid-body simulations in which all\ncontacts are treated as point contacts. Area contacts between\nflat surfaces produce multiple contact points, for example at\nthe corners of the contact area. As soft-body physics is very\ncomputationally expensive, we do not adjust these physics but\nweakly simulate the deformation of the skin to these point\ncontacts, by distributing the point contact forces over nearby\nsensor points according to a surface response function. Our\nresponse function decreases sensed force linearly with distance\nfrom the contact point and then normalizes over all sensor\npoints, such that the total sensed force remains identical to\nthe initial point contact force from MuJoCo. The distance\nmeasure is the euclidean distance as geodesic approximations\nwere too computationally expensive. To avoid bleed-through\nto the opposite side of thin bodies (such as the palm), we only\nselect nearby sensor points with a breadth-first-search on the\nmesh of sensor points. While also expensive, the results of this\nsearch can be cached and reused, leading to a only a small run\ntime penalty compared to using just the euclidean distance.\nAll of these aspects, from the sensor point density through\nthe response function can be easily adjusted or expanded.\nPerformance can be improved for specific experiments in\nmultiple ways. In addition to adjusting the configuration of\nthe modalities or the scene, the performance of the simulation\ncan be improved by reducing the frequency of control steps,\nreducing the frequency of observation collection and thus the\ntime spent in the sensory modules."}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201dHuman intelligence and human consciousness\nemerge gradually during the process of cognitive development.\nUnderstanding this development is an essential aspect of un-\nderstanding the human mind and may facilitate the construc-\ntion of artificial minds with similar properties. Importantly,\nhuman cognitive development relies on embodied interactions\nwith the physical and social environment, which is perceived via\ncomplementary sensory modalities. These interactions allow the\ndeveloping mind to probe the causal structure of the world. This\nis in stark contrast to common machine learning approaches, e.g.,\nfor large language models, which are merely passively \u00e2\u20ac\u0153digesting\u00e2\u20ac\ufffd\nlarge amounts of training data, but are not in control of their\nsensory inputs. However, computational modeling of the kind\nof self-determined embodied interactions that lead to human\nintelligence and consciousness is a formidable challenge. Here\nwe present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simula-\ntions. MIMo\u00e2\u20ac\u2122s body is modeled after an 18-month-old child with\ndetailed five-fingered hands. MIMo perceives its surroundings via\nbinocular vision, a vestibular system, proprioception, and touch\nperception through a full-body virtual skin, while two different\nactuation models allow control of his body. We describe the design\nand interfaces of MIMo and provide examples illustrating its use.\nAll code is available at https://github.com/trieschlab/MIMo.\n\nIndex Terms\u00e2\u20ac\u201dcognitive development, developmental AI, infant\n\nmodel, multimodal perception, physics simulation\n\nI. "}]}
