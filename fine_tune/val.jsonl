{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nIntroduction\nIn our opinion, it is worth taking a step back and mea-\nsuring the goodness of XAI systems from the information\nthey can provide to users. Although measuring how much\ninformation a system can generate could be challenging, we\ncan focus on the amount of new knowledge that such flow\nof information creates in the users\u00e2\u20ac\u2122 mental models. Hence,\nif we let only non-expert users interact with XAI systems,\nwe can state that the knowledge they acquired arose from\nthe interaction with the system. Finally, if such knowledge\nis quantifiable, we can assess how much the system has been\ninformative.\nIn this paper, we aim to propose an assessment task to\nobjectively and quantitatively measure the goodness of XAI\nsystems during human-AI decision-making tasks intended\nas how much they are informative to non-expert users. More-\nover, we plan to use such a task to study the effectiveness\nof user-centred XAI in HRI scenarios. In particular, we are\ninterested in understanding whether a user-centred XAI ap-\nproach is more informative than a classical one.\nMethods\nWe are interested in introducing an objective and quantita-\ntive assessment task to measure the goodness of XAI sys-\ntems without relying only on indirect measures. Hence, we\nplan to measure the XAI systems\u00e2\u20ac\u2122 information power di-\nrectly and validate the task through a user study.\nThe task\nThe assessment consists of a decision-making task where\nusers can interact with a control panel to perform actions\nin a simulated environment (see Section ). During the task,\nusers can interact with an expert AI (in our case, with the\n2\n\f\nhumanoid robot iCub) by asking what it would do, and its\nXAI system by asking why it would do that. Apart from the\ninstructions about interacting with the control panel and the\nrobot, users start the task knowing nothing about it.\nIn a fixed amount of time, the users have to perform ac-\ntions and interact with the robot to discover (1) which is the\ntask at hand (e.g., what is the goal of the task), as well as\n(2) the rules that the AI model uses to make its actions and,\nsince the robot is an expert agent, (3) the rules that govern\nthe simulated environment.\nInteraction modalities The roles between the human-\nrobot team and their interaction modalities are simple. The\nrobot can not perform actions, but its role is limited to assist-\ning users during decision-making. However, the robot can\nnot take the initiative in giving suggestions either, but it al-\nways replies to users\u00e2\u20ac\u2122 questions. Thus, only the users can\ninteract with the control panel and act in the simulated envi-\nronment.\nWe expect the following HRI modalities. If the user al-\nready knows what to do, they act on the control panel. Oth-\nerwise, they ask the robot what it would do; then, the robot\nanswers this what question saying what action it would do in\nthe current scenario. If the user understands the robot\u00e2\u20ac\u2122s rea-\nsons, they act on the control panel (not necessarily the sug-\ngested one). Otherwise, the user asks the robot why it would\ndo that. At this point, the robot answers this why question by\nexplaining as justification for its suggestion. All that remains\nfor the user is to act on the control panel.\nCharacteristics of the task We need non-expert users to\nconsider the information passed through the interaction as\nnew information. Taking it to extremes, we consider only\nparticipants without knowledge about the task and its under-\nlying rules. For this reason, we implemented a simulation\nof a nuclear power plant management task (Figure 2). We\nchose this kind of task because it met all the requirements\nwe needed: it is challenging and captivating for non-expert\nusers, simple rules govern it, an AI model can learn those\nrules and, usually, people know nothing about the function-\ning of nuclear power plants.\nThe main objectives of the task (which we hide from\nusers) are to generate as much energy as possible and main-\ntain the system in an equilibrium state. The features of the\nenvironment are subject to rules and constraints, which we\ncan summarise as follow:\n\u00c2\u00b7 Each action corresponds to an effect on the environment:\nthus, a change of its features\u00e2\u20ac\u2122 value.\n\u00c2\u00b7 Several preconditions must be satisfied to start and con-\ntinue nuclear fission and produce energy.\n\u00c2\u00b7 Some conditions irremediably damage the plant.\n\u00c2\u00b7 The task is divided into steps of fixed time duration (e.g.,\n10 seconds), in which the users can interact with either\nthe robot or the control panel.\nThe simulated environment\nFeatures of the environment Our simulated power plant\nis composed of 4 continuous features:\n\u00c2\u00b7 The pressure in the reactor\u00e2\u20ac\u2122s core.\n\u00c2\u00b7 The temperature of the water in the reactor\n\u00c2\u00b7 The amount of water in the steam generator\n\u00c2\u00b7 The reactor\u00e2\u20ac\u2122s power.\nFurthermore, the power plant has four other discrete features\nthat regard the reactor\u00e2\u20ac\u2122s rods: security rods, fuel rods, sustain\nrods, and regulatory rods. The firsts two have two levels:\nup and down. Instead, the latter two have three levels: up,\nmedium, and down.\nThe reactor power linearly decreases over time for the ef-\nfect of the de-potentiation of the fuel rods. Hence, the reac-\ntor\u00e2\u20ac\u2122s power depends on the values of the environment\u00e2\u20ac\u2122s fea-\ntures and whether nuclear fission is taking place. Moreover,\n3\n\fthe energy produced at each step is computed by dividing the\nreactor\u00e2\u20ac\u2122s power by 360, which is the power that a 1000MW\nreactor without power dispersion produces in 10 seconds.\nActions to perform on the environment The actions that\nthe user can perform (12 in total) go from changing the po-\nsition of the rods to adding water to the steam generator or\nskipping to the next step. All those actions change the value\nof 3 parameters - T , P , and L - which correspond to the\nwater\u00e2\u20ac\u2122s temperature in the core, the core\u00e2\u20ac\u2122s pressure, and the\nwater level in the steam generator, respectively. The setting\nof the rods determines the entity of the feature updates; such\nupdates are performed at the end of each step, right after\nthe users\u00e2\u20ac\u2122 action. For example, if the safety rods are lowered\nin the reactor\u00e2\u20ac\u2122s core, the nuclear fission stops; thus, T and P\ndecrease until they reach their initial values, and the water in\nthe steam generator remains still. On the other hand, if nu-\nclear fission occurs and the user lowers the regulatory rods,\nthe fission accelerates. This acceleration consumes more wa-\nter in the steam generator, raising the core\u00e2\u20ac\u2122s temperature and\npressure more quickly, but also raising the reactor\u00e2\u20ac\u2122s power\nand the electricity produced. If the users do not act within\nthe time provided for each step, the application automati-\ncally chooses a skip action, which applies the features\u00e2\u20ac\u2122 up-\ndates based on the setting of the rods at hand.\nThe robot\u00e2\u20ac\u2122s AI\nRegarding the robot\u00e2\u20ac\u2122s AI model, we trained a deterministic\ndecision tree (DT) using the Conservative Q-Improvement\nlearning algorithm (Roth et al. 2019), which allowed us to\ntrain the DT using a reinforcement learning strategy. In-\nstead of extracting the DT from a more complex ML model\n(Vasilev, Mincheva, and Nikolov 2020)(Xiong, Zhang, and\nZhu 2017), we used this learning strategy to simplify the\ntranslation from the AI to the XAI without losing perfor-\nmance. The robot uses this expert DT to choose its action: it\ncan perform each of the 12 actions based on the eight envi-\nronment\u00e2\u20ac\u2122s features.\nStarting from its root node, the DT is queried on each of\nits internal nodes - which represent binary splits - to decide\nwhich of the two sub-trees continue the descent. Each inter-\nnal node regards a feature xi and a value for that feature vi:\nthe left sub-tree contains instances with values of xi \u00e2\u2030\u00a4 vi,\nwhile the right sub-tree contains instances with values of\nxi > vi (Buhrman and de Wolf 2002).\nThe DT\u00e2\u20ac\u2122s leaf nodes represent actions; in the implementa-\ntion of Roth et al. (Roth et al. 2019), they are defined with an\narray containing the actions\u00e2\u20ac\u2122 expected Q-values: the greater\nQ-value is associated with the most valuable action. This\nway, the DT can be queried by users with both what and why\nquestions. To answer a what question, we only need to nav-\nigate the DT using the current values of the environment\u00e2\u20ac\u2122s\nfeatures and present the resulting action to the user.\nThe robot\u00e2\u20ac\u2122s XAI\nAs we already saw in Section , answering why questions re-\ngards the robot XAI. Since the AI model to explain is already\ntransparent (Adadi and Berrada 2018), it also constitutes the\nXAI model. We can use DTs to provide explanations simply\nby using one (or more) of the feature values we encounter\nduring the descent.\nWhich of the features we encounter during descent to use\nis a problem called \u00e2\u20ac\u0153explanation selection\u00ef\u00bf\u00bd. In our case, the\nexplanation selection depends on the XAI strategy we want\nto test. For example, classical approaches use the most rel-\nevant features (in terms of the Gini index, information gain,\nor other well-established measures (Stoffel and Raileanu\n2001)). We plan to objectively and quantitatively compare\ntwo XAI strategies at first glance: a classical approach vs an\nuser-aware one.\nThe classical XAI explains using only the AI outcomes\nand environment\u00e2\u20ac\u2122s states. In particular, it justifies the robot\u00e2\u20ac\u2122s\nsuggestions by using the most relevant features, which are\nthe first ones in the DT\u00e2\u20ac\u2122s structure (see (Roth et al. 2019)).\nThe system tries to give always different explanations by\ntaking track of the DT\u00e2\u20ac\u2122s node already used and preferring\nto use the others in decreasing the level of relevance.\nOn the other hand, the user-aware XAI approach, through\nmonitoring and scaffolding (Figure 1), maintains a model of\nthe users to take track of their previous actions and predict\ntheir future behaviour (Rohlfing et al. 2020). Hence, the pre-\ndicted users\u00e2\u20ac\u2122 actions can be used to produce counterfactual\nexplanations: the fact (the outcome to explain) will be the\nrobot\u00e2\u20ac\u2122s suggestion; in contrast, the foil (the expected out-\n4\n\fcome) will be the predicted users\u00e2\u20ac\u2122 action. Figure 3 shows an\nexample of these two approaches.\nMeasuring the explanations\u00e2\u20ac\u2122 goodness\nThe assessment of the model\u00e2\u20ac\u2122s information power involves\nthe interaction between non-expert users and the system it-\nself. During this assessment, the users aim to learn as many\nsystem rules as possible. Thus, we need to collect measures\nfor each rule and combine them to obtain the model\u00e2\u20ac\u2122s infor-\nmation power. The general assessment steps are the follow-\ning:\n1. To quantify how many rules regard each feature and de-\nfine a method for measuring the number of learned rules\nrelative to each feature.\n2. To quantify how much informative weight each fea-\nture has. For the sake of simplicity, we can assume\nthat they have the same informative weight (equal to 1\nk ,\nwhere k is the number of features). The features\u00e2\u20ac\u2122 infor-\nmative weights must respect the following rule: \u00e2\u02c6\u20acj \u00e2\u02c6\u02c6\n{1, ..., k}, (cid:80)\nj \u00ce\u00b3j = 1. The features\u00e2\u20ac\u2122 informative weight\nis intended to describe how difficult it is to understand\nthe rules regarding such features.\n3. To measure the model\u00e2\u20ac\u2122s informative power for each user,\nobtaining a measure (and a set of secondary descriptive\nmeasures) for each of them.\n4. To average those measures obtaining the final results; the\nsecondary descriptive measures could also have valuable\nmeaning.\nHence, if k \u00e2\u02c6\u02c6 N is the number of the model\u00e2\u20ac\u2122s features,\nj \u00e2\u02c6\u02c6 N\n\u00ce\u00b3j \u00e2\u02c6\u02c6 [0, 1] is the informative weight of the feature j, nr\nis the number of rules regarding the feature j, nlr(i)\n\u00e2\u02c6\u02c6 N is\nthe number of rules regarding the feature j learned by the\nuser i, and am is the accuracy of the AI model m, then the\ninformative power of the model m for the user i is computed\nas follows:\nApart from the number of rules regarding each feature,\nthe most delicate aspect of the assessment regards the def-\ninition of the features\u00e2\u20ac\u2122 information weights. We suggest at\nleast two ways to set them: to make them all equal or to\ndefine the weights using experimental data. The former ap-\nproach depends on the task at hand. For our task (Section ),\nwe set the features\u00e2\u20ac\u2122 information weight by normalising the\nnumber of interactions with the system that users needed to\nunderstand those features. To state about which feature the\ninteraction regarded, we plan to proceed as follows: if the\ninteraction stopped at the request for suggestions, then the\ninteraction regards the feature on which the action suggested\neffects; otherwise, if the interaction continues with a request\nfor an explanation, then the interaction regards the feature\ncontained into the explanation.\nExperimental measures During our task, we plan to col-\nlect several quantitative measures to compute the model\u00e2\u20ac\u2122s\ninformation power as defined above:\n(M1) Measures of performance, such as the users\u00e2\u20ac\u2122 final score.\n(M2) Measures of rules understanding, such as the number of\ntask rules learned, the number of requests and interac-\ntions users needed to learn such rules, and the number of\ncorrect answers to the post-experiment test.\n(M3) Measures of generalisation, such as the number of correct\nanswers to what-if questions about the robot\u00e2\u20ac\u2122s decisions\nin particular states of the environment.\nMoreover, we collect some subjective measures:\n(M4) Measures of satisfaction, such as users\u00e2\u20ac\u2122 satisfaction level\nabout the explanations and the interaction.\n(M5) Measures of robot perception, such as users\u00e2\u20ac\u2122 feelings to-\nwards the robot and perception of it.\nDiscussion\nThe assessment task we propose satisfies properties unique\nto the XAI research field. Firstly, it focuses on the infor-\nmation power of the XAI system intended as the amount\nof information it can give to the user. Then, it defines the\ngoodness of the XAI system as a function of its information\npower. Secondly, it allows for an objective and quantitative\nanalysis of such goodness.\nThe most critical requirement of our assessment task is\nuser interaction. It allows for a two-way, possibly iterative,\ninteraction with the users. In particular, it allows the users\nto query the system by asking what it would do in a specific\nsituation and why. Consequently, the XAI system should be\nable to answer both what and why questions to exploit the\nfull potential of the assessment task.\nAnother essential factor of our task is that we can easily\ngeneralise it. To generalise our task, we need the following:\n\u00c2\u00b7 A decision-making task with characteristics similar to the\nones listed in Section .\n\u00c2\u00b7 An expert AI and several non-expert users.\n\u00c2\u00b7 The approaches to compare could be XAI algorithms or\nHRI dynamics.\n\u00c2\u00b7 At least one quantitative measure about users\u00e2\u20ac\u2122 under-\nstanding of the task: if two or more, a method to compact\nthem into a single measure is also needed.\n\u00c2\u00b7 At least one quantitative measure about users\u00e2\u20ac\u2122 ability to\ngeneralise to unseen scenarios: if two or more, a method\nto compact them into a single measure is also needed.\nAn assessment task with those characteristics is flexible\nenough to test different AI models and XAI techniques as\nlong as they allow interaction between the user and the sys-\ntem. Moreover, it can be used to objectively compare two\nor more HRI approaches to test whether HRI dynamics ease\nthe users in understanding the robot\u00e2\u20ac\u2122s suggestions and expla-\nnations. Indeed, in our future work, we plan to use this task\n5\n\fto test whether interacting with a social robot makes users\nmore receptive to learning than interacting with a non-social\none.\n"}, {"role": "assistant", "content": "Abstract\n\nThere is an increasing consensus about the effectiveness of\nuser-centred approaches in the explainable artificial intelli-\ngence (XAI) field. Indeed, the number and complexity of per-\nsonalised and user-centred approaches to XAI have rapidly\ngrown in recent years. Often, these works have a two-fold ob-\njective: (1) proposing novel XAI techniques able to consider\nthe users and (2) assessing the goodness of such techniques\nwith respect to others. From these new works, it emerged\nthat user-centred approaches to XAI positively affect the in-\nteraction between users and systems. However, so far, the\ngoodness of XAI systems has been measured through indi-\nrect measures, such as performance. In this paper, we propose\nan assessment task to objectively and quantitatively measure\nthe goodness of XAI systems in terms of their information\npower, which we intended as the amount of information the\nsystem provides to the users during the interaction. Moreover,\nwe plan to use our task to objectively compare two XAI tech-\nniques in a human-robot decision-making task to understand\ndeeper whether user-centred approaches are more informative\nthan classical ones.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a paper summarizer who takes in papers and generates good abstracts"}, {"role": "user", "content": "Summarize the following text: \nINTRODUCTION\nIn this work, we aim to model pedestrian behaviour in\nurban areas so as to create a risk map of the environment\nthat can then be used by other parts of the AV system to\nnavigate through such an area. We do this by modelling the\nsocial forces around the car in the environment, identifying\nthe different possible goals for each pedestrian in the scene\nand utilising the map information around each detected\npedestrian at each time step in their trajectory.\nIn summary, our contributions presented in this work are\nas follows: (1) we propose a novel model architecture that\ncaptures social and environmental interactions to accurately\nmodel spatio-temporal pedestrian trajectories over a short\nto medium term horizon (2) this architecture incorporates\na novel goal identification module that generates multiple\nlikely end points for each observed pedestrian partial trajec-\ntory. (3) A generative model that beats the state of the art\non two drone datasets proving that the model can generalise\nto similar environmental scenarios.\nIn our work, we take ideas from each of these systems such\nas using Transformers to capture temporal trajectories, Graph\nNetworks to capture varied and heterogeneous interactions\nand CVAEs to model the multi-modality of human behaviour.\nThe exact method in which these components are assembled\ntogether in our architecture will be described in the next\nsection.\nIII. THEORETICAL APPROACH\nA. Problem formulation\nOur problem of multimodally predicting pedestrian trajec-\ntories can be formally prescribed as follows. Starting with a\nbird\u00e2\u20ac\u2122s eye view (BEV) scene of an environment captured in\nRGB, semantically segmented into different classes, denoted\nby M contains a history of positions of each pedestrian in\nthis scene. For each scene, there exist a finite number of\ngoals that can be reached by a pedestrian denoted by G.\nEach pedestrian also interacts with every other agent in the\nscene, denoted by E.\nGiven this information, the model aims to predict the\ndistribution of pedestrian\u00e2\u20ac\u2122s trajectory for H timesteps in the\nfuture.\nB. Proposed model\nOur proposed model can be split into three main compo-\nnents, in two phases as depicted in Fig. 1. In the Encoding\nPhase, we encode the ego agent\u00e2\u20ac\u2122s interactions with the\nenvironment and other agents in the scene to create \u00e2\u20ac\u0153Social\nEmbeddings\u00ef\u00bf\u00bd. These Embeddings are used along with other\ndata to gererate multi-modal trajectories in the Generative\nPhase of the model. The three main components are the\nGraph Attention Encoder (GAE), the Temporal Transfmer\nEncoder-Decoder (TT) and the generative CVAE model for\nthe ultimate output of multi-modal trajectories. Each of these\ncomponents and their sub-components are described in detail\nbelow.\nFig. 1: GSGFormer model overview. Consider various VRUs in a\nstructured urban environment, in BEV as shown. Our model uses\nthe understanding of the interplay of social forces between different\nVRUs and the environment in which the motion is captured. A\nmap encoder captures the influence of the environment on the VRU\nunder consideration while the other encoders - for encoding the ego\nmotion of the VRU, identifying the different goals of this VRU and\nthose of its neighbours - are fed into a Graph Attention Network\nEncoder to capture a \u00e2\u20ac\u0153Social Embedding\u00ef\u00bf\u00bd within the environment.\nSocial Embeddings for each time step are then passed on to the\nGenerative Phase of the model, where different contexts from the\nTransformer Decoder and the CVAE Encoder are concatenated\nalong the last dimension of the tensor and passed to the Residual\nGMM generator. Finally, positions for a time interval h from current\ntime t are predicted by integrating these velocities over the time\ninterval dt.\n3) Goal Encoder: This subcomponent is one of our novel\ncontributions in this paper. We propose a 1D CNN to estimate\nthe goal position. Initially, The state of the ego agent for\neach history time step is embedded using the agent state\nencoder. Then the embedding is stacked over the time di-\nmension, resulting in a tensor of shape (n\u00c3\u2014dmodel). The 1D\nconvolutional neural network extracts the temporal feature\nbetween the n channels(time steps). The map embedding is\nconcatenated to the output of the goal encoder and passed\nto a linear layer. The output of the network is a vector of\nsize dmodel. The vector then serves as the input condition for\nthe CVAE-Residual-GMM module detailed further in III-B.6,\nfacilitating the generation of a range of potential goals.\n4) Social Graph Attention Neural Network: To model the\nsocial force acting on the ego agent, we introduce a hetero-\ngeneous graph representation. This representation comprises\nfour distinct nodes: the ego agent, neighboring agents, the\nsemantic map, and the goal populated by their respective\nembeddings. Physically, we only consider the effect of these\nembeddings unidirectionally on the ego agent at time step t.\nThen a heterogeneous graph attention network[35] performs\nboth node-level and semantic-level attention aggregating the\nsocial information to the ego agent node. The output is\nthe social force embedding in size of dmodel is embedded\nfrom the aggregated ego agent node. This embedding is then\nelement-wise combined with the state embedding of the ego\nagent, producing the social embedding at time step t.\n5) Temporal Transformer Model: Once the social infor-\nmation are encoded from the social graph of the ego agent\nat each time step, it is necessary to build the dependencies\nbetween graphs across different time steps. We employ a\nTransformer framework [12] to model the temporal relation-\nship. The history sequence of social embedding are fed into\nthe Transformer encoder as the source sequence with a fixed\nlength n. Since the primary operation in Transformer is the\nattention mechanism which is not sensitive to the relative or\nthe absolute position of the elements in the sequence, we add\nthe \u00ef\u00bf\u00bdpositional encodings\u00ef\u00bf\u00bd with the same dimension to each\nsocial embedding in the sequence. We compute positional\nencodings through sine and cosine functions, as described\nin [12], which allows the model to easily learn to attend by\nrelative positions. Meanwhile, it enables the model recognize\nany missing observations, as each comes with its distinct\npositional encoding.\n6) CVAE-Residual-GMM Module: We propose a gener-\native CVAE-Residual-GMM Module to realize the one-to-\nmany mapping for both goal and waypoints. The module is\ncomposed of two main parts: a Conditional Variational Auto-\nEncoder(CVAE) [36] and a Gaussian Mixture Model(GMM)\n[37]. Let\u00e2\u20ac\u2122s denote the input condition as X and its cor-\nresponding output (be it goal position or waypoint state)\nas Y. The high dimensional\nlatent variable Z is drawn\nfrom the prior distribution P\u00ce\u00b8(Z|X), facilitated by a prior\nthe\nnetwork. Throughout\nground truth is accessible, Z is drawn from the posterior\ndistribution Q\u00cf\u2022(Z|X, Y) by the recognition network. Y is\nfinally generated from the distribution P\u00cf\u02c6(Y|X, Z) through\nthe generation network.\nthe training phase, given that\nDifferent from the vanilla CVAE, the generated Y is the\nsum of two parts: the output \u00cb\u2020Y from CVAE decoder decoding\nthe concatenated (X, Z) and the residual \u00e2\u02c6\u2020Y, which adheres\nto a GMM distribution. This distribution is parameterized by\nan MLP, defining means, variances, and component weights\nbased on (X, Z). Notably, every waypoint within a single\ntrajectory utilizes a consistent Z for each sampling iteration.\n7) Training Strategy: During training, the most recent\nhistorical state, alongside the entire future state sequence, is\nconcurrently fed into the Transformer decoder. A subsequent\nmasking is applied to speed up the training and to ensure\npredictions for time step t rely solely on preceding time\n\fsteps. The latent variable Z is drawn both from P\u00ce\u00b8(Z|X)\nand Q\u00cf\u2022(Z|X, Y).\nIV. EXPERIMENTS AND RESULTS\nA. Datasets\nWe train and evaluate our model on three stationary BEV\ndatasets - the intersection Drone (inD) Dataset [33], the\nStanford Drone Dataset (SDD) [32] and the ETH/UCY\nDataset [38], [39].\ninD dataset contains 32 drone recordings of various VRU\ntrajectories of 5 different classes (pedestrian, bicycle, car,\ntruck and bus) in 4 different scenes at German intersections.\nThe raw data is samples in 25Hz. Training and test sets are\nsplit based on previous work done [40] such that scenes 1\nand 2 of the dataset are considered as training data, scene 3\nto be the validation set and scene 4 as the test set.\nSDD is a similar drone dataset, with 60 recordings at 8\nunique scenes in the Stanford university campus, each scene\ncontaining a mixture of 6 classes of VRUs (pedestrians,\nskateboarders, bicyclists, cars, cars and buses). The raw data\nis sampled at 30 Hz. The entire dataset is split in two specific\nways - one proposed in TrajNet [41] and another full split\nas in DESIRE [8].\nETH/UCY is a classic dataset widely used for the evalu-\nation for pedestrian trajectory prediction which contain only\ndata of pedestrians with rich human-human interactions. The\nraw data is extracted and annotated at 2.5Hz. We follow the\nleave-one-out strategy as presented in previous works [4], [7]\nfor evaluation.\nThe first two datasets are more representative of scenarios\nthat AVs can face in urban environments, especially in shared\nspaces than the classic ETH/UCY dataset. The background\nRGB frames in these datasets act as the maps for our model.\nIn each dataset, we only concentrate on predicting trajecto-\nries of the pedestrian VRU class while utilizing the effects of\nthe other VRU classes on these pedestrians. Another rationale\nfor utilizing similar datasets is to investigate our model\u00e2\u20ac\u2122s\nrobustness to different scenes and vantages.\nB. Experimental Setup\nFor each of the datasets chosen and all their corresponding\nscenes, we perform trajectory preprocessing as provided by\nOpenTraj tools [42]. All VRU trajectories are sampled at\n2.5Hz and velocities for these trajectories are calculated as\n\u00e2\u02c6\u2020X|t+1\n/\u00e2\u02c6\u2020t where X represents the sampled position at\nt\ntime t and \u00e2\u02c6\u2020t is the sampling interval. Each observed partial\ntrajectory lasts for 8 timesteps with a prediction horizon\nh of 12 timesteps thus implying a medium term trajectory\nprediction window of 4.8s after an observation window of\n3.2s, consistent with the baselines (IV-C) chosen to compare\nagainst our work.\nFor each trajectory, a distance of 15m (or equivalent) is\nchosen as an effective threshold for neighbour interactions\nwhile a crop patch of 10m (or equivalent) is chosen for\nmap interaction for social embeddings. Parentheses imply an\nestimation from pixel space to Cartesian space where direct\nmeasurements are unavailable in the dataset.\nThe model is set up to be trained on a single NVIDIA\nRTX A6000 GPU with 48 GB VRAM.\nC. Baselines and Metrics\na) Baselines: We compare our model\u00e2\u20ac\u2122s effectiveness\nwith different baselines for each of the datasets described\nin Section IV-A since each dataset has its own merits and\ndrawbacks. However, we explain some choices of baselines\nas below:\nSocial GAN [7]: One of the earliest generative architec-\ntures that focus on multi-modal trajectory prediction. They\napply a an LSTM Encoder-Decoder architecture connected\nby a social pooling module which used to aggregate the\nsocial interactions between actors in the scene.\nST-GAT [6]: ST-GAT is a classic work on human trajec-\ntory prediction which uses graph attention network to model\nhuman interactions. LSTM is applied to model the temporal\nfeature of the pedestrians.\nY-Net [11]: This multi-modal trajectory generation model\nuses goals, waypoints and segmented maps to predict pedes-\ntrian motion. However, it does not explicitly model any VRU\n\fMethod\nD. Implementation details\nAll input states - ego state, neighbour states, goal states -\nare embedded in a space of dmodel = 128. We use a map\npatch centered on the ego agent parsed through a pretrained\nUNet architecture to extract semantic segments from the\npatch and subsequently embed it in a space of 128. The\nGraph Attention Network is composed of two input layers\nof size 128 in mean aggregation mode with 4 attention heads\nfor each layer. The single layer Transformer encoder decoder\nmodule has a feed forward dimension set to 256 with 4 heads\nof multi-head attention. Each MLP in the CVAE encoder,\ndecoder and GMM consists of two hidden layers of size\n512 \u00e2\u2020\u2019 256 with ReLU activations on each. The dimension of\nthe latent variable is 16 for the goal and 32 for the waypoint.\nThe number of mixture components is set as 4 for goal and\n20 for waypoint.\nThe semantic segmentation network for the map encoder\nis a single model trained on both the drone datasets, with\nmanual annotations in 6 classes where unavailable as pre-\nsented in Sec. III-B.2.\nThe model is trained using the Adam optimizer with a\nlearning rate starting at 0.001 and decaying by a factor of\n0.5 every 10 epochs for a total of 50 epochs with a batch\nsize of 512.\nE. Quantitative Results\nWe organise our results by dataset, with the reported\nminADE and minFDE for 20 samples each. Clearly, for all\ndatasets, lower scores are better for both metrics with the\nbest result shown in bold. Table II compares metrics for the\nbaselines with ours. As observed, our model beats the state\nof the art in one metric while performing at the state of the\nart in the other.\nTable III shows the compared metrics for two different\nsplits for SDD - overlapping trajectories for the entire dataset\nat the top with the Trajnet split at the bottom. It can be\nseen that our model performs better than the state of the art\nwhen trained on the entire dataset compared to the Trajnet\nsplit. We conjecture that this is due to the Trajnet split not\ncontaining sufficient social interaction information among its\ntrajectories.\nTable I compares metrics of the baselines with our ap-\nproach, however as can be seen, our model does not perform\nas well on this dataset as the others. This is because of the\nrelative simplicity of ETH/UCY scenes and the inability of\nthe Map Encoder module of our approach to reach its full\npotential, proving our model performs best on large scale\ndatasets with environmental diversity.\nF. Qualitative Results\nFig. 3 presents the qualitative results of our model, de-\npicted here for the inD and SDD datasets. In all presented\ncases, we see that our model predicts trajectories accurately\nover time - even in complicated situations like fig. 3e where\na turn in the trajectory is captured and predicted as one of the\nsamples. In contrast, fig. 3f shows a case of another turning\ntrajectory with a less confident goal sampling, leading to a\nchaotic trajectory and thus, risk map, prediction.\n\f(a) Best of 20 result (inD)\n(e) Turning trajectory (inD)\n(b) Risk map (inD)\n(c) Best of 20 result (SDD)\n(d) Risk map (SDD)\n(f) Chaotic prediction (inD)\nA second conclusion we would like to draw from this work\nis how small the changes in the state of the art has become.\nWe would like to draw attention to Table II where the average\nprediction is smaller than the average stride of a human and\nwhile we have beaten the state of the art here, there is a\nplateauing of scores with changes only in the second decimal.\nIt is our conclusion that we as a community have reached\nthe limits of resolution for prediction in certain situations.\nACKNOWLEDGMENT\nThese\nresearches have been conducted within the\nAI4CCAM project, financed by the EU Project AI4CCAM\nwith grant number 101076911.\nG. Ablation Study\nWe performed an ablation study, module-by-module, of\nour proposed model and verified on the inD dataset for its\neffectiveness in prediction tasks with results shown in Table\nIV. Beginning by removing all component modules except\na single CVAE layer, trained by the l2 loss, produces an\nFDE and ADE as seen. Continuing, we add modules one\nafter another and compare against the minimum baseline\nproduced by the single CVAE layer. As seen,we notice a\nconsistent decline of ADE and FDE with the addition of\neach component module thereby showcasing the importance\nof each of the four enumerated components in our module\nas well as in pedestrian prediction tasks in general."}, {"role": "assistant", "content": "Abstract\u00e2\u20ac\u201d Pedestrian trajectory prediction, vital for self-\ndriving cars and socially-aware robots, is complicated due to\nintricate interactions between pedestrians, their environment,\nand other Vulnerable Road Users. This paper presents GS-\nGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interac-\ntions and offering a plethora of potential modal behaviors. We\nincorporate a heterogeneous graph neural network to capture\ninteractions between pedestrians, semantic maps, and potential\ndestinations. The Transformer module extracts temporal fea-\ntures, while our novel CVAE-Residual-GMM module promotes\ndiverse behavioral modality generation. Through evaluations\non multiple public datasets, GSGFormer not only outperforms\nleading methods with ample data but also remains competitive\nwhen data is limited.\n\nIndex Terms\u00e2\u20ac\u201d Situational Awareness, Pedestrian Behaviour,\n\nAutonomous Vehicles.\n\nI. "}]}
